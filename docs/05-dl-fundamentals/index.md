Deep Learning Fundamentals is your step-by-step path into feedforward neural networks and the core ideas that make deep models actually train, generalize, and stay stable. 

![Image Hero](https://i.imgur.com/IvqsAmC.png)


## [Classification Using Neural Networks](./classification-nn.md)
**From linear models to multilayer perceptrons**

Move from classic ML to neural networks by using fully connected layers for classification. Connect activation functions, loss functions, and backpropagation to the project's implementation.

**You'll learn:** Perceptrons and MLPs, activations (ReLU, sigmoid, softmax), cross-entropy loss, forward/backward pass intuition, basic multiclass classification with neural nets



## [TensorFlow 2 & Keras](./tf-keras.md)
**Train models with modern deep learning tools**

Learn to build, train, and evaluate neural networks using TensorFlow 2 and Keras, exactly as required in the TensorFlow 2 & Keras project. Focus on the APIs you actually use in assignments.

**You'll learn:** Keras Sequential and Functional APIs, layers and models, compiling with optimizers and losses, training loops (`fit`, `evaluate`, `predict`), saving and loading models


## [Optimization](./optimization.md)
**Make training actually converge**

Go beyond "it trains" by understanding how optimizers and learning rate choices shape your loss curve, mirroring the Optimization project tasks. Learn to debug training runs instead of guessing.

**You'll learn:** Gradient descent variants (SGD, momentum, RMSProp, Adam), learning rate schedules, loss landscapes, exploding/vanishing gradients, practical tips for stable training


## [Error Analysis](./error-analysis.md)
**Debug models with data, not vibes**

Use systematic error analysis to understand *why* your model fails, directly supporting the Error Analysis project. Learn to look at examples, slices, and metrics before changing architectures.

**You'll learn:** Confusion matrices revisited, per-class and per-slice analysis, common failure patterns, building checklists, prioritizing fixes that actually move metrics


## [Regularization](./regularization.md)
**Control overfitting in deep networks**

Connect regularization theory to the Regularization project, seeing how each technique changes training and validation curves in practice. Learn to keep models flexible but not memorizing.

**You'll learn:** L1/L2 weight penalties, dropout, early stopping, data-related regularization, capacity vs. overfitting, reading learning curves to choose regularization strength



> Treat each project as a lab: run small experiments, track metrics, and use error analysis and regularization to turn "it works" models into reliable baselines before you move on to CNNs.

