{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Holberton ML Handbook (Albania)","text":"<p>Learn modern machine learning by doing. Clear steps, small wins, and projects that build real skills over 9 months.</p> <p></p>"},{"location":"#start-here","title":"Start here","text":"<ul> <li>Begin with Tools, then move to Math, Data, Core ML,  Deep Learning, and Generative AI.</li> <li>Each page has a goal, steps, code, and links to go further.</li> <li>Use the search bar to jump straight to what\u2019s needed.</li> </ul>"},{"location":"#who-its-for","title":"Who it\u2019s for","text":"<ul> <li>Holberton School Albania trainees.</li> <li>Motivated learners comfortable with basic Python, terminal, and Git.</li> <li>Anyone who wants a focused ML guide without fluff.</li> </ul>"},{"location":"#whats-inside","title":"What's inside","text":"<ul> <li>Tools: Set up Git/GitHub, Jupyter Notebooks, VS Code, and sharpen Python fundamentals</li> <li>Math: Master linear algebra, NumPy, calculus, and probability/statistics for ML</li> <li>Data: Collect, clean, visualize, and query data with pandas, SQL, and MongoDB</li> <li>Core ML: Understand the ML lifecycle and implement supervised and unsupervised learning algorithms</li> </ul>"},{"location":"#suggested-path","title":"Suggested path","text":"<ul> <li>Foundations first: Tools \u2192 Math \u2192 Data.</li> <li>Core ML \u2192 Deep Learning \u2192 Capstone and deployment.</li> <li>Practice steadily; reflect and iterate.</li> </ul>"},{"location":"#study-tips","title":"Study tips","text":"<ul> <li>Keep short notes after each lesson.</li> <li>Recreate examples, then tweak them to test understanding.</li> <li>Explain what was learned to a peer\u2014teaching locks it in.</li> </ul>"},{"location":"#word-of-encouragement","title":"Word of encouragement","text":"<p>Keep going. Small steps, real projects, steady progress\u2014skill by skill, week by week.</p> <p>When it gets hard, return to the basics, build something small, and move forward again.</p> <p></p>"},{"location":"01-tools/","title":"Tools Overview","text":"<p>Three essential tools to get your machine learning projects off the ground. Each tutorial takes 25-45 minutes and gets you productive immediately.</p> <p></p>"},{"location":"01-tools/#zero-day-set-up","title":"Zero Day Set Up","text":"<p>Get ready for your Machine Learning journey</p> <p>You'll configure your learning platform access, communication channels, version control, and development environment to ensure you're ready for day one of training.</p>"},{"location":"01-tools/#git-and-github","title":"Git and GitHub","text":"<p>Save your work and collaborate with others</p> <p>Never lose code again. Track every change, work safely with teammates, and showcase your projects online. Includes step-by-step authentication setup and the commands you'll actually use daily.</p> <p>You'll learn: How to save and sync your code, collaborate without conflicts, and recover from mistakes</p>"},{"location":"01-tools/#jupyter-notebook-setup","title":"Jupyter Notebook Setup","text":"<p>Write code, see results instantly</p> <p>Perfect for experimenting with data and trying out ideas. Comes with all the math and data tools you need - no complex setup required. Just install and start exploring your data</p> <p>You'll learn: Interactive coding, instant visualizations, and sharing your discoveries</p>"},{"location":"01-tools/#vs-code-setup","title":"VS Code Setup","text":"<p>Your complete coding workspace</p> <p>Get the same professional editor used by top tech companies. Works on Windows with Linux power under the hood - giving you the best of both worlds for serious development.</p> <p>You'll learn: Professional editing, debugging, and building real applications</p> <p>Start here: These three tools work together well. Git keeps your work safe, Jupyter lets you experiment quickly, and VS Code handles everything else you need to build amazing ML projects.</p>"},{"location":"01-tools/git-and-github/","title":"Git and GitHub Setup and Essentials","text":"<p>This tutorial shows you how to set up and use Git and GitHub for effective version control and collaboration in machine learning projects. You'll learn essential commands, authentication setup, and workflows that form the foundation of professional ML development. </p> <p>Estimated time: 45 minutes</p>"},{"location":"01-tools/git-and-github/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: Managing code changes, collaborating with teammates, and maintaining project history becomes impossible without proper version control as ML projects grow in complexity.</p> <p>Practical benefits: Git and GitHub enable you to track experiments, collaborate safely with teammates, maintain reproducible ML workflows, and recover from mistakes without losing work. These skills are essential for any ML role where you'll work on shared codebases and need to manage model iterations.</p> <p>Professional context: Every ML team uses Git for code management, GitHub for collaboration, and version control for tracking model experiments and dataset changes. Mastering these tools early accelerates your integration into professional ML workflows.</p> <p></p>"},{"location":"01-tools/git-and-github/#prerequisites-learning-objectives","title":"Prerequisites &amp; Learning Objectives","text":"<p>Required knowledge: - Basic command line navigation - Understanding of files and folders - GitHub account (create at github.com)</p> <p>Required tools: - Git installed on your system - Command line access (Terminal/Command Prompt) - Text editor or IDE</p> <p>Learning outcomes: - Configure Git with your identity and GitHub authentication - Create, stage, commit, and push changes to repositories - Clone existing projects and collaborate through pull/push workflows - Troubleshoot common Git authentication and workflow issues</p> <p>High-level approach: You'll configure Git locally, set up secure GitHub authentication, practice core workflow commands, and connect local repositories to GitHub for collaboration.</p>"},{"location":"01-tools/git-and-github/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"01-tools/git-and-github/#step-1-configure-your-git-identity","title":"Step 1: Configure Your Git Identity","text":"<p>Git needs to know who you are to properly track changes and contributions.</p> <pre><code>git config --global user.name \"Your Full Name\"\ngit config --global user.email \"your-email@example.com\"\n</code></pre> <p>Why this matters: These details appear in your commit history and help teammates identify who made changes. Use the same email associated with your GitHub account for consistency.</p> <p>Expected output: No output means success. Verify with: <pre><code>git config --global --list\n</code></pre></p>"},{"location":"01-tools/git-and-github/#step-2-set-up-github-authentication","title":"Step 2: Set Up GitHub Authentication","text":"<p>GitHub requires secure authentication through Personal Access Tokens (PATs) instead of passwords.</p> <p>Create your PAT: - Log in to GitHub - Navigate to Settings \u2192 Developer settings \u2192 Personal access tokens \u2192 Tokens (classic) - Click Generate new token (classic) - Select scopes: <code>repo</code>, <code>workflow</code>, <code>write:packages</code> - Set expiration (no expiration, or 90 days) - Copy the token immediately - See short video tutorial</p> <p>Store your token securely: Save it in a password manager or secure note - you won't see it again.</p> <p>Expected outcome: You have a PAT that starts with <code>ghp_</code> and is 40+ characters long.</p>"},{"location":"01-tools/git-and-github/#step-3-create-your-repository-on-github-first","title":"Step 3: Create Your Repository on GitHub First","text":"<p>The most streamlined approach is to create your repository directly on GitHub, then clone it to your local machine. This eliminates potential conflicts and simplifies the authentication process.</p> <p>Create a new repository on GitHub: - Navigate to github.com and click the green \"New\" button - Name your repository (e.g., <code>my-ml-project</code>) - Keep it public for learning purposes - Important: Leave \"Initialize this repository with a README\" unchecked - Click \"Create repository\"</p> <p>Why this approach works better: Creating an empty repository first gives you a clean foundation and GitHub generates the exact clone URL you need, eliminating configuration errors.</p>"},{"location":"01-tools/git-and-github/#step-4-clone-your-repository-locally","title":"Step 4: Clone Your Repository Locally","text":"<p>Since you already have your Personal Access Token, you can clone the repository using the embedded authentication method:</p> <pre><code>git clone https://username:token@github.com/username/repository-name.git\n</code></pre> <p>**Make sure to replace <code>username</code>, <code>token</code> and <code>repository-name</code> with your actual data. </p> <p>Example: <pre><code>git clone https://your-username:ghp_your_token_here@github.com/your-username/my-ml-project.git\ncd my-ml-project\n</code></pre></p> <p>Why embed the token: This method stores your credentials temporarily and eliminates repeated authentication prompts during your session.</p> <p>Expected output: <pre><code>Cloning into 'my-ml-project'...\nremote: Enumerating objects: 3, done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\nReceiving objects: 100% (3/3), done.\n</code></pre></p>"},{"location":"01-tools/git-and-github/#step-5-master-the-essential-git-workflow","title":"Step 5: Master the Essential Git Workflow","text":"<p>Now that your repository is connected, follow the fundamental pattern: create/modify \u2192 stage \u2192 commit \u2192 push.</p> <p>Create your first file: <pre><code>echo \"# My ML Project\" &gt; README.md # this creates a file with heading \"My ML Project\"\necho \"This repository contains my machine learning experiments.\" &gt;&gt; README.md\n</code></pre></p> <p>Check what's changed: <pre><code>git status\n</code></pre></p> <p>Stage your changes: <pre><code>git add README.md\n</code></pre></p> <p>Commit with a descriptive message: <pre><code>git commit -m \"Add project README with initial description\"\n</code></pre></p> <p>Push to GitHub: <pre><code>git push origin main\n</code></pre></p> <p>Why this workflow matters: This four-step process (modify \u2192 add \u2192 commit \u2192 push) forms the essential of version control. Each commit creates a checkpoint you can return to, and pushing synchronizes your work with GitHub.</p> <p>Expected output after pushing: <pre><code>Enumerating objects: 3, done.\nCounting objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 245 bytes | 245.00 KiB/s, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo https://github.com/username/my-ml-project.git\n * [new branch]      main -&gt; main\n</code></pre></p> <p>Pro tip: Always use present-tense commit messages (\"Add feature\" not \"Added feature\") to maintain consistency with Git's own messaging style.</p>"},{"location":"01-tools/git-and-github/#quick-reference-for-daily-use","title":"Quick reference for daily use","text":"Command Purpose When to Use <code>git status</code> Check what changed Before staging files <code>git add .</code> Stage all changes When ready to commit everything <code>git commit -m \"message\"</code> Save changes locally After staging files <code>git push</code> Upload to GitHub After committing changes <code>git pull</code> Download latest changes Before starting new work"},{"location":"01-tools/git-and-github/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've configured Git with your identity, set up secure GitHub authentication, mastered the core workflow (stage \u2192 commit \u2192 push), and can now collaborate on repositories professionally.</p> <p>Best practices for ML development: - Commit frequently with descriptive messages to track experiment iterations - Pull before pushing to avoid conflicts when collaborating - Use <code>.gitignore</code> to exclude large model files and sensitive data - Branch for experiments to keep main branch stable during model development</p> <p>External resources for deeper learning:</p> <ul> <li>Pro Git Book - comprehensive Git reference</li> <li>GitHub Flow - branching strategies for teams  </li> <li>Git Branching Interactive Tutorial - visual practice environment</li> </ul>"},{"location":"01-tools/notebook-setup/","title":"Jupyter Notebook Setup with Anaconda","text":"<p>This tutorial shows you how to install and launch Jupyter Notebooks using Anaconda, the industry-standard platform for data science that comes pre-configured with essential ML libraries. You'll learn to navigate the Jupyter interface and master core notebook workflows for machine learning development.</p> <p>Estimated time: 25 minutes</p>"},{"location":"01-tools/notebook-setup/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: Machine learning development requires experimenting with code interactively, visualizing results immediately, and documenting analysis workflows - tasks that become overwhelming without proper tooling.</p> <p>Practical benefits: Jupyter Notebooks provide an interactive environment where you can write code, visualize results immediately, document your thought process, and share reproducible analyses with teammates. Anaconda eliminates setup complexity by pre-installing essential ML libraries including NumPy, Pandas, Matplotlib, and Scikit-learn.</p> <p>Professional context: Jupyter Notebooks are the standard for ML experimentation, data exploration, and collaborative research across all major tech companies and research institutions. Mastering this workflow is essential for any ML role involving data analysis, model prototyping, or research.</p> <p></p>"},{"location":"01-tools/notebook-setup/#prerequisites-learning-objectives","title":"Prerequisites &amp; Learning Objectives","text":"<p>Required knowledge: - Basic familiarity with Python syntax - Understanding of command line navigation (helpful but not required) - Concept of file systems and directories</p> <p>Learning outcomes: - Install Anaconda with pre-configured ML libraries - Launch Jupyter Notebook through multiple methods - Create, execute, and manage notebook cells and kernels - Apply best practices for notebook organization and reproducibility - Navigate the Jupyter interface efficiently for ML workflows</p> <p>High-level approach: You'll install Anaconda (which includes Jupyter and ML packages), learn multiple ways to launch Jupyter, and practice core notebook workflows including cell execution, markdown documentation, and kernel management.</p>"},{"location":"01-tools/notebook-setup/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"01-tools/notebook-setup/#step-1-download-and-install-anaconda","title":"Step 1: Download and Install Anaconda","text":"<p>Download the installer: - Visit the official Anaconda Distribution page - Select your operating system (Windows, macOS, or Linux) - Download the latest Python 3.x graphical installer (64-bit recommended)</p> <p>Run the installation: <pre><code># For Windows: Double-click the .exe file\n# For macOS: Double-click the .pkg file\n# For Linux: bash Anaconda3-2025.09-Linux-x86_64.sh\n</code></pre></p> <p>Installation settings: - Accept the license agreement - Choose \"Just Me\" installation type (recommended) - Use the default installation directory - Important: Do NOT add Anaconda to PATH when prompted (prevents conflicts) - Allow Anaconda to become your default Python</p> <p>Why these settings matter: Keeping Anaconda separate from system Python prevents version conflicts while giving you access to 250+ pre-installed packages including all essential ML libraries.</p> <p>Expected outcome: Anaconda Navigator appears in your applications menu, and essential packages like NumPy, Pandas, Matplotlib, and Scikit-learn are ready to use immediately.</p>"},{"location":"01-tools/notebook-setup/#step-2-launch-jupyter-notebook","title":"Step 2: Launch Jupyter Notebook","text":"<p>Method 1 - Anaconda Navigator (Recommended for beginners): - Open Anaconda Navigator from your applications menu - Click Launch under Jupyter Notebook in the main interface</p> <p>Method 2 - Windows Command Line: <pre><code># Open Command Prompt or PowerShell\njupyter notebook\n\n# Alternative if above doesn't work:\npython -m notebook\n</code></pre></p> <p>Method 3 - Anaconda Prompt (Windows): <pre><code># Search for \"Anaconda Prompt\" in start menu\njupyter notebook\n</code></pre> Expected behavior: Your default web browser opens showing the Jupyter dashboard at <code>http://localhost:8888</code>, displaying your file system.</p> <p>Understanding the interface: The dashboard shows your files and folders, allowing you to navigate, create notebooks, and manage running sessions.</p>"},{"location":"01-tools/notebook-setup/#step-3-create-and-navigate-your-first-notebook","title":"Step 3: Create and Navigate Your First Notebook","text":"<p>Create your first notebook: - In the Jupyter dashboard, click New \u2192 Python 3 - The notebook opens in a new tab with an empty code cell</p> <p>Test pre-installed ML libraries: <pre><code># Run this cell with Shift + Enter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\nprint(\"All essential ML libraries loaded successfully!\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")\n\n# Quick data visualization test\ndata = load_iris()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf.head()\n</code></pre></p> <p>Essential cell operations and shortcuts: - Execute cells: <code>Shift + Enter</code> (runs cell and moves to next) - Insert cells: <code>A</code> (above current) or <code>B</code> (below current) - Change cell type: <code>M</code> (Markdown) or <code>Y</code> (Code) - Delete cells: <code>D + D</code> (press D twice) - Save notebook: <code>Ctrl + S</code> (Windows/Linux) or <code>Cmd + S</code> (Mac)</p>"},{"location":"01-tools/notebook-setup/#step-4-master-essential-notebook-workflows","title":"Step 4: Master Essential Notebook Workflows","text":"<p>Notebook structure for ML projects: <pre><code># Project Title: [Your ML Experiment Name]\n\n## 1. Environment Setup and Data Loading\n## 2. Exploratory Data Analysis  \n## 3. Data Preprocessing and Feature Engineering\n## 4. Model Training and Hyperparameter Tuning\n## 5. Model Evaluation and Validation\n## 6. Results Analysis and Conclusions\n</code></pre></p> <p>Kernel management essentials: <pre><code># Access through menu: Kernel \u2192 Restart\n# Keyboard shortcut: 0 + 0 (press 0 twice)\nKernel \u2192 Restart &amp; Clear Output    # Fresh start\nKernel \u2192 Restart &amp; Run All         # Reproduce full analysis\n</code></pre></p> <p>Why kernel management matters: Restarting kernels ensures reproducible execution order, clears memory usage, and helps debug variable conflicts - essential for reliable ML experiments.[10][11]</p> <p>Markdown documentation best practices: <pre><code>## Data Analysis Summary\n\n**Findings:**\n- Dataset contains 150 samples with 4 features\n- No missing values detected\n- Clear separation between species classes\n\n**Next Steps:**\n- Apply feature scaling for SVM models\n- Test multiple classification algorithms\n- Perform cross-validation analysis\n</code></pre></p>"},{"location":"01-tools/notebook-setup/#step-5-save-export-and-manage-notebooks","title":"Step 5: Save, Export, and Manage Notebooks","text":"<p>Save and backup options: <pre><code># Auto-save is enabled, but manual save is recommended\nCtrl + S  # Save current notebook\n\n# Export to different formats\nFile \u2192 Download as \u2192 HTML          # For sharing results\nFile \u2192 Download as \u2192 Python (.py)  # Convert to script\nFile \u2192 Download as \u2192 PDF via LaTeX # Professional reports\n</code></pre></p> <p>Organize your ML workspace: <pre><code># Recommended folder structure:\nml-projects/\n\u251c\u2500\u2500 data/           # Raw datasets\n\u251c\u2500\u2500 notebooks/      # Jupyter notebooks\n\u251c\u2500\u2500 scripts/        # Python modules\n\u251c\u2500\u2500 models/         # Saved models\n\u2514\u2500\u2500 results/        # Output files and plots\n</code></pre></p> <p>Stop Jupyter safely: - Save all notebooks first (<code>Ctrl + S</code>) - Close browser tabs - Return to terminal/command prompt where Jupyter is running - Press <code>Ctrl + C</code> twice to shut down the server cleanly[1]</p> <p>Best practices for professional ML development: - Document your methodology with markdown cells explaining approach and findings - Clear outputs before version control to keep repositories clean - Use descriptive notebook names like <code>01-data-exploration.ipynb</code>, <code>02-model-training.ipynb</code> - Restart and run all cells periodically to ensure reproducible results</p>"},{"location":"01-tools/notebook-setup/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've installed Anaconda with pre-configured ML libraries, mastered multiple methods to launch Jupyter Notebook, learned essential cell operations and shortcuts, and established professional workflow practices for reproducible ML development.</p> <p>Best practices for ML development: - Use markdown extensively to document methodology, findings, and next steps - Restart kernels regularly to ensure reproducible execution and catch hidden dependencies - Organize notebooks systematically with clear naming conventions and logical project structure - Test essential imports at the beginning of each session to verify environment integrity</p> <p>External resources for deeper learning: - Jupyter Notebook Official Documentation - comprehensive reference guide - Anaconda Package List - explore 250+ included packages - Jupyter Notebook Best Practices - Google's professional workflow guide</p>"},{"location":"01-tools/python-warm-up/","title":"Practice with Python: Functions, Loops, and Data Structures","text":""},{"location":"01-tools/python-warm-up/#why-this-matters","title":"Why This Matters","text":"<p>Real-world machine learning (and software work) almost always involves reading, processing, and summarizing structured data. Becoming comfortable manipulating lists and dictionaries in Python will enable you to prepare datasets, analyze results, and build robust ML code.</p> <p>For this hands-on practice, you'll step through a mini-project working with a common data format (JSON), using real-world-style user data. You'll build small functions and use loops to transform, filter, and analyze the data, exactly the skillset needed for both ML work and technical interviews.</p> <p></p> <p>You'll use a set of sample user data provided by JSONPlaceholder, which mimics the structure of typical web APIs. Each user includes fields like name, email, address, company, etc.</p> <p>You have two options for how to get this data:</p> <ul> <li>Option 1: Copy the result from https://jsonplaceholder.typicode.com/users directly into your Python file as a list, assigning it to a variable named, say <code>data</code>.</li> <li>Option 2 (If curious): Use the <code>requests</code> module to fetch the data at runtime.</li> </ul> <p>Example setup (with <code>requests</code>): <pre><code>import requests\n\ndata = requests.get(\"https://jsonplaceholder.typicode.com/users\").json()\n\n# If you get an error, run this in the terminal first:\n# pip install requests\n</code></pre></p>"},{"location":"01-tools/python-warm-up/#step-by-step-practice-tasks","title":"Step-by-Step Practice Tasks","text":"<p>Let\u2019s start simple and build up to more complex processing. Create a Python script called <code>user_processing.py</code>. Each task can be a separate function in that file.</p>"},{"location":"01-tools/python-warm-up/#task-1-print-all-user-names","title":"Task 1: Print All User Names","text":"<p>Write a function that loops through the <code>data</code> and prints the <code>name</code> of each user.</p>"},{"location":"01-tools/python-warm-up/#task-2-collect-emails-with-a-specific-domain","title":"Task 2: Collect Emails with a Specific Domain","text":"<p>Write a function that returns a list of all users' emails ending with <code>.biz</code>.</p>"},{"location":"01-tools/python-warm-up/#task-3-find-users-in-a-given-city","title":"Task 3: Find Users in a Given City","text":"<p>Write a function that takes a city name as an argument and prints the names and emails of all users living in that city (<code>data[i][\"address\"][\"city\"]</code>).</p>"},{"location":"01-tools/python-warm-up/#task-4-count-companies","title":"Task 4: Count Companies","text":"<p>Write a function that builds and prints a dictionary: keys are company names (from <code>data[i][\"company\"][\"name\"]</code>), values are counts of how many users work at each company.</p>"},{"location":"01-tools/python-warm-up/#task-5-list-unique-zipcodes","title":"Task 5: List Unique Zipcodes","text":"<p>Write a function that extracts all unique zip codes from the address section of each user, and prints the sorted list of zip codes.</p>"},{"location":"01-tools/python-warm-up/#task-6-summarize-users-by-username-initial","title":"Task 6: Summarize Users by Username Initial","text":"<p>Write a function that creates a dictionary mapping first letters of usernames (from <code>data[i][\"username\"]</code>) to a list of names of users starting with that letter (e.g., <code>'B': ['Bret', 'Brandon']</code>).</p>"},{"location":"01-tools/python-warm-up/#task-7-nicely-print-a-users-full-address","title":"Task 7: Nicely Print a User's Full Address","text":"<p>Write a function that takes a username and prints their full address in this format: <code>\"Leanne Graham: Apt. 556, Kulas Light, Gwenborough, 92998-3874\"</code></p>"},{"location":"01-tools/python-warm-up/#submission-guidelines","title":"Submission Guidelines","text":"<ul> <li>Organize your code: Place each task in its own function, and call the functions from a main block (<code>if __name__ == \"__main__\":</code>).</li> <li>Comment your code: Briefly explain what each function does.</li> <li>Test each function: Call them with sample values to demonstrate the output.</li> </ul>"},{"location":"01-tools/python-warm-up/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Completing these tasks will help you: - Practice for-loops, list/dictionary processing, and function writing - Work with real-world nested data - Get ready for ML work, interviews, and project tasks</p> <p>Next, you'll expand this skillset to analyze larger datasets and write code for typical ML preprocessing steps.</p>"},{"location":"01-tools/vscode-setup/","title":"Visual Studio Code Setup with WSL for Machine Learning","text":"<p>This tutorial shows you how to install and configure Visual Studio Code with Windows Subsystem for Linux (WSL) to create a professional ML development environment. You'll learn to combine Windows usability with Linux power for seamless machine learning workflows.</p> <p>Estimated time: 35 minutes</p>"},{"location":"01-tools/vscode-setup/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: Machine learning development often requires Linux tools and libraries, but many developers use Windows machines. Traditional solutions like dual-boot or virtual machines create workflow friction and performance overhead.</p> <p>Practical benefits: VS Code with WSL provides the best of both worlds - Windows productivity tools with native Linux performance for ML development. You get access to Linux package managers, Python environments, and ML frameworks while maintaining familiar Windows workflows and file system integration.</p> <p>Professional context: Most ML production environments run on Linux, making WSL essential for developing code that matches deployment targets. VS Code with remote development is the industry standard for teams working across different operating systems and cloud environments.</p> <p></p>"},{"location":"01-tools/vscode-setup/#prerequisites-learning-objectives","title":"Prerequisites &amp; Learning Objectives","text":"<p>Required knowledge: - Basic Windows navigation and file management - Understanding of command line concepts - Familiarity with text editors or IDEs</p> <p>Learning outcomes: - Install and configure VS Code with essential ML extensions - Set up WSL with Ubuntu for Linux development environment - Connect VS Code to WSL for seamless remote development - Navigate between Windows and Linux file systems efficiently - Configure Python environments and run ML code in integrated terminals</p> <p>High-level approach: You'll install VS Code first, then enable WSL with Ubuntu, install the Remote-WSL extension, and establish an integrated development workflow that combines Windows interface with Linux backend.</p>"},{"location":"01-tools/vscode-setup/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"01-tools/vscode-setup/#step-1-install-visual-studio-code","title":"Step 1: Install Visual Studio Code","text":"<p>Download and install VS Code: - Visit https://code.visualstudio.com/ - Download the Windows installer (64-bit recommended) - Run the installer with these important settings:   - Add \"Open with Code\" action to Windows Explorer file context menu   - Add \"Open with Code\" action to Windows Explorer directory context menu   - Add to PATH (enables command line access)</p> <p>Why these settings matter: Adding to PATH allows you to open VS Code from any terminal, while context menu integration provides quick access to editing files and folders directly from Windows Explorer.</p> <p>Expected outcome: VS Code launches successfully and you can access it from Start menu, desktop shortcut, or by typing <code>code</code> in any command prompt.</p>"},{"location":"01-tools/vscode-setup/#step-2-enable-and-install-wsl-with-ubuntu","title":"Step 2: Enable and Install WSL with Ubuntu","text":"<p>Install WSL using PowerShell: <pre><code># Open PowerShell as Administrator\n# Right-click Start button \u2192 \"Terminal (Admin)\" or \"PowerShell (Admin)\"\nwsl --install\n</code></pre></p> <p>What this command accomplishes: - Enables WSL and Virtual Machine Platform features - Downloads and installs the Linux kernel - Sets WSL 2 as default (better performance) - Installs Ubuntu Linux distribution automatically[2]</p> <p>Complete the installation: - Restart your computer when prompted - After restart, Ubuntu setup will launch automatically - Create a Linux username and password (can be different from Windows) - Important: Remember this password - you'll need it for <code>sudo</code> commands</p> <p>Verify WSL installation: <pre><code># In any command prompt or PowerShell\nwsl --list --verbose\n</code></pre></p> <p>Expected output: <pre><code>  NAME      STATE           VERSION\n* Ubuntu    Running         2\n</code></pre></p>"},{"location":"01-tools/vscode-setup/#step-3-install-wsl-extension-and-connect-vs-code","title":"Step 3: Install WSL Extension and Connect VS Code","text":"<p>Install the Remote-WSL extension: - Open VS Code - Press <code>Ctrl + Shift + X</code> to open Extensions - Search for \"Remote - WSL\" by Microsoft - Click Install</p> <p>Connect to WSL: - Press <code>Ctrl + Shift + P</code> to open Command Palette - Type and select \"Remote-WSL: New WSL Window\" - Choose Ubuntu from the distribution list - A new VS Code window opens connected to your Linux environment</p> <p>Verify the connection: - Look for \"WSL: Ubuntu\" in the bottom-left corner of VS Code - Open integrated terminal with <code>Ctrl +</code> (backtick) - The terminal should show your Linux username and prompt</p> <p>Expected terminal prompt: <pre><code>username@DESKTOP-NAME:~$\n</code></pre></p>"},{"location":"01-tools/vscode-setup/#step-4-configure-your-linux-development-environment","title":"Step 4: Configure Your Linux Development Environment","text":"<p>Update Ubuntu packages: <pre><code># Run in the WSL-connected VS Code terminal\nsudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre></p> <p>Install essential development tools: <pre><code># Python and development essentials (if not already installed)\nsudo apt install python3 python3-pip python3-venv git curl wget -y\n\n# Verify installations\npython3 --version\npip3 --version\ngit --version\n</code></pre></p> <p>Install Python ML libraries: <pre><code># Create a virtual environment for ML projects\npython3 -m venv ~/ml-env\nsource ~/ml-env/bin/activate\n\n# Install essential ML packages\npip install numpy pandas matplotlib seaborn scikit-learn jupyter notebook\n</code></pre></p> <p>Why virtual environments are essential: Virtual environments isolate project dependencies, preventing conflicts between different ML projects and ensuring reproducible development across team members.</p> <p>Expected outcome: You can run Python ML code natively in Linux while editing comfortably in VS Code.</p>"},{"location":"01-tools/vscode-setup/#step-5-master-the-integrated-workflow","title":"Step 5: Master the Integrated Workflow","text":"<p>File system navigation: <pre><code># Access Windows files from WSL\ncd /mnt/c/Users/YourUsername/Documents\n\n# Create ML project in Linux home (recommended for performance)\ncd ~\nmkdir ml-projects\ncd ml-projects\n</code></pre></p> <p>Open projects efficiently: <pre><code># From WSL terminal, open current directory in VS Code\ncode .\n\n# Create and edit files directly\ncode my_ml_script.py\n</code></pre></p> <p>Essential VS Code shortcuts for remote development: - <code>Ctrl + Shift + P</code>: Command Palette (most important!) - <code>Ctrl +</code>: Toggle integrated terminal - <code>Ctrl + Shift + E</code>: File Explorer - <code>Ctrl + B</code>: Toggle sidebar - <code>F1</code>: Alternative Command Palette access</p> <p>Python development workflow: <pre><code># Activate your ML environment\nsource ~/ml-env/bin/activate\n\n# Run Python scripts\npython my_ml_script.py\n</code></pre></p>"},{"location":"01-tools/vscode-setup/#step-6-install-essential-vs-code-extensions-for-ml","title":"Step 6: Install Essential VS Code Extensions for ML","text":"<p>Install Python development extensions: - Press <code>Ctrl + Shift + X</code> in your WSL-connected window - Install these essential extensions:   - Python (Microsoft) - Python language support   - Jupyter (Microsoft) - Notebook support in VS Code</p> <p>File system best practices: - Store Linux-specific projects in <code>~/</code> (Linux home) for best performance - Access Windows files when needed via <code>/mnt/c/Users/YourUsername/</code> - Use WSL for running code, Windows for file management when convenient</p>"},{"location":"01-tools/vscode-setup/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've installed VS Code with WSL integration, created a Linux development environment with Python ML libraries, mastered file system navigation between Windows and Linux, and established a professional workflow for ML development.</p> <p>Best practices for ML development: - Store ML projects in Linux home (<code>~/ml-projects/</code>) for optimal performance and native tool access - Use virtual environments for each project to maintain clean dependency management - Leverage integrated terminal to avoid switching between VS Code and separate command windows - Keep sensitive data in WSL to benefit from Linux security and permissions model</p> <p>For non-Windows users: - macOS/Linux users: Install VS Code directly and use local Python development - no WSL needed - All platforms: Consider Remote-SSH extension for connecting to remote servers or cloud instances</p> <p>External resources for deeper learning: - VS Code WSL Tutorial - official comprehensive guide[4] - WSL Best Practices - Microsoft's development environment guide[5] - Remote Development Extension Pack - additional remote development capabilities[1]</p> <p>Practice exercises: - Create a sample ML project using scikit-learn in your WSL environment - Practice navigating between Windows and Linux file systems - Set up a Git repository and commit code from VS Code with WSL</p>"},{"location":"01-tools/zero-day/","title":"Zero Day: Essential Setup for Machine Learning Training","text":"<p>This tutorial guides you through setting up all accounts and tools required for your Machine Learning training at Holberton. You'll configure your learning platform access, communication channels, version control, and development environment to ensure you're ready for day one of training.</p> <p>Estimated time: 20 minutes</p>"},{"location":"01-tools/zero-day/#why-this-matters","title":"Why This Matters","text":"<p>Starting ML training without proper account setup and tool configuration leads to lost time troubleshooting access issues, missed communications, and inability to submit assignments when you should be focusing on learning.</p> <p></p>"},{"location":"01-tools/zero-day/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"01-tools/zero-day/#step-1-access-and-configure-your-holberton-intranet","title":"Step 1: Access and Configure Your Holberton Intranet","text":"<p>The intranet serves as your central hub for all course materials, project specifications, and progress tracking.</p> <p>Access the platform: - Navigate to https://intranet.hbtn.io/. Then log in with the credentials provided to you by email. You may bookmark this page for easy daily access. </p> <p>Why this matters: The intranet contains all project requirements, learning resources, and submission deadlines. It's your single source of truth throughout the program.</p> <p>Complete your profile: - Click on your profile icon (bottom left corner) - Critical step: Fill in all mandatory fields marked with asterisks (*) - Add a (professional) profile photo - Save your changes - At this phase, do not yet change the password.</p>"},{"location":"01-tools/zero-day/#step-2-connect-to-slack-for-team-communication","title":"Step 2: Connect to Slack for Team Communication","text":"<p>Slack serves as your real-time communication channel with instructors, mentors, and fellow students throughout the program.</p> <p>Access Slack: Locate the Slack icon or link within the intranet (left side panel on the navigation menu). Click to launch Slack. </p> <p>Important: Use the same credentials as your intranet login. Alternatively, download the Slack desktop app for better notifications. You'll automatically be added to your cohort's group channel. </p>"},{"location":"01-tools/zero-day/#step-3-create-your-github-account","title":"Step 3: Create Your GitHub Account","text":"<p>GitHub hosts your code repositories and integrates with the platform's automated grading system.</p> <p>If you already have a GitHub account you may use it. </p> <p>Alternatively,</p> <p>Create your account: - Navigate to https://github.com/signup. Enter your email address (use a professional email you'll access long-term) and create a strong password. </p> <p>Choose a professional username (avoid numbers or special characters if possible)</p> <p>Why username matters: Your GitHub username becomes part of your professional identity. Choose something you'd be comfortable sharing with future employers, as your ML projects will remain visible in your portfolio.</p> <p>Next action: Remember to add this exact username to your intranet profile as described in Step 1.</p>"},{"location":"01-tools/zero-day/#step-4-access-your-cloud-development-environment","title":"Step 4: Access Your Cloud Development Environment","text":"<p>Containers on Demand (COD) provides pre-configured Linux machines with all necessary ML libraries installed, eliminating local setup complexity.</p> <p>Access the platform: - Navigate to https://cod.hbtn.io/sign_in. Once there, log in with your same intranet credentials and wait for the dashboard to load</p> <p>Why cloud environments matter: COD ensures everyone works in identical environments with consistent library versions, eliminating \"it works on my machine\" problems common in ML development.</p> <p>Configure your container settings:</p> <p>Step 4.1: Select your region - Locate the \"Region\" dropdown at the top of the page  - Important: Select Europe for optimal performance and compliance - This choice affects connection speed and data residency</p> <p>Step 4.2: Choose your container - Scroll through the container list - Find and select ml_ubuntu_2204 - Click <code>Spin Up Container</code> - Wait 30-60 seconds for the container to initialize</p> <p>Step 4.3: Access your development environment - Click \"<code>Actions</code> and select <code>VS Code</code> to launch the web-based VS Code interface - The interface loads with a Linux terminal and file explorer</p> <p>Why this container: <code>ml_ubuntu_2204</code> comes pre-installed with Python, NumPy, pandas, scikit-learn, TensorFlow, PyTorch, and other essential ML libraries on Ubuntu 22.04 LTS.</p> <p>Important info: the container on demand expands after 4 hours. You need to repeat this process any time you work witht the platform. You can add more time as you are working.</p>"},{"location":"01-tools/zero-day/#quick-reference-for-daily-workflow","title":"Quick Reference for Daily Workflow","text":"Platform URL Purpose Credentials Intranet intranet.hbtn.io Course materials, projects, progress Primary account Slack Via intranet link Communication, support Same as intranet GitHub github.com Code hosting, version control Separate account COD cod.hbtn.io Development environment Same as intranet"},{"location":"01-tools/zero-day/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've configured your Holberton intranet profile with GitHub integration, connected to Slack for team communication, created a professional GitHub account, and launched your pre-configured ML development environment with VS Code customization.</p> <p>Next tutorial: Complete the Git and GitHub tutorial to finish setting up version control and learn the essential workflow for submitting projects.</p>"},{"location":"02-math/","title":"Math Overview","text":"<p>Build the math intuition that powers machine learning. Short, focused lessons with clear examples and practice you can use right away.</p> <p></p> <p>The language of data and models</p> <p>Understand vectors, matrices, and transformations\u2014the backbone of features, embeddings, and neural networks. Learn the operations that show up everywhere: dot products, norms, matrix multiplication, and projections.</p> <p>You\u2019ll learn: How data is represented, how models combine inputs, and how to work with matrices confidently.</p>"},{"location":"02-math/#numpy","title":"NumPy","text":"<p>Fast and easy math with arrays</p> <p>Learn to use NumPy, Python\u2019s main library for number crunching. See how to make and change arrays, do math on whole groups of numbers at once, and handle data the way ML tools expect.</p> <p>You\u2019ll learn: How to create arrays, use built-in functions, reshape data, pick out values, and do fast math without loops.</p>"},{"location":"02-math/#calculus","title":"Calculus","text":"<p>Change, slopes, and optimization</p> <p>Grasp derivatives, gradients, and chain rule to see how models learn. Connect loss functions to gradient descent and learn why step size and curvature matter.</p> <p>You\u2019ll learn: How to compute and interpret gradients, tune learning, and reason about optimization.</p>"},{"location":"02-math/#probability-statistics","title":"Probability &amp; Statistics","text":"<p>Uncertainty, inference, and decisions</p> <p>Work with distributions, sampling, estimation, and confidence. Learn how to evaluate models with sound metrics and understand variance, bias, and overfitting.</p> <p>You\u2019ll learn: How to quantify uncertainty, choose the right metrics, and make data\u2011driven decisions.</p> <p>Start here: Pick one topic, do a quick review, then try a small exercise in code. Repeat often as clarity compounds.</p>"},{"location":"02-math/calculus/","title":"Calculus for Machine Learning","text":"<p>This tutorial introduces the essential calculus concepts that power machine learning algorithms. You'll learn summation and product notation, derivatives for optimization, and integrals for probability\u2014all with clear examples connecting math to ML practice.</p> <p>Estimated time: 70 minutes</p>"},{"location":"02-math/calculus/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: Machine learning models learn by adjusting parameters to minimize errors. Without understanding derivatives (how functions change), gradients (direction of steepest change), and optimization (finding best parameters), you cannot grasp how models actually learn or why training sometimes fails.</p> <p>Practical benefits: Calculus provides the mathematical framework for optimization algorithms like gradient descent, the backbone of neural network training. Understanding derivatives helps you debug vanishing gradients, tune learning rates, interpret loss curves, and implement custom training loops. Integrals appear in probability calculations and computing expected values for model evaluation.</p> <p>Professional context: Every time a neural network trains, it's computing thousands of derivatives using the chain rule (backpropagation). Understanding calculus transforms \"black box\" training into something you can reason about, debug, and improve. When a model fails to converge or gradients explode, calculus knowledge helps you identify and fix the problem.</p> <p></p>"},{"location":"02-math/calculus/#prerequisites-learning-objectives","title":"Prerequisites &amp; Learning Objectives","text":"<p>Required knowledge:</p> <ul> <li>Algebra fundamentals (functions, equations, exponents)</li> <li>Basic understanding of graphs and coordinate systems</li> <li>Python basics for computational examples</li> <li>Completed Linear Algebra tutorial (vectors, matrices)</li> </ul> <p>Learning outcomes:</p> <ul> <li>Read and write summation and product notation</li> <li>Understand what derivatives measure and why they matter for ML</li> <li>Apply derivative rules (sum, product, chain) to compute gradients</li> <li>Compute partial derivatives for multivariate functions</li> <li>Understand integrals as area under curves and cumulative sums</li> <li>Connect calculus concepts to gradient descent and backpropagation</li> </ul> <p>Start with notation for sums and products, build intuition for derivatives as rates of change, master derivative rules used in ML, then understand integrals for probability and optimization.</p>"},{"location":"02-math/calculus/#core-concepts","title":"Core Concepts","text":""},{"location":"02-math/calculus/#why-calculus-for-machine-learning","title":"Why Calculus for Machine Learning?","text":"<p>Calculus is the mathematics of change and accumulation. Machine learning is fundamentally about:</p> <ol> <li>Understanding complex systems - how small input changes affect outputs</li> <li>Optimizing algorithms - finding parameter values that minimize loss</li> <li>Computing gradients - determining which direction improves the model</li> <li>Working with probability - integrating probability density functions</li> </ol> <p>Training a model means repeatedly asking \"if I change this parameter slightly, does my loss go up or down?\" That question is answered by derivatives.</p>"},{"location":"02-math/calculus/#calculus-in-the-ml-workflow","title":"Calculus in the ML Workflow","text":"ML Task Calculus Concept Example Model training Derivatives Gradient descent optimization Backpropagation Chain rule Computing gradients layer-by-layer Loss functions Derivatives Finding minimum prediction error Probability Integrals Computing expected values, cumulative distributions Feature analysis Partial derivatives Understanding feature sensitivity"},{"location":"02-math/calculus/#summation-notation","title":"Summation Notation","text":"<p>Summation notation provides a compact way to express the sum of many terms\u2014essential for expressing loss functions and aggregating predictions.</p> <p>Sigma notation $\\(\\Sigma\\)$:</p> <p>The symbol $\\(\\Sigma\\)$ (capital Greek sigma) means \"sum up all these terms.\"</p> \\[ \\sum_{i=1}^{n} a_i = a_1 + a_2 + a_3 + \\cdots + a_n \\] <p>Breaking it down:</p> <ul> <li>$\\(i\\)$ is the index variable (usually starts at 1)</li> <li>$\\(i=1\\)$ is the starting value</li> <li>$\\(n\\)$ is the ending value (upper limit)</li> <li>$\\(a_i\\)$ is the term being summed (depends on $\\(i\\)$)</li> </ul> <p>Example 1: Sum of first 10 integers</p> \\[ \\sum_{i=1}^{10} i = 1 + 2 + 3 + \\cdots + 10 = 55 \\] <p>Example 2: Sum of squares</p> \\[ \\sum_{i=1}^{3} i^2 = 1^2 + 2^2 + 3^2 = 1 + 4 + 9 = 14 \\] <p>Python implementation:</p> <pre><code>import numpy as np\n\n# Sum of first 10 integers\nresult = sum(range(1, 11))\nprint(f\"Sum of 1 to 10: {result}\")\n\n# Sum of squares from 1 to 3\nsquares = [i**2 for i in range(1, 4)]\nresult = sum(squares)\nprint(f\"Sum of squares: {result}\")\n\n# Using NumPy\narr = np.arange(1, 11)\nprint(f\"NumPy sum: {np.sum(arr)}\")\n</code></pre> <p>Expected output:</p> <pre><code>Sum of 1 to 10: 55\nSum of squares: 14\nNumPy sum: 55\n</code></pre> <p>ML connection: The mean squared error (MSE) loss function is a summation:</p> \\[ \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\] <p>This sums up squared errors across all $\\(n\\)$ data points.</p>"},{"location":"02-math/calculus/#product-notation","title":"Product Notation","text":"<p>Product notation uses $\\(\\Pi\\)$ (capital Greek pi) to represent multiplication of multiple terms.</p> <p>Pi notation $\\(\\Pi\\)$:</p> \\[ \\prod_{i=1}^{n} a_i = a_1 \\times a_2 \\times a_3 \\times \\cdots \\times a_n \\] <p>Example 1: Product of first 5 integers (factorial)</p> \\[ \\prod_{i=1}^{5} i = 1 \\times 2 \\times 3 \\times 4 \\times 5 = 120 \\] <p>Example 2: Nested products</p> \\[ \\prod_{i=1}^{2} \\prod_{j=1}^{3} (i \\cdot j) = (1 \\cdot 1)(1 \\cdot 2)(1 \\cdot 3)(2 \\cdot 1)(2 \\cdot 2)(2 \\cdot 3) = 1 \\cdot 2 \\cdot 3 \\cdot 2 \\cdot 4 \\cdot 6 = 288 \\] <p>Python implementation:</p> <pre><code>import numpy as np\n\n# Product of first 5 integers\nresult = np.prod(range(1, 6))\nprint(f\"Product of 1 to 5: {result}\")\n\n# Nested products\nresult = 1\nfor i in range(1, 3):\n    for j in range(1, 4):\n        result *= (i * j)\nprint(f\"Nested product: {result}\")\n</code></pre> <p>Expected output:</p> <pre><code>Product of 1 to 5: 120\nNested product: 288\n</code></pre> <p>ML connection: Probability calculations often involve products. For example, the likelihood of independent events:</p> \\[ P(\\text{all events}) = \\prod_{i=1}^{n} P(\\text{event}_i) \\]"},{"location":"02-math/calculus/#common-mathematical-series","title":"Common Mathematical Series","text":"<p>Certain series appear frequently in ML and have known formulas.</p> <p>Faulhaber's formulas for power sums:</p> <p></p> <p>Sum of first $\\(m\\)$ integers:</p> \\[ \\sum_{k=1}^{m} k = \\frac{m(m+1)}{2} \\] <p>Example: $\\(\\sum_{k=1}^{100} k = \\frac{100 \\cdot 101}{2} = 5050\\)$</p> <p>Sum of first $\\(m\\)$ squares:</p> \\[ \\sum_{k=1}^{m} k^2 = \\frac{m(m+1)(2m+1)}{6} \\] <p>Example: $\\(\\sum_{k=1}^{10} k^2 = \\frac{10 \\cdot 11 \\cdot 21}{6} = 385\\)$</p> <p>Sum of first $\\(m\\)$ cubes:</p> \\[ \\sum_{k=1}^{m} k^3 = \\left[\\frac{m(m+1)}{2}\\right]^2 \\] <p>Example: $\\(\\sum_{k=1}^{5} k^3 = \\left[\\frac{5 \\cdot 6}{2}\\right]^2 = 15^2 = 225\\)$</p> <p>Python verification:</p> <pre><code>import numpy as np\n\nm = 10\n\n# Sum of integers\nformula_result = m * (m + 1) // 2\nactual_result = sum(range(1, m + 1))\nprint(f\"Sum of integers - Formula: {formula_result}, Actual: {actual_result}\")\n\n# Sum of squares\nformula_result = m * (m + 1) * (2*m + 1) // 6\nactual_result = sum([k**2 for k in range(1, m + 1)])\nprint(f\"Sum of squares - Formula: {formula_result}, Actual: {actual_result}\")\n\n# Sum of cubes\nformula_result = (m * (m + 1) // 2) ** 2\nactual_result = sum([k**3 for k in range(1, m + 1)])\nprint(f\"Sum of cubes - Formula: {formula_result}, Actual: {actual_result}\")\n</code></pre> <p>Expected output:</p> <pre><code>Sum of integers - Formula: 55, Actual: 55\nSum of squares - Formula: 385, Actual: 385\nSum of cubes - Formula: 3025, Actual: 3025\n</code></pre> <p>Why these matter: Understanding series helps you analyze algorithm complexity and compute aggregate statistics efficiently.</p>"},{"location":"02-math/calculus/#what-is-a-derivative","title":"What is a Derivative?","text":"<p>A derivative measures the rate of change of a function. It answers the question: \"If I change the input by a tiny amount, how much does the output change?\"</p> <p>Formal definition:</p> \\[ f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h} \\] <p>This says: \"Take the change in output $\\((f(x+h) - f(x))\\)$, divide by the change in input $\\((h)\\)$, and see what happens as $\\(h\\)$ gets infinitely small.\"</p> <p>Geometric interpretation: The derivative is the slope of the tangent line to the function at a point.</p> <p></p> <p>Notation variations:</p> <ul> <li>$\\(f'(x)\\)$ (prime notation)</li> <li>$\\(\\frac{df}{dx}\\)$ (Leibniz notation)</li> <li>$\\(\\frac{dy}{dx}\\)$ (when $\\(y = f(x)\\)$)</li> </ul> <p>Example: Derivative of $\\(f(x) = x^2\\)$</p> <p>Using the definition:</p> \\[ f'(x) = \\lim_{h \\to 0} \\frac{(x+h)^2 - x^2}{h} = \\lim_{h \\to 0} \\frac{x^2 + 2xh + h^2 - x^2}{h} = \\lim_{h \\to 0} \\frac{2xh + h^2}{h} = \\lim_{h \\to 0} (2x + h) = 2x \\] <p>So $\\(\\frac{d}{dx}(x^2) = 2x\\)$.</p> <p>Physical intuition: If $\\(f(x)\\)$ is position, $\\(f'(x)\\)$ is velocity. If $\\(f(x)\\)$ is velocity, $\\(f'(x)\\)$ is acceleration.</p> <p>ML connection: In gradient descent, the derivative tells us which direction to move parameters to reduce loss.</p>"},{"location":"02-math/calculus/#common-derivative-rules","title":"Common Derivative Rules","text":"<p>Instead of using the limit definition every time, we use standard rules.</p> <p>Power rule:</p> \\[ \\frac{d}{dx} x^n = n \\cdot x^{n-1} \\] <p>Examples:</p> <ul> <li> \\[\\frac{d}{dx} x^3 = 3x^2\\] </li> <li> \\[\\frac{d}{dx} x^{10} = 10x^9\\] </li> <li> \\[\\frac{d}{dx} \\sqrt{x} = \\frac{d}{dx} x^{1/2} = \\frac{1}{2}x^{-1/2} = \\frac{1}{2\\sqrt{x}}\\] </li> </ul> <p>Constant rule:</p> \\[ \\frac{d}{dx} c = 0 \\] <p>Constants don't change, so their rate of change is zero.</p> <p>Linear function:</p> \\[ \\frac{d}{dx} (ax + b) = a \\] <p>The slope of a line is constant.</p> <p>Logarithm:</p> \\[ \\frac{d}{dx} \\ln(x) = \\frac{1}{x} \\] <p>Exponential:</p> \\[ \\frac{d}{dx} e^x = e^x \\] <p>The exponential function is its own derivative!</p> <p>Reciprocal:</p> \\[ \\frac{d}{dx} \\frac{1}{x} = \\frac{d}{dx} x^{-1} = -x^{-2} = -\\frac{1}{x^2} \\] <p>Python verification:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Function f(x) = x^2\ndef f(x):\n    return x**2\n\n# Derivative f'(x) = 2x\ndef f_prime(x):\n    return 2*x\n\n# Numerical derivative approximation\ndef numerical_derivative(f, x, h=1e-5):\n    return (f(x + h) - f(x)) / h\n\n# Test at x = 3\nx = 3\nanalytical = f_prime(x)\nnumerical = numerical_derivative(f, x)\n\nprint(f\"Analytical derivative at x={x}: {analytical}\")\nprint(f\"Numerical derivative at x={x}: {numerical:.6f}\")\n</code></pre> <p>Expected output:</p> <pre><code>Analytical derivative at x=3: 6\nNumerical derivative at x=3: 6.000001\n</code></pre>"},{"location":"02-math/calculus/#sum-rule-for-derivatives","title":"Sum Rule for Derivatives","text":"<p>When functions are added, their derivatives add.</p> <p>Sum rule:</p> \\[ \\frac{d}{dx}\\left[f(x) + g(x)\\right] = f'(x) + g'(x) \\] <p>Example:</p> \\[ h(x) = x^3 + 2x^2 + 5x + 7 \\] \\[ h'(x) = 3x^2 + 4x + 5 + 0 = 3x^2 + 4x + 5 \\] <p>ML connection: Loss functions are often sums of individual errors across data points:</p> \\[ L = \\sum_{i=1}^{n} L_i \\] <p>The derivative is:</p> \\[ \\frac{dL}{d\\theta} = \\sum_{i=1}^{n} \\frac{dL_i}{d\\theta} \\] <p>This is why we can compute gradients for mini-batches and sum them.</p>"},{"location":"02-math/calculus/#product-rule-for-derivatives","title":"Product Rule for Derivatives","text":"<p>When functions are multiplied, the derivative follows a specific pattern.</p> <p>Product rule:</p> \\[ \\frac{d}{dx}\\left[f(x) \\cdot g(x)\\right] = f'(x) \\cdot g(x) + f(x) \\cdot g'(x) \\] <p>Mnemonic: \"First times derivative of second, plus second times derivative of first.\"</p> <p>Example:</p> \\[ h(x) = x^2 \\cdot \\sin(x) \\] <p>Let $\\(f(x) = x^2\\)$ and $\\(g(x) = \\sin(x)\\)$.</p> \\[ h'(x) = 2x \\cdot \\sin(x) + x^2 \\cdot \\cos(x) \\] <p>Why not just $\\(f'(x) \\cdot g'(x)\\)$? Because multiplication is not a linear operation. The product rule accounts for how both functions change together.</p> <p>Python verification:</p> <pre><code>import numpy as np\n\n# Function h(x) = x^2 * e^x\ndef h(x):\n    return x**2 * np.exp(x)\n\n# Derivative using product rule: h'(x) = 2x * e^x + x^2 * e^x\ndef h_prime(x):\n    return 2*x * np.exp(x) + x**2 * np.exp(x)\n\n# Numerical verification\ndef numerical_derivative(f, x, h=1e-5):\n    return (f(x + h) - f(x)) / h\n\nx = 2\nanalytical = h_prime(x)\nnumerical = numerical_derivative(h, x)\n\nprint(f\"Analytical: {analytical:.6f}\")\nprint(f\"Numerical: {numerical:.6f}\")\n</code></pre> <p>Expected output:</p> <pre><code>Analytical: 33.556303\nNumerical: 33.556303\n</code></pre> <p>ML connection: Computing derivatives of combined transformations during optimization.</p>"},{"location":"02-math/calculus/#chain-rule-for-derivatives","title":"Chain Rule for Derivatives","text":"<p>The chain rule handles composite functions\u2014functions inside other functions. This is the most important derivative rule for neural networks.</p> <p>Chain rule:</p> \\[ \\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x) \\] <p>Or in Leibniz notation, if $\\(y = f(u)\\)$ and $\\(u = g(x)\\)$:</p> \\[ \\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} \\] <p>Intuition: \"How does $\\(y\\)$ change with $\\(x\\)$? First see how $\\(y\\)$ changes with $\\(u\\)$, then see how $\\(u\\)$ changes with $\\(x\\)$, and multiply.\"</p> <p>Example 1: $\\(h(x) = (x^2 + 1)^3\\)$</p> <p>Let $\\(u = x^2 + 1\\)$, so $\\(h = u^3\\)$.</p> \\[ \\frac{dh}{dx} = \\frac{dh}{du} \\cdot \\frac{du}{dx} = 3u^2 \\cdot 2x = 3(x^2 + 1)^2 \\cdot 2x = 6x(x^2 + 1)^2 \\] <p>Example 2: $\\(h(x) = e^{x^2}\\)$</p> <p>Let $\\(u = x^2\\)$, so $\\(h = e^u\\)$.</p> \\[ \\frac{dh}{dx} = e^u \\cdot 2x = 2x \\cdot e^{x^2} \\] <p>Python verification:</p> <pre><code>import numpy as np\n\n# Function h(x) = (x^2 + 1)^3\ndef h(x):\n    return (x**2 + 1)**3\n\n# Derivative using chain rule: h'(x) = 6x(x^2 + 1)^2\ndef h_prime(x):\n    return 6*x * (x**2 + 1)**2\n\n# Numerical verification\ndef numerical_derivative(f, x, h=1e-5):\n    return (f(x + h) - f(x)) / h\n\nx = 2\nanalytical = h_prime(x)\nnumerical = numerical_derivative(h, x)\n\nprint(f\"Analytical: {analytical}\")\nprint(f\"Numerical: {numerical:.6f}\")\n</code></pre> <p>Expected output:</p> <pre><code>Analytical: 300\nNumerical: 300.000030\n</code></pre> <p>ML connection: Backpropagation</p> <p>Neural networks are deeply nested composite functions. Backpropagation is just the chain rule applied layer by layer:</p> \\[ \\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial a_n} \\cdot \\frac{\\partial a_n}{\\partial a_{n-1}} \\cdot \\cdots \\cdot \\frac{\\partial a_2}{\\partial w_1} \\] <p>Each $\\(\\frac{\\partial a_i}{\\partial a_{i-1}}\\)$ is computed using the chain rule, propagating gradients backward through the network.</p>"},{"location":"02-math/calculus/#partial-derivatives","title":"Partial Derivatives","text":"<p>When functions have multiple variables, we use partial derivatives to see how the function changes with respect to one variable while keeping others constant.</p> <p>Notation:</p> \\[ \\frac{\\partial f}{\\partial x} \\quad \\text{(partial derivative of } f \\text{ with respect to } x\\text{)} \\] <p>How to compute: Treat all other variables as constants and differentiate normally.</p> <p>Example: $\\(f(x, y) = x^2y + 3xy^2\\)$</p> <p>Partial derivative with respect to $\\(x\\)$ (treat $\\(y\\)$ as constant):</p> \\[ \\frac{\\partial f}{\\partial x} = 2xy + 3y^2 \\] <p>Partial derivative with respect to $\\(y\\)$ (treat $\\(x\\)$ as constant):</p> \\[ \\frac{\\partial f}{\\partial y} = x^2 + 6xy \\] <p>Gradient vector:</p> <p>The gradient combines all partial derivatives into a vector:</p> \\[ \\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 2xy + 3y^2 \\\\ x^2 + 6xy \\end{bmatrix} \\] <p>Python implementation:</p> <pre><code>import numpy as np\n\n# Function f(x, y) = x^2 * y + 3*x*y^2\ndef f(x, y):\n    return x**2 * y + 3*x*y**2\n\n# Partial derivatives\ndef df_dx(x, y):\n    return 2*x*y + 3*y**2\n\ndef df_dy(x, y):\n    return x**2 + 6*x*y\n\n# Evaluate at (x=2, y=3)\nx, y = 2, 3\nprint(f\"f({x}, {y}) = {f(x, y)}\")\nprint(f\"\u2202f/\u2202x at ({x}, {y}) = {df_dx(x, y)}\")\nprint(f\"\u2202f/\u2202y at ({x}, {y}) = {df_dy(x, y)}\")\n\n# Gradient vector\ngradient = np.array([df_dx(x, y), df_dy(x, y)])\nprint(f\"Gradient \u2207f at ({x}, {y}) = {gradient}\")\n</code></pre> <p>Expected output:</p> <pre><code>f(2, 3) = 66\n\u2202f/\u2202x at (2, 3) = 39\n\u2202f/\u2202y at (2, 3) = 40\nGradient \u2207f at (2, 3) = [39 40]\n</code></pre> <p>ML connection: Gradient descent computes partial derivatives of the loss function with respect to each model parameter:</p> \\[ w_{\\text{new}} = w_{\\text{old}} - \\alpha \\frac{\\partial L}{\\partial w} \\] <p>Each parameter is updated based on its partial derivative.</p>"},{"location":"02-math/calculus/#what-is-an-integral","title":"What is an Integral?","text":"<p>An integral is the reverse of a derivative. While derivatives measure rates of change, integrals measure accumulation or total area under a curve.</p> <p>Two types:</p> <ol> <li>Indefinite integral (antiderivative): Find a function whose derivative gives you the original function</li> <li>Definite integral: Calculate the actual area under the curve between two points</li> </ol> <p>Notation:</p> \\[ \\int f(x) \\, dx \\] <p>The $\\(\\int\\)$ symbol means \"integrate,\" and $\\(dx\\)$ indicates we're integrating with respect to $\\(x\\)$.</p> <p></p> <p>Why integrals matter in ML:</p> <ul> <li>Computing probabilities (area under probability density functions)</li> <li>Calculating expected values</li> <li>Optimization (finding cumulative sums)</li> <li>Analyzing convergence</li> </ul>"},{"location":"02-math/calculus/#indefinite-integrals","title":"Indefinite Integrals","text":"<p>An indefinite integral finds the antiderivative\u2014a function $\\(F(x)\\)$ such that $\\(F'(x) = f(x)\\)$.</p> <p>General form:</p> \\[ \\int f(x) \\, dx = F(x) + C \\] <p>The $\\(+ C\\)$ is the constant of integration (because derivatives of constants are zero, we can't determine $\\(C\\)$ from integration alone).</p> <p>Basic integration rules:</p> <p>Constant:</p> \\[ \\int a \\, dx = ax + C \\] <p>Power rule:</p> \\[ \\int x^n \\, dx = \\frac{x^{n+1}}{n+1} + C \\quad (n \\neq -1) \\] <p>Examples:</p> <ul> <li> \\[\\int x^2 \\, dx = \\frac{x^3}{3} + C\\] </li> <li> \\[\\int x^5 \\, dx = \\frac{x^6}{6} + C\\] </li> <li> \\[\\int 1 \\, dx = x + C\\] </li> </ul> <p>Reciprocal (special case when $\\(n = -1\\)$):</p> \\[ \\int \\frac{1}{x} \\, dx = \\ln|x| + C \\] <p>Exponential:</p> \\[ \\int e^x \\, dx = e^x + C \\] <p>Verification with derivatives:</p> <pre><code>import sympy as sp\n\n# Define variable\nx = sp.Symbol('x')\n\n# Function to integrate\nf = x**2\n\n# Compute indefinite integral\nF = sp.integrate(f, x)\nprint(f\"\u222b{f} dx = {F} + C\")\n\n# Verify by taking derivative\nderivative = sp.diff(F, x)\nprint(f\"Derivative of {F}: {derivative}\")\nprint(f\"Matches original? {derivative == f}\")\n</code></pre> <p>Expected output:</p> <pre><code>\u222bx**2 dx = x**3/3 + C\nDerivative of x**3/3: x**2\nMatches original? True\n</code></pre>"},{"location":"02-math/calculus/#definite-integrals","title":"Definite Integrals","text":"<p>A definite integral computes the exact area under a curve between two limits $\\(a\\)$ and $\\(b\\)$.</p> <p>Notation:</p> \\[ \\int_a^b f(x) \\, dx \\] <p>Fundamental Theorem of Calculus:</p> \\[ \\int_a^b f(x) \\, dx = F(b) - F(a) \\] <p>Where $\\(F(x)\\)$ is any antiderivative of $\\(f(x)\\)$.</p> <p>Example: $\\(\\int_0^2 x^2 \\, dx\\)$</p> <ol> <li>Find antiderivative: $\\(F(x) = \\frac{x^3}{3}\\)$</li> <li>Evaluate at limits: $\\(F(2) - F(0) = \\frac{2^3}{3} - \\frac{0^3}{3} = \\frac{8}{3} - 0 = \\frac{8}{3}\\)$</li> </ol> <p>So $\\(\\int_0^2 x^2 \\, dx = \\frac{8}{3} \\approx 2.667\\)$.</p> <p>Python verification:</p> <pre><code>import numpy as np\nfrom scipy import integrate\n\n# Function to integrate\ndef f(x):\n    return x**2\n\n# Definite integral from 0 to 2\nresult, error = integrate.quad(f, 0, 2)\nprint(f\"\u222b\u2080\u00b2 x\u00b2 dx = {result:.6f}\")\n\n# Analytical solution\nanalytical = (2**3)/3 - (0**3)/3\nprint(f\"Analytical result: {analytical:.6f}\")\n</code></pre> <p>Expected output:</p> <pre><code>\u222b\u2080\u00b2 x\u00b2 dx = 2.666667\nAnalytical result: 2.666667\n</code></pre> <p>ML connection: Probability</p> <p>Probability density functions (PDFs) integrate to 1 over their domain:</p> \\[ \\int_{-\\infty}^{\\infty} p(x) \\, dx = 1 \\] <p>To find the probability that $\\(X\\)$ falls in range $\\([a, b]\\)$:</p> \\[ P(a \\leq X \\leq b) = \\int_a^b p(x) \\, dx \\]"},{"location":"02-math/calculus/#double-integrals","title":"Double Integrals","text":"<p>Double integrals extend integration to functions of two variables, computing volume under a surface.</p> <p>Notation:</p> \\[ \\iint_R f(x, y) \\, dA = \\int_a^b \\int_c^d f(x, y) \\, dy \\, dx \\] <p>How to compute:</p> <ol> <li>Integrate with respect to $\\(y\\)$ first (treat $\\(x\\)$ as constant)</li> <li>Then integrate the result with respect to $\\(x\\)$</li> </ol> <p>Example: $\\(\\int_0^1 \\int_0^2 xy \\, dy \\, dx\\)$</p> <p>Step 1: Inner integral (with respect to $\\(y\\)$):</p> \\[ \\int_0^2 xy \\, dy = x \\left[\\frac{y^2}{2}\\right]_0^2 = x \\cdot \\frac{4}{2} = 2x \\] <p>Step 2: Outer integral (with respect to $\\(x\\)$):</p> \\[ \\int_0^1 2x \\, dx = 2 \\left[\\frac{x^2}{2}\\right]_0^1 = 2 \\cdot \\frac{1}{2} = 1 \\] <p>So $\\(\\int_0^1 \\int_0^2 xy \\, dy \\, dx = 1\\)$.</p> <p>Python verification:</p> <pre><code>from scipy import integrate\n\n# Function f(x, y) = x*y\ndef f(y, x):  # Note: order is reversed for dblquad\n    return x * y\n\n# Double integral\nresult, error = integrate.dblquad(f, 0, 1, 0, 2)\nprint(f\"\u222b\u222b xy dy dx = {result:.6f}\")\n</code></pre> <p>Expected output:</p> <pre><code>\u222b\u222b xy dy dx = 1.000000\n</code></pre> <p>ML connection: Expected values of functions over joint probability distributions use double integrals:</p> \\[ E[g(X, Y)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x, y) p(x, y) \\, dx \\, dy \\]"},{"location":"02-math/calculus/#quick-reference-for-calculus-operations","title":"Quick Reference for Calculus Operations","text":"Concept Notation Formula/Rule ML Application Summation $\\(\\sum_{i=1}^n a_i\\)$ $\\(a_1 + a_2 + \\cdots + a_n\\)$ Loss functions, batch gradients Product $\\(\\prod_{i=1}^n a_i\\)$ $\\(a_1 \\times a_2 \\times \\cdots \\times a_n\\)$ Probability of independent events Derivative $\\(f'(x)\\)$ or $\\(\\frac{df}{dx}\\)$ $\\(\\lim_{h \\to 0} \\frac{f(x+h)-f(x)}{h}\\)$ Gradient descent Power rule $\\(\\frac{d}{dx}x^n\\)$ $\\(nx^{n-1}\\)$ Computing gradients Sum rule $\\(\\frac{d}{dx}[f+g]\\)$ $\\(f' + g'\\)$ Additive loss functions Product rule $\\(\\frac{d}{dx}[f \\cdot g]\\)$ $\\(f'g + fg'\\)$ Combined transformations Chain rule $\\(\\frac{d}{dx}f(g(x))\\)$ $\\(f'(g(x)) \\cdot g'(x)\\)$ Backpropagation Partial derivative $\\(\\frac{\\partial f}{\\partial x}\\)$ Derivative treating other vars constant Multi-parameter optimization Indefinite integral $\\(\\int f(x) \\, dx\\)$ $\\(F(x) + C\\)$ where $\\(F' = f\\)$ Finding antiderivatives Definite integral $\\(\\int_a^b f(x) \\, dx\\)$ $\\(F(b) - F(a)\\)$ Probability calculations"},{"location":"02-math/calculus/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned summation and product notation for expressing series, understood derivatives as rates of change and their computation rules, mastered the chain rule essential for backpropagation, computed partial derivatives for multivariate optimization, and understood integrals for probability and accumulation.</p> <p>Best practices:</p> <ul> <li>Verify derivatives numerically when implementing custom functions</li> <li>Use automatic differentiation (like PyTorch's autograd) in production, but understand the math</li> <li>Check dimensions when computing gradients to catch errors early</li> <li>Visualize functions and their derivatives to build geometric intuition</li> <li>Start with simple examples before tackling complex nested functions</li> </ul> <p>Connections to ML:</p> <ul> <li>Gradient descent: Uses derivatives to find parameter updates: $\\(w := w - \\alpha \\frac{\\partial L}{\\partial w}\\)$</li> <li>Backpropagation: Applies chain rule layer-by-layer to compute gradients in neural networks</li> <li>Loss functions: Derivatives show which direction reduces error</li> <li>Probability: Integrals compute cumulative distributions and expected values</li> <li>Optimization: Second derivatives (curvature) help adaptive learning rate methods</li> </ul> <p>External resources:</p> <ul> <li>3Blue1Brown: Essence of Calculus - visual and intuitive calculus explanations</li> <li>Khan Academy: Calculus - comprehensive practice problems</li> <li>MIT OpenCourseWare: Single Variable Calculus - rigorous mathematical foundation</li> </ul> <p>Next tutorial: Apply these calculus concepts to understand gradient descent optimization and implement a simple neural network from scratch using derivatives.</p>"},{"location":"02-math/linear-algebra/","title":"Introduction to Linear Algebra for Machine Learning","text":"<p>This tutorial introduces the fundamental concepts of linear algebra that form the mathematical foundation of machine learning. You'll build geometric intuition for vectors and matrices, understand key operations, and implement them in Python to solidify your understanding.</p> <p>Estimated time: 60 minutes</p>"},{"location":"02-math/linear-algebra/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: Machine learning algorithms operate on numerical data represented as vectors and matrices. Without understanding linear algebra fundamentals, it's impossible to grasp how models process data, optimize parameters, or make predictions.</p> <p>Practical benefits: Linear algebra provides the mathematical framework to represent datasets, transform features, and understand model internals. Mastering these concepts enables you to implement ML algorithms from scratch, debug model behavior, optimize performance, and understand research papers describing new architectures.</p> <p>Professional context: Every ML practitioner uses linear algebra daily. Linear regression uses matrix operations to find optimal coefficients, Principal Component Analysis (PCA) relies on eigenvalues for dimensionality reduction, Convolutional Neural Networks perform matrix multiplications and convolutions, and Support Vector Machines use linear algebra to find optimal decision boundaries.</p> <p></p>"},{"location":"02-math/linear-algebra/#core-concepts","title":"Core Concepts","text":""},{"location":"02-math/linear-algebra/#what-is-linear-algebra","title":"What is Linear Algebra?","text":"<p>Linear algebra is the branch of mathematics dealing with vectors, vector spaces, and linear transformations. It provides the language and tools to work with multidimensional data and transformations.</p> <p>Why \"linear\"? Linear algebra studies operations that preserve vector addition and scalar multiplication. These operations can be visualized as transformations that keep grid lines parallel and evenly spaced.</p> <p>Key insight: Every ML model learns a transformation from input space to output space. Linear algebra gives us the tools to understand and manipulate these transformations.</p>"},{"location":"02-math/linear-algebra/#key-terminology","title":"Key Terminology","text":"<p>Before learning about operations, let's establish essential vocabulary:</p> <p>Scalar: A single number (e.g., 5, -3.14, 0.5)</p> <p>Vector: An ordered collection of numbers representing magnitude and direction. Can be thought of as</p> <ul> <li>An arrow in space (geometric view)</li> <li>A list of coordinates (computer science view)</li> <li>A point in <code>n</code>-dimensional space (mathematical view)</li> </ul> <p>Matrix: A rectangular array of numbers arranged in rows and columns. Represents:</p> <ul> <li>A linear transformation (e.g., rotation, scaling, shearing)</li> <li>A collection of vectors (each row or column is a vector)</li> <li>A dataset (<code>rows = samples</code>, <code>columns = features</code>)</li> </ul> <p>Dimension: The number of components in a vector, or the size of the space it lives in</p> <p>Linear transformation: An operation that transforms vectors while preserving lines and the origin</p>"},{"location":"02-math/linear-algebra/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"02-math/linear-algebra/#1-understanding-vectors","title":"1. Understanding Vectors","text":"<p>A vector is the fundamental building block of linear algebra. Think of it as an arrow pointing from the origin to a specific point in space.</p> <p></p> <p>Three perspectives on vectors:</p> <p>Physics perspective: A vector has magnitude (length) and direction. For example, velocity or force.</p> <p>Computer Science perspective: A vector is simply an ordered list of numbers: <code>[1][2][3][4][5]</code>.</p> <p>Mathematics perspective: A vector represents a point in <code>n</code>-dimensional space, where each number is a coordinate along one axis.</p> <p>Why vectors matter in ML: Every data point is a vector. A customer's age, income, and purchase history form a <code>3D</code> vector. An image with <code>1000</code> pixels is a <code>1000</code>-dimensional vector.</p> <p>Create a vector in Python:</p> <pre><code># Create a 5-dimensional vector\nmy_vector = [1, 2, 3, 4, 5]\n\n# Print the vector\nprint(my_vector)\nprint(f\"This vector has {len(my_vector)} dimensions\")\n</code></pre> <p>Expected output: <pre><code>[1, 2, 3, 4, 5]\nThis vector has 5 dimensions\n</code></pre></p>"},{"location":"02-math/linear-algebra/#2-vector-operations","title":"2. Vector Operations","text":"<p>There are several key operations which can be done with vectors.</p> <p>Vectors can be combined and scaled using fundamental operations that have geometric interpretations.</p>"},{"location":"02-math/linear-algebra/#vector-addition","title":"Vector Addition","text":"<p>Adding two vectors $\\(\\mathbf{v_1} + \\mathbf{v_2}\\)$ means adding corresponding components element-wise. Geometrically, place the tail of $\\(\\mathbf{v_2}\\)$ at the head of $\\(\\mathbf{v_1}\\)$.</p> <p>Why it matters: Gradient descent updates parameters by adding the gradient vector (scaled) to the current parameter vector.</p> <p></p> <p>Implement vector addition:</p> <pre><code>def add_vectors(vec1, vec2):\n    \"\"\"\n    Add two vectors element-wise.\n    Returns None if vectors have different lengths.\n    \"\"\"\n    if len(vec1) != len(vec2):\n        print(\"Error: Vectors must have the same dimension\")\n        return None\n\n    result = []\n    for i in range(len(vec1)):\n        result.append(vec1[i] + vec2[i])\n\n    return result\n\n# Example usage\nv1 = [1, 2, 3]\nv2 = [4, 5, 6]\nresult = add_vectors(v1, v2)\nprint(f\"{v1} + {v2} = {result}\")\n</code></pre> <p>Expected output: <pre><code>[1, 2, 3] + [4, 5, 6] = [5, 7, 9]\n</code></pre></p>"},{"location":"02-math/linear-algebra/#vector-subtraction","title":"Vector Subtraction","text":"<p>Subtracting $\\(\\mathbf{v_2}\\)$ from $\\(\\mathbf{v_1}\\)$ gives the vector pointing from $\\(\\mathbf{v_2}\\)$ to $\\(\\mathbf{v_1}\\)$.</p> <pre><code>def subtract_vectors(vec1, vec2):\n    \"\"\"Subtract vec2 from vec1 element-wise.\"\"\"\n    if len(vec1) != len(vec2):\n        print(\"Error: Vectors must have the same dimension\")\n        return None\n\n    result = []\n    for i in range(len(vec1)):\n        result.append(vec1[i] - vec2[i])\n\n    return result\n\n# Example usage\nv1 = [10, 8, 6]\nv2 = [1, 2, 3]\nresult = subtract_vectors(v1, v2)\nprint(f\"{v1} - {v2} = {result}\")\n</code></pre> <p>Expected output: <pre><code>[10, 8, 6] - [1, 2, 3] = [9, 6, 3]\n</code></pre></p>"},{"location":"02-math/linear-algebra/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>Multiplying a vector by a scalar $\\(c\\)$ scales its length by $\\(|c|\\)$ and reverses its direction if $\\(c &lt; 0\\)$.</p> <p></p> <pre><code>def scalar_multiply(scalar, vec):\n    \"\"\"Multiply every element of the vector by a scalar.\"\"\"\n    result = []\n    for element in vec:\n        result.append(scalar * element)\n\n    return result\n\n# Example usage\nv = [1, 2, 3]\nprint(f\"2 * {v} = {scalar_multiply(2, v)}\")\nprint(f\"0.5 * {v} = {scalar_multiply(0.5, v)}\")\nprint(f\"-1 * {v} = {scalar_multiply(-1, v)}\")\n</code></pre> <p>Expected output: <pre><code>2 * [1, 2, 3] = [2, 4, 6]\n0.5 * [1, 2, 3] = [0.5, 1.0, 1.5]\n-1 * [1, 2, 3] = [-1, -2, -3]\n</code></pre></p>"},{"location":"02-math/linear-algebra/#dot-product","title":"Dot Product","text":"<p>The dot product of two vectors $\\(\\mathbf{v_1} \\cdot \\mathbf{v_2}\\)$ is the sum of the products of corresponding components: $\\(\\sum_{i} v_{1i} \\times v_{2i}\\)$.</p> <p>Geometric interpretation: $\\(\\mathbf{v_1} \\cdot \\mathbf{v_2} = |\\mathbf{v_1}| \\times |\\mathbf{v_2}| \\times \\cos(\\theta)\\)$, where $\\(\\theta\\)$ is the angle between the vectors.</p> <p>Why it matters: The dot product measures similarity. In ML, cosine similarity for text embeddings relies on the dot product. Neural networks compute dot products between inputs and weights.</p> <p></p> <pre><code>def dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Returns the scalar result.\n    \"\"\"\n    if len(vec1) != len(vec2):\n        print(\"Error: Vectors must have the same dimension\")\n        return None\n\n    result = 0\n    for i in range(len(vec1)):\n        result += vec1[i] * vec2[i]\n\n    return result\n\n# Example usage\nv1 = [1, 2, 3]\nv2 = [4, 5, 6]\nresult = dot_product(v1, v2)\nprint(f\"{v1} \u00b7 {v2} = {result}\")\n# Calculation: (1*4) + (2*5) + (3*6) = 4 + 10 + 18 = 32\n</code></pre> <p>Expected output: <pre><code>[1, 2, 3] \u00b7 [4, 5, 6] = 32\n</code></pre></p>"},{"location":"02-math/linear-algebra/#3-understanding-matrices","title":"3. Understanding Matrices","text":"<p>A matrix is a rectangular grid of numbers. Think of it as organizing multiple vectors together, or as representing a linear transformation.</p> <p>Key perspectives:</p> <p>Data representation: Each row is a data sample, each column is a feature. A dataset with <code>100</code> customers and <code>5</code> features is a $\\(100 \\times 5\\)$ matrix.</p> <p>Linear transformation: A matrix transforms input vectors to output vectors. Rotation, scaling, and shearing are all matrix operations.</p> <p>Collection of vectors: Each row (or column) can be viewed as a separate vector.</p> <p>Matrix notation: A matrix with $\\(m\\)$ rows and $\\(n\\)$ columns is called an $\\(m \\times n\\)$ matrix. Element at row $\\(i\\)$, column $\\(j\\)$ is denoted $\\(A_{ij}\\)$.</p> \\[ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{bmatrix} \\] <p>Create a matrix in Python:</p> <pre><code># Create a 3x3 matrix\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\n# Print the matrix\nfor row in matrix:\n    print(row)\n</code></pre> <p>Expected output: <pre><code>[1, 2, 3]\n[4, 5, 6]\n[7, 8, 9]\n</code></pre></p>"},{"location":"02-math/linear-algebra/#4-matrix-shape-and-dimensions","title":"4. Matrix Shape and Dimensions","text":"<p>Understanding matrix shape is critical for ensuring operations are valid.</p> <p>Get matrix shape:</p> <pre><code>def get_matrix_shape(matrix):\n    \"\"\"\n    Return the shape of a matrix as (rows, columns).\n    \"\"\"\n    num_rows = len(matrix)\n    num_cols = len(matrix[0]) if num_rows &gt; 0 else 0\n    return (num_rows, num_cols)\n\n# Example usage\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nshape = get_matrix_shape(matrix)\nprint(f\"Matrix shape: {shape[0]} rows \u00d7 {shape[1]} columns\")\n</code></pre> <p>Expected output: <pre><code>Matrix shape: 3 rows \u00d7 3 columns\n</code></pre></p>"},{"location":"02-math/linear-algebra/#step-5-matrix-indexing-and-slicing","title":"Step 5: Matrix Indexing and Slicing","text":"<p>Accessing specific elements or submatrices is essential for feature extraction and batch processing.</p> <p>Access individual elements:</p> <pre><code>matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n# Access element at row 0, column 0 (first element)\nprint(f\"Element [0][0]: {matrix[0][0]}\")\n\n# Access element at row 1, column 2\nprint(f\"Element [1][2]: {matrix[1][2]}\")\n\n# Access element at row 2, column 1\nprint(f\"Element [2][1]: {matrix[2][1]}\")\n</code></pre> <p>Expected output: <pre><code>Element [0][0]: 1\nElement [1][2]: 6\nElement [2][1]: 8\n</code></pre></p> <p>Extract rows and columns:</p> <pre><code>def get_row(matrix, row_index):\n    \"\"\"Extract a specific row from the matrix.\"\"\"\n    return matrix[row_index]\n\ndef get_column(matrix, col_index):\n    \"\"\"Extract a specific column from the matrix.\"\"\"\n    column = []\n    for row in matrix:\n        column.append(row[col_index])\n    return column\n\n# Example usage\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(f\"Row 1: {get_row(matrix, 1)}\")\nprint(f\"Column 2: {get_column(matrix, 2)}\")\n</code></pre> <p>Expected output: <pre><code>Row 1: [4, 5, 6]\nColumn 2: [3, 6, 9]\n</code></pre></p> <p>Extract submatrices:</p> <pre><code>def get_submatrix(matrix, row_start, row_end, col_start, col_end):\n    \"\"\"\n    Extract a submatrix from row_start to row_end (exclusive)\n    and col_start to col_end (exclusive).\n    \"\"\"\n    submatrix = []\n    for i in range(row_start, row_end):\n        row = []\n        for j in range(col_start, col_end):\n            row.append(matrix[i][j])\n        submatrix.append(row)\n    return submatrix\n\n# Example: Extract bottom-right 2x2 submatrix\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nsub = get_submatrix(matrix, 1, 3, 1, 3)\nprint(\"Bottom-right 2\u00d72 submatrix:\")\nfor row in sub:\n    print(row)\n</code></pre> <p>Expected output: <pre><code>Bottom-right 2\u00d72 submatrix:\n[5, 6]\n[8, 9]\n</code></pre></p>"},{"location":"02-math/linear-algebra/#6-matrix-addition-and-subtraction","title":"6. Matrix Addition and Subtraction","text":"<p>Matrices of the same shape can be added or subtracted element-wise.</p> <p></p> <p>Why it matters: Model ensembling often involves averaging predictions (matrix addition). Residuals in regression are computed via matrix subtraction.</p> <pre><code>def add_matrices(mat1, mat2):\n    \"\"\"\n    Add two matrices element-wise.\n    Returns None if shapes don't match.\n    \"\"\"\n    if len(mat1) != len(mat2) or len(mat1[0]) != len(mat2[0]):\n        print(\"Error: Matrices must have the same shape\")\n        return None\n\n    result = []\n    for i in range(len(mat1)):\n        row = []\n        for j in range(len(mat1[0])):\n            row.append(mat1[i][j] + mat2[i][j])\n        result.append(row)\n\n    return result\n\n# Example usage\nmat1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nmat2 = [[9, 8, 7], [6, 5, 4], [3, 2, 1]]\nresult = add_matrices(mat1, mat2)\n\nprint(\"Matrix 1 + Matrix 2:\")\nfor row in result:\n    print(row)\n</code></pre> <p>Expected output: <pre><code>Matrix 1 + Matrix 2:\n[10, 10, 10]\n[10, 10, 10]\n[10, 10, 10]\n</code></pre></p>"},{"location":"02-math/linear-algebra/#7-matrix-multiplication","title":"7.  Matrix Multiplication","text":"<p>Matrix multiplication is the most important operation in linear algebra for ML. Unlike addition, it's not element-wise.</p> <p>Rules: To multiply matrix $\\(A\\)$ (shape $\\(m \\times n\\)$) by matrix $\\(B\\)$ (shape $\\(n \\times p\\)$), the number of columns in $\\(A\\)$ must equal the number of rows in $\\(B\\)$. The result is an $\\(m \\times p\\)$ matrix.</p> <p>Geometric interpretation: Matrix multiplication represents composing linear transformations. If $\\(A\\)$ rotates and $\\(B\\)$ scales, then $\\(AB\\)$ first scales then rotates.</p> <p>Why it matters: Every layer in a neural network performs matrix multiplication: $\\(output = weight\\_matrix \\times input + bias\\)$. Understanding this is essential for deep learning.</p> <p></p> <p>Formula: Element $\\(C_{ij}\\)$ in the result is the dot product of row $\\(i\\)$ from $\\(A\\)$ and column $\\(j\\)$ from $\\(B\\)$.</p> \\[ C_{ij} = \\sum_{k=1}^{n} A_{ik} \\times B_{kj} \\] <pre><code>def matrix_multiply(mat1, mat2):\n    \"\"\"\n    Multiply two matrices using the standard algorithm.\n    Returns None if dimensions are incompatible.\n    \"\"\"\n    m1_rows = len(mat1)\n    m1_cols = len(mat1[0])\n    m2_rows = len(mat2)\n    m2_cols = len(mat2[0])\n\n    # Check compatibility\n    if m1_cols != m2_rows:\n        print(f\"Error: Cannot multiply {m1_rows}\u00d7{m1_cols} by {m2_rows}\u00d7{m2_cols}\")\n        return None\n\n    # Initialize result matrix with zeros\n    result = [[0 for _ in range(m2_cols)] for _ in range(m1_rows)]\n\n    # Compute each element as dot product of row and column\n    for i in range(m1_rows):\n        for j in range(m2_cols):\n            dot_product = 0\n            for k in range(m1_cols):\n                dot_product += mat1[i][k] * mat2[k][j]\n            result[i][j] = dot_product\n\n    return result\n\n# Example usage\nA = [[1, 2, 3], [4, 5, 6]]  # 2\u00d73 matrix\nB = [[7, 8], [9, 10], [11, 12]]  # 3\u00d72 matrix\nresult = matrix_multiply(A, B)  # Result will be 2\u00d72\n\nprint(\"A \u00d7 B =\")\nfor row in result:\n    print(row)\n</code></pre> <p>Expected output: <pre><code>A \u00d7 B =\n[58, 64]\n[139, 154]\n</code></pre></p> <p>How it works:</p> <ul> <li>First row of $\\(A\\)$: <code>[1][2][3]</code>, first column of $\\(B\\)$: <code>[7][9][11]</code> \u2192 $\\(1\u00d77 + 2\u00d79 + 3\u00d711 = 58\\)$</li> <li>First row of $\\(A\\)$: <code>[1][2][3]</code>, second column of $\\(B\\)$: <code>[8][10][12]</code> \u2192 $\\(1\u00d78 + 2\u00d710 + 3\u00d712 = 64\\)$</li> </ul>"},{"location":"02-math/linear-algebra/#8-transpose-operation","title":"8. Transpose Operation","text":"<p>The transpose of a matrix flips it over its diagonal, swapping rows and columns.</p> <p>Notation: $\\(A^T\\)$ denotes the transpose of matrix $\\(A\\)$.</p> <p>Why it matters: Transposes are used to compute covariance matrices, in backpropagation for neural networks, and to ensure dimension compatibility for matrix multiplication.</p> <p></p> <pre><code>def transpose_matrix(matrix):\n    \"\"\"\n    Return the transpose of a matrix.\n    Rows become columns and columns become rows.\n    \"\"\"\n    rows = len(matrix)\n    cols = len(matrix[0])\n\n    # Create result matrix with swapped dimensions\n    result = [[0 for _ in range(rows)] for _ in range(cols)]\n\n    for i in range(rows):\n        for j in range(cols):\n            result[j][i] = matrix[i][j]\n\n    return result\n\n# Example usage\nmatrix = [[1, 2, 3], [4, 5, 6]]\nprint(\"Original matrix (2\u00d73):\")\nfor row in matrix:\n    print(row)\n\ntransposed = transpose_matrix(matrix)\nprint(\"\\nTransposed matrix (3\u00d72):\")\nfor row in transposed:\n    print(row)\n</code></pre> <p>Expected output: <pre><code>Original matrix (2\u00d73):\n[1, 2, 3]\n[4, 5, 6]\n\nTransposed matrix (3\u00d72):\n[1, 4]\n[2, 5]\n[3, 6]\n</code></pre></p>"},{"location":"02-math/linear-algebra/#quick-reference-for-linear-algebra-operations","title":"Quick Reference for Linear Algebra Operations","text":"Operation Formula Python Implementation ML Application Vector Addition $\\(\\mathbf{v_1} + \\mathbf{v_2}\\)$ Element-wise sum Gradient updates Scalar Multiplication $\\(c \\cdot \\mathbf{v}\\)$ Multiply each element by $\\(c\\)$ Learning rate scaling Dot Product $\\(\\mathbf{v_1} \\cdot \\mathbf{v_2} = \\sum v_{1i}v_{2i}\\)$ Sum of element products Similarity measures Matrix Addition $\\(A + B\\)$ Element-wise sum Model ensembling Matrix Multiplication $\\(AB\\)$, where $\\(C_{ij} = \\sum A_{ik}B_{kj}\\)$ Nested loops Neural network layers Transpose $\\(A^T\\)$, swap rows/columns Flip across diagonal Backpropagation"},{"location":"02-math/linear-algebra/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've built geometric intuition for vectors and matrices, learned the fundamental operations of linear algebra, implemented these operations in pure Python, and connected each concept to machine learning applications.</p> <p>Best practices:</p> <ul> <li>Visualize operations geometrically before implementing them algorithmically</li> <li>Check dimensions carefully before matrix operations to avoid errors</li> <li>Understand the why behind each operation, not just the mechanics</li> <li>Practice with small examples (2D, 3D) before scaling to high dimensions</li> </ul> <p>Connections to ML:</p> <ul> <li>Linear regression: Solve $\\(\\mathbf{w} = (X^TX)^{-1}X^T\\mathbf{y}\\)$ using matrix operations</li> <li>Neural networks: Each layer computes $\\(\\mathbf{h} = \\sigma(W\\mathbf{x} + \\mathbf{b})\\)$</li> <li>PCA: Find eigenvectors of covariance matrix $\\(C = \\frac{1}{n}X^TX\\)$</li> <li>SVMs: Solve optimization problems using matrix formulations</li> </ul>"},{"location":"02-math/linear-algebra/#external-resources","title":"External resources","text":"<ul> <li>3Blue1Brown: Essence of Linear Algebra - visual and intuitive explanations</li> <li>Interactive Linear Algebra (Georgia Tech) - free interactive online textbook with visualizations and exercises</li> </ul> <p>Next tutorial: Learn to implement these operations efficiently using NumPy, Python's numerical computing library optimized for array operations.</p>"},{"location":"02-math/numpy/","title":"NumPy for Machine Learning: Efficient Array Operations","text":"<p>This tutorial teaches you to use NumPy, Python's fundamental library for numerical computing. You'll learn to create, manipulate, and perform operations on arrays efficiently - skills essential for every machine learning task.</p> <p>Estimated time: 50 minutes</p>"},{"location":"02-math/numpy/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: </p> <p>Pure Python lists and loops are too slow for machine learning workloads involving millions of data points. </p> <p>Without NumPy's optimized array operations, training even simple models becomes impractically slow, and implementing ML algorithms from scratch is unnecessarily complex.</p> <p>Practical benefits: NumPy provides extremely fast array operations through optimized C and Fortran libraries underneath. What takes seconds with Python loops completes in milliseconds with NumPy. </p> <p>Every major ML library\u2014scikit-learn, TensorFlow, PyTorch, pandas - is built on top of NumPy, making it the universal language of numerical computing in Python.</p> <p>Professional context: NumPy is non-negotiable for ML work. Data preprocessing uses NumPy arrays, model inputs are NumPy arrays, predictions return as NumPy arrays. Understanding NumPy array operations, broadcasting, and vectorization enables you to write production-quality ML code and understand how libraries work under the hood.</p>"},{"location":"02-math/numpy/#core-concepts","title":"Core Concepts","text":""},{"location":"02-math/numpy/#what-is-numpy","title":"What is NumPy?","text":"<p>NumPy (Numerical Python) is the foundational library for scientific computing in Python. It provides a powerful <code>N</code>-dimensional array object and functions for fast operations on arrays.</p> <p>Why NumPy is fast: NumPy operations are implemented in C and execute at compiled speeds, not interpreted Python speeds. </p> <p>A NumPy operation on <code>1</code> million elements can be <code>100x</code> faster than an equivalent Python loop.</p> <p>Key insight: The secret to NumPy's power is vectorization; applying operations to entire arrays at once instead of looping through elements. This shift in thinking from <code>loop over each element</code> to <code>operate on the whole array</code> is fundamental to efficient ML code.</p>"},{"location":"02-math/numpy/#arrays-vs-lists","title":"Arrays vs Lists","text":"Feature Python List NumPy Array Speed Slow (interpreted) Fast (compiled C) Memory More overhead Compact, efficient Data types Mixed types allowed Homogeneous (one type) Operations Manual loops needed Vectorized operations Use case General programming Numerical computing <p>When to use NumPy: Anytime you're working with numerical data, especially large datasets, mathematical operations, or preparing data for ML models.</p>"},{"location":"02-math/numpy/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"02-math/numpy/#1-installing-and-importing-numpy","title":"1.  Installing and Importing NumPy","text":"<p>NumPy must be installed and imported before use.</p> <p>Install NumPy:</p> <pre><code>pip install numpy\n</code></pre> <p>Import NumPy (standard convention):</p> <pre><code>import numpy as np\n</code></pre> <p>Why <code>np</code>: The alias <code>np</code> is universally used in the Python data science community, making code readable and recognizable across projects.</p> <p>Verify installation:</p> <pre><code>import numpy as np\nprint(np.__version__)\n</code></pre> <p>Expected output:</p> <pre><code>1.24.3 # or a similar version\n</code></pre>"},{"location":"02-math/numpy/#2-creating-numpy-arrays-from-lists","title":"2. Creating NumPy Arrays from Lists","text":"<p>The simplest way to create arrays is by converting Python lists.</p> <p>Create a 1D array (vector):</p> <pre><code># From a simple list\nmy_list = [1, 2, 3, 4, 5]\nvector = np.array(my_list)\n\nprint(vector)\nprint(f\"Type: {type(vector)}\")\nprint(f\"Shape: {vector.shape}\")\n</code></pre> <p>Expected output:</p> <pre><code>[1 2 3 4 5]\nType: lass 'numpy.ndarrayay'&gt;\nShape: (5,)\n</code></pre> <p>Why shape matters: The shape <code>(5,)</code> indicates a 1D array with 5 elements. Understanding shapes is critical for ensuring array operations are compatible.</p> <p>Create a 2D array (matrix):</p> <pre><code># From a nested list\nmy_matrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\nmatrix = np.array(my_matrix)\nprint(matrix)\nprint(f\"Shape: {matrix.shape}\")\n</code></pre> <p>Expected output:</p> <pre><code>[[1 2 3]\n [4 5 6]\n [7 8 9]]\nShape: (3, 3)\n</code></pre> <p>Interpretation: A shape of <code>(3, 3)</code> means 3 rows and 3 columns. In ML terminology, this could represent 3 data samples with 3 features each.</p>"},{"location":"02-math/numpy/#3-built-in-array-creation-functions","title":"3. Built-in Array Creation Functions","text":"<p>NumPy provides convenient functions for generating arrays without manually typing values.</p>"},{"location":"02-math/numpy/#arange-evenly-spaced-values","title":"arange: Evenly Spaced Values","text":"<p>Similar to Python's <code>range()</code>, but returns a NumPy array.</p> <pre><code># Create array from 0 to 10 (exclusive)\narr = np.arange(0, 11)\nprint(arr)\n\n# With step size\narr_step = np.arange(0, 11, 2)\nprint(arr_step)\n\n# Start from non-zero\narr_tens = np.arange(10, 101, 10)\nprint(arr_tens)\n</code></pre> <p>Expected output:</p> <pre><code>[ 0  1  2  3  4  5  6  7  8  9 10]\n[ 0  2  4  6  8 10]\n[ 10  20  30  40  50  60  70  80  90 100]\n</code></pre> <p>ML use case: Creating indices, generating sequences for time series data, or creating evenly spaced feature bins.</p>"},{"location":"02-math/numpy/#zeros-and-ones-initialize-arrays","title":"zeros and ones: Initialize Arrays","text":"<p>Useful for creating placeholder arrays before filling them with computed values.</p> <pre><code># 1D array of zeros\nzeros_1d = np.zeros(5)\nprint(zeros_1d)\n\n# 2D array of zeros\nzeros_2d = np.zeros((3, 4))\nprint(zeros_2d)\n\n# 2D array of ones\nones_2d = np.ones((2, 3))\nprint(ones_2d)\n</code></pre> <p>Expected output:</p> <pre><code>[0. 0. 0. 0. 0.]\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n[[1. 1. 1.]\n [1. 1. 1.]]\n</code></pre> <p>Why initialize with zeros: Many ML algorithms accumulate results (like gradient sums in backpropagation) starting from zero arrays.</p>"},{"location":"02-math/numpy/#linspace-linearly-spaced-values","title":"linspace: Linearly Spaced Values","text":"<p>Unlike <code>arange</code> which uses step size, <code>linspace</code> specifies how many values you want.</p> <pre><code># 10 evenly spaced values between 0 and 100\nlin = np.linspace(0, 100, 10)\nprint(lin)\n\n# 5 values between 2 and 20\nlin2 = np.linspace(2, 20, 5)\nprint(lin2)\n</code></pre> <p>Expected output:</p> <pre><code>[  0.          11.11111111  22.22222222  33.33333333  44.44444444\n  55.55555556  66.66666667  77.77777778  88.88888889 100.        ]\n[ 2.   6.5 11.  15.5 20. ]\n</code></pre> <p>ML use case: Creating evenly spaced feature values for plotting decision boundaries or generating test inputs.</p>"},{"location":"02-math/numpy/#eye-identity-matrix","title":"eye: Identity Matrix","text":"<p>An identity matrix has ones on the diagonal and zeros elsewhere. Critical for linear algebra operations.</p> <pre><code># 3x3 identity matrix\nidentity = np.eye(3)\nprint(identity)\n\n# 5x5 identity matrix\nidentity_5 = np.eye(5)\nprint(identity_5)\n</code></pre> <p>Expected output:</p> <pre><code>[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n[[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]\n</code></pre> <p>Why identity matrices matter: Used in matrix inversion, solving linear systems, and initializing certain neural network layers.</p>"},{"location":"02-math/numpy/#4-random-number-generation","title":"4. Random Number Generation","text":"<p>Random numbers are essential for initializing model weights, creating synthetic data, and splitting datasets.</p>"},{"location":"02-math/numpy/#rand-uniform-random-values","title":"rand: Uniform Random Values","text":"<p>Generates random numbers uniformly distributed between 0 and 1.</p> <pre><code># 1D array of 5 random values\nrand_1d = np.random.rand(5)\nprint(rand_1d)\n\n# 3x3 matrix of random values\nrand_2d = np.random.rand(3, 3)\nprint(rand_2d)\n</code></pre> <p>Expected output (values will vary):</p> <pre><code>[0.52134728 0.76543218 0.23498765 0.98123456 0.12345678]\n[[0.41234567 0.87654321 0.23456789]\n [0.65432109 0.34567890 0.78901234]\n [0.12345678 0.56789012 0.90123456]]\n</code></pre> <p>ML use case: Initializing neural network weights with small random values to break symmetry.</p>"},{"location":"02-math/numpy/#randn-standard-normal-distribution","title":"randn: Standard Normal Distribution","text":"<p>Generates random numbers from a Gaussian (normal) distribution with mean 0 and standard deviation 1.</p> <pre><code># 2x3 matrix from standard normal distribution\nrandn_2d = np.random.randn(2, 3)\nprint(randn_2d)\n</code></pre> <p>Expected output (values will vary):</p> <pre><code>[[ 0.51234567 -1.23456789  0.87654321]\n [-0.34567890  1.45678901 -0.67890123]]\n</code></pre> <p>Why normal distribution: Many ML algorithms assume data follows a normal distribution. Random normal values are commonly used for weight initialization in deep learning.</p>"},{"location":"02-math/numpy/#randint-random-integers","title":"randint: Random Integers","text":"<p>Generates random integers within a specified range.</p> <pre><code># 10 random integers between 1 and 100 (exclusive)\nrand_ints = np.random.randint(1, 100, 10)\nprint(rand_ints)\n\n# Single random integer\nsingle = np.random.randint(1, 100)\nprint(single)\n</code></pre> <p>Expected output (values will vary):</p> <pre><code>[42 73 18 91 35 67 22 88 14 59]\n76\n</code></pre> <p>ML use case: Creating random indices for data shuffling, generating discrete synthetic data, or random sampling.</p>"},{"location":"02-math/numpy/#5-array-attributes-and-methods","title":"5. Array Attributes and Methods","text":"<p>Understanding array properties helps you debug shape mismatches and verify data dimensions.</p> <p>Key attributes:</p> <pre><code>arr = np.arange(25)\nprint(f\"Array: {arr}\")\nprint(f\"Shape: {arr.shape}\")\nprint(f\"Size (total elements): {arr.size}\")\nprint(f\"Data type: {arr.dtype}\")\nprint(f\"Number of dimensions: {arr.ndim}\")\n</code></pre> <p>Expected output:</p> <pre><code>Array: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\nShape: (25,)\nSize (total elements): 25\nData type: int64\nNumber of dimensions: 1\n</code></pre>"},{"location":"02-math/numpy/#6-reshaping-arrays","title":"6. Reshaping Arrays","text":"<p>Reshaping changes array dimensions without changing the data. Essential for preparing data for ML models.</p> <p>Reshape a 1D array to 2D:</p> <pre><code>arr = np.arange(30)\nprint(\"Original shape:\", arr.shape)\n\n# Reshape to 5 rows, 6 columns\nreshaped = arr.reshape(5, 6)\nprint(\"\\nReshaped to (5, 6):\")\nprint(reshaped)\nprint(\"New shape:\", reshaped.shape)\n</code></pre> <p>Expected output:</p> <pre><code>Original shape: (30,)\n\nReshaped to (5, 6):\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]\n [24 25 26 27 28 29]]\nNew shape: (5, 6)\n</code></pre> <p>Important rule: Total elements must remain the same. You cannot reshape 30 elements into a <code>(4, 8)</code> array because $\\(4 \\times 8 = 32 \\neq 30\\)$.</p> <p>ML use case: Image data often needs reshaping from flat vectors to $\\((height, width, channels)\\)$ format for CNNs.</p>"},{"location":"02-math/numpy/#7-finding-maximum-minimum-and-indices","title":"7. Finding Maximum, Minimum, and Indices","text":"<p>These methods help identify extreme values in data.</p> <pre><code>ranarr = np.random.randint(0, 100, 10)\nprint(\"Array:\", ranarr)\nprint(\"Maximum value:\", ranarr.max())\nprint(\"Minimum value:\", ranarr.min())\nprint(\"Index of maximum:\", ranarr.argmax())\nprint(\"Index of minimum:\", ranarr.argmin())\n</code></pre> <p>Expected output (values will vary):</p> <pre><code>Array: [42 87 15 93 28 61 74 19 56 38]\nMaximum value: 93\nMinimum value: 15\nIndex of maximum: 3\nIndex of minimum: 2\n</code></pre> <p>ML use case: Finding the predicted class in classification (index of maximum probability), identifying outliers, or locating peak activations in neural networks.</p>"},{"location":"02-math/numpy/#8-array-indexing-and-slicing","title":"8. Array Indexing and Slicing","text":"<p>NumPy indexing works similarly to Python lists but extends to multiple dimensions.</p>"},{"location":"02-math/numpy/#basic-1d-indexing","title":"Basic 1D Indexing","text":"<pre><code>arr = np.arange(0, 11)\nprint(\"Array:\", arr)\n\n# Access single element\nprint(\"Element at index 5:\", arr[5])\n\n# Slice a range\nprint(\"Elements 1-5:\", arr[1:6])\n\n# Slice from start\nprint(\"First 4 elements:\", arr[:4])\n\n# Slice to end\nprint(\"Last 3 elements:\", arr[-3:])\n</code></pre> <p>Expected output:</p> <pre><code>Array: [ 0  1  2  3  4  5  6  7  8  9 10]\nElement at index 5: 5\nElements 1-5: [1 2 3 4 5]\nFirst 4 elements: [0 1 2 3]\nLast 3 elements: [ 8  9 10]\n</code></pre>"},{"location":"02-math/numpy/#broadcasting-assigning-values-to-slices","title":"Broadcasting: Assigning Values to Slices","text":"<p>NumPy allows assigning a single value to multiple array elements at once.</p> <pre><code>arr = np.arange(0, 11)\nprint(\"Original:\", arr)\n\n# Set indices 3-6 to 300\narr[3:6] = 300\nprint(\"After broadcasting:\", arr)\n</code></pre> <p>Expected output:</p> <pre><code>Original: [ 0  1  2  3  4  5  6  7  8  9 10]\nAfter broadcasting: [  0   1   2 300 300 300   6   7   8   9  10]\n</code></pre> <p>Why broadcasting matters: Enables efficient operations on entire array slices without loops, a cornerstone of vectorized programming.</p>"},{"location":"02-math/numpy/#9-2d-array-indexing","title":"9. 2D Array Indexing","text":"<p>Matrices require row and column indices.</p> <p>Access elements and slices:</p> <pre><code>arr_2d = np.array([[5, 10, 15],\n                   [20, 25, 30],\n                   [35, 40, 45]])\n\nprint(\"Full matrix:\")\nprint(arr_2d)\n\n# Access single element (row 1, column 2)\nprint(\"\\nElement [1, 2]:\", arr_2d[1, 2])\n\n# Get entire row\nprint(\"\\nRow 2:\", arr_2d[2])\n\n# Get entire column\nprint(\"\\nColumn 1:\", arr_2d[:, 1])\n\n# Slice submatrix (top-right 2x2)\nprint(\"\\nTop-right 2x2:\")\nprint(arr_2d[0:2, 1:3])\n\n# All rows, columns 1 and 2\nprint(\"\\nAll rows, columns 1-2:\")\nprint(arr_2d[:, 1:3])\n</code></pre> <p>Expected output:</p> <pre><code>Full matrix:\n[[ 5 10 15]\n [20 25 30]\n [35 40 45]]\n\nElement [1, 2]: 30\n\nRow 2: [35 40 45]\n\nColumn 1: [10 25 40]\n\nTop-right 2x2:\n[[10 15]\n [25 30]]\n\nAll rows, columns 1-2:\n[[10 15]\n [25 30]\n [40 45]]\n</code></pre> <p>Notation: <code>arr_2d[row, col]</code> is clearer than <code>arr_2d[row][col]</code> and slightly faster.</p>"},{"location":"02-math/numpy/#10-boolean-indexing-conditional-selection","title":"10. Boolean Indexing (Conditional Selection)","text":"<p>Select array elements that meet a condition\u2014extremely powerful for data filtering.</p> <pre><code>arr = np.arange(1, 11)\nprint(\"Array:\", arr)\n\n# Boolean mask: which elements are greater than 5?\nmask = arr &gt; 5\nprint(\"\\nBoolean mask (arr &gt; 5):\", mask)\n\n# Select only elements greater than 5\nfiltered = arr[arr &gt; 5]\nprint(\"Elements &gt; 5:\", filtered)\n\n# Select elements less than or equal to 5\nfiltered_le = arr[arr &lt;= 5]\nprint(\"Elements &lt;= 5:\", filtered_le)\n</code></pre> <p>Expected output:</p> <pre><code>Array: [ 1  2  3  4  5  6  7  8  9 10]\n\nBoolean mask (arr &gt; 5): [False False False False False  True  True  True  True  True]\nElements &gt; 5: [ 6  7  8  9 10]\nElements &lt;= 5: [1 2 3 4 5]\n</code></pre> <p>How it works: The condition <code>arr &gt; 5</code> returns a Boolean array. Using this as an index returns only <code>True</code> positions.</p> <p>ML use case: Filtering outliers, selecting samples meeting criteria, or applying threshold-based decisions.</p>"},{"location":"02-math/numpy/#11-vectorized-arithmetic-operations","title":"11. Vectorized Arithmetic Operations","text":"<p>NumPy performs element-wise operations across entire arrays without loops.</p> <pre><code>arr = np.arange(0, 10)\nprint(\"Array:\", arr)\n\n# Array + Array (element-wise)\nprint(\"arr + arr:\", arr + arr)\n\n# Array * Array (element-wise)\nprint(\"arr * arr:\", arr * arr)\n\n# Array - Array\nprint(\"arr - arr:\", arr - arr)\n\n# Scalar operations\nprint(\"arr + 10:\", arr + 10)\nprint(\"arr * 2:\", arr * 2)\nprint(\"arr ** 2:\", arr ** 2)\n</code></pre> <p>Expected output:</p> <pre><code>Array: [0 1 2 3 4 5 6 7 8 9]\narr + arr: [ 0  2  4  6  8 10 12 14 16 18]\narr * arr: [ 0  1  4  9 16 25 36 49 64 81]\narr - arr: [0 0 0 0 0 0 0 0 0 0]\narr + 10: [10 11 12 13 14 15 16 17 18 19]\narr * 2: [ 0  2  4  6  8 10 12 14 16 18]\narr ** 2: [ 0  1  4  9 16 25 36 49 64 81]\n</code></pre> <p>Critical insight: Operations like <code>arr * arr</code> multiply corresponding elements, not matrix multiplication. For matrix multiplication, use <code>np.dot()</code> or <code>@</code> operator.</p>"},{"location":"02-math/numpy/#12-universal-functions-ufuncs","title":"12. Universal Functions (ufuncs)","text":"<p>NumPy provides optimized mathematical functions that operate element-wise.</p> <pre><code>arr = np.arange(1, 11)\n\n# Square root\nprint(\"Square roots:\", np.sqrt(arr))\n\n# Exponential\nprint(\"Exponentials:\", np.exp(arr[:3]))  # First 3 to avoid huge numbers\n\n# Logarithm\nprint(\"Natural log:\", np.log(arr))\n\n# Trigonometric\nangles = np.array([0, np.pi/2, np.pi])\nprint(\"Sine:\", np.sin(angles))\n</code></pre> <p>Expected output:</p> <pre><code>Square roots: [1.         1.41421356 1.73205081 2.         2.23606798 2.44948975\n 2.64575131 2.82842712 3.         3.16227766]\nExponentials: [ 2.71828183  7.3890561  20.08553692]\nNatural log: [0.         0.69314718 1.09861229 1.38629436 1.60943791 1.79175947\n 1.94591015 2.07944154 2.19722458 2.30258509]\nSine: [0.0000000e+00 1.0000000e+00 1.2246468e-16]\n</code></pre> <p>ML use case: Activation functions (sigmoid, tanh), normalizing data (log transform), computing distances (square root of sum of squares).</p>"},{"location":"02-math/numpy/#quick-reference","title":"Quick Reference","text":"Operation Code Use Case Create from list <code>np.array([1, 2, 3])</code> Convert Python data to NumPy Range <code>np.arange(0, 10, 2)</code> Sequential values Evenly spaced <code>np.linspace(0, 1, 100)</code> Smooth intervals Zeros/Ones <code>np.zeros((3, 4))</code>, <code>np.ones((2, 3))</code> Initialize arrays Random uniform <code>np.random.rand(3, 3)</code> Random weights (0-1) Random normal <code>np.random.randn(3, 3)</code> Gaussian initialization Random integers <code>np.random.randint(1, 100, 10)</code> Discrete sampling Reshape <code>arr.reshape(5, 6)</code> Change dimensions Indexing <code>arr</code>, <code>arr[1:5]</code>, <code>arr[arr &gt; 3]</code> Access elements 2D indexing <code>matrix[row, col]</code>, <code>matrix[:, col]</code> Access matrix data Element-wise ops <code>arr + 5</code>, <code>arr * arr</code>, <code>arr ** 2</code> Vectorized arithmetic Math functions <code>np.sqrt()</code>, <code>np.exp()</code>, <code>np.log()</code> Element-wise functions"},{"location":"02-math/numpy/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned to create NumPy arrays using multiple methods, manipulate array shapes and dimensions, perform efficient indexing and slicing including Boolean selection, and apply vectorized operations that replace slow Python loops.</p> <p>Best practices:</p> <ul> <li>Always use NumPy arrays for numerical data instead of lists</li> <li>Think in terms of array operations rather than element-by-element loops</li> <li>Check array shapes frequently when debugging to catch dimension mismatches early</li> <li>Use vectorization wherever possible for speed and code clarity</li> <li>Understand broadcasting to efficiently combine arrays of different shapes</li> </ul> <p>Performance mindset: If you find yourself writing a <code>for</code> loop to process array elements, ask: \"Can I vectorize this?\" The answer is almost always yes in NumPy.</p> <p>Connections to ML:</p> <ul> <li>Data representation: Datasets become NumPy arrays with shape <code>(samples, features)</code></li> <li>Model operations: Linear layers compute $\\(\\mathbf{y} = W\\mathbf{x} + \\mathbf{b}\\)$ using NumPy operations</li> <li>Gradient descent: Parameter updates use vectorized addition and multiplication</li> <li>Image processing: Images are NumPy arrays with shape <code>(height, width, channels)</code></li> </ul> <p>External resources:</p> <ul> <li>NumPy Official Documentation - comprehensive reference and tutorials</li> <li>NumPy for Absolute Beginners - official beginner guide</li> <li>From Python to NumPy - free online book on vectorization</li> </ul>"},{"location":"02-math/probability-statistics/","title":"Probability &amp; Statistics for Machine Learning","text":"<p>This tutorial introduces probability and statistics fundamentals that are essential for understanding and applying machine learning. You'll learn to quantify uncertainty, understand distributions, and learn how probabilistic thinking enables ML models to make predictions under uncertainty.</p> <p>Estimated time: 70 minutes</p>"},{"location":"02-math/probability-statistics/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: Machine learning models don't make perfectly accurate predictions\u2014they work with noisy, incomplete data and must quantify uncertainty. Without probability theory, we cannot model randomness, evaluate model reliability, or understand why algorithms make certain predictions.</p> <p>Practical benefits: Probability provides the mathematical framework for handling uncertainty. Understanding distributions helps you choose appropriate models, compute confidence intervals, detect outliers, and interpret what predictions actually mean. Statistics enables you to validate model improvements and make data-driven decisions backed by evidence.</p> <p>Professional context: Every ML algorithm involves probability. Classification models output probability distributions over classes (\"80% cat, 20% dog\"), regression models have probabilistic error terms, and neural networks learn probability distributions. Understanding these foundations is essential for building and trusting ML systems.</p> <p></p>"},{"location":"02-math/probability-statistics/#prerequisites-learning-objectives","title":"Prerequisites &amp; Learning Objectives","text":"<p>Required knowledge: - Basic algebra and arithmetic - Understanding of functions and graphs - Familiarity with summation notation - Basic Python (minimal needed)</p> <p>Learning outcomes: - Understand what probability measures and how to compute it - Distinguish between independent and disjoint events - Apply addition and multiplication rules for probabilities - Work with probability distributions and density functions - Compute and interpret mean, variance, and standard deviation - Recognize common distributions and their ML applications</p>"},{"location":"02-math/probability-statistics/#core-concepts","title":"Core Concepts","text":""},{"location":"02-math/probability-statistics/#what-is-probability","title":"What is Probability?","text":"<p>Probability is a number between 0 and 1 that measures the likelihood of an event occurring.</p> <p>Properties:</p> <ul> <li> \\[0 \\leq P(A) \\leq 1$$ for any event $$A\\] </li> <li> \\[P(\\text{certain event}) = 1\\] </li> <li> \\[P(\\text{impossible event}) = 0\\] </li> </ul> <p>Example: Rolling a fair die</p> <ul> <li> \\[P(\\text{rolling a 6}) = \\frac{1}{6} \\approx 0.167\\] </li> <li> \\[P(\\text{rolling even number}) = \\frac{3}{6} = 0.5\\] </li> <li>$\\(P(\\text{rolling 7}) = 0\\)$ (impossible)</li> </ul> <p>In ML, probability models uncertainty. A classifier doesn't say \"this IS a cat\"\u2014it says \"I'm 85% confident this is a cat.\"</p>"},{"location":"02-math/probability-statistics/#random-variables","title":"Random Variables","text":"<p>A random variable is a variable whose value is determined by chance.</p> <p>Types:</p> <ul> <li>Discrete: Countable values (coin flips, dice rolls, number of customers)</li> <li>Continuous: Any value in a range (height, temperature, prediction error)</li> </ul> <p>Features are random variables (customer age varies), predictions have uncertainty, and errors are random.</p>"},{"location":"02-math/probability-statistics/#basic-probability-notation","title":"Basic Probability Notation","text":"<p>Key notation:</p> <ul> <li>$\\(P(A)\\)$: Probability event $\\(A\\)$ occurs</li> <li>$\\(P(A \\cap B)\\)$: Probability both $\\(A\\)$ AND $\\(B\\)$ occur</li> <li>$\\(P(A \\cup B)\\)$: Probability $\\(A\\)$ OR $\\(B\\)$ occurs</li> <li>$\\(P(A|B)\\)$: Probability of $\\(A\\)$ given $\\(B\\)$ occurred (conditional)</li> <li>$\\(P(A^c)\\)$: Probability $\\(A\\)$ does NOT occur (complement)</li> </ul> <p>Complement rule:</p> \\[ P(A^c) = 1 - P(A) \\] <p>Example: Weather forecasting</p> <ul> <li> \\[P(\\text{rain}) = 0.3\\] </li> <li> \\[P(\\text{no rain}) = 1 - 0.3 = 0.7\\] </li> </ul>"},{"location":"02-math/probability-statistics/#independent-vs-disjoint-events","title":"Independent vs Disjoint Events","text":"<p>These concepts are different and often confused.</p> <p>Disjoint (Mutually Exclusive): Cannot both happen simultaneously</p> <ul> <li>Rolling a 3 AND rolling a 5 on one die (impossible)</li> <li>$\\(A \\cap B = \\emptyset\\)$ (empty)</li> <li>If $\\(A\\)$ happens, $\\(B\\)$ definitely doesn't</li> </ul> <p>Independent: One event doesn't affect the other's probability</p> <ul> <li>Flipping two different coins</li> <li> \\[P(A \\cap B) = P(A) \\cdot P(B)\\] </li> <li>Knowing $\\(A\\)$ occurred doesn't change $\\(P(B)\\)$</li> </ul> <p></p> <p>Critical distinction: Disjoint events are NEVER independent (unless probability is 0). If events can't occur together, they're dependent!</p> <p>Naive Bayes (a ML model) assumes features are independent. Checking this assumption matters for model accuracy.</p>"},{"location":"02-math/probability-statistics/#addition-rule","title":"Addition Rule","text":"<p>Computes probability that at least one event occurs.</p> <p>General addition rule: $$ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) $$</p> <p>Why subtract? Adding $\\(P(A) + P(B)\\)$ double-counts the overlap.</p> <p>Special case (disjoint): $$ P(A \\cup B) = P(A) + P(B) $$</p> <p>Example: Drawing a card</p> <ul> <li> \\[P(\\text{Heart}) = \\frac{13}{52} = 0.25\\] </li> <li> \\[P(\\text{Ace}) = \\frac{4}{52} = 0.077\\] </li> <li> \\[P(\\text{Ace of Hearts}) = \\frac{1}{52} = 0.019\\] </li> <li> \\[P(\\text{Heart OR Ace}) = 0.25 + 0.077 - 0.019 = 0.308\\] </li> </ul>"},{"location":"02-math/probability-statistics/#multiplication-rule","title":"Multiplication Rule","text":"<p>Computes probability that both events occur.</p> <p>General multiplication rule: $$ P(A \\cap B) = P(A) \\cdot P(B|A) $$</p> <p>Independent events: $$ P(A \\cap B) = P(A) \\cdot P(B) $$</p> <p>Example: Drawing two cards without replacement</p> <ul> <li> \\[P(\\text{first ace}) = \\frac{4}{52}\\] </li> <li> \\[P(\\text{second ace | first ace}) = \\frac{3}{51}\\] </li> <li> \\[P(\\text{both aces}) = \\frac{4}{52} \\times \\frac{3}{51} = 0.0045\\] </li> </ul> <p>ML connection: Computing joint probabilities in probabilistic models.</p>"},{"location":"02-math/probability-statistics/#conditional-probability-and-bayes-theorem","title":"Conditional Probability and Bayes' Theorem","text":"<p>Conditional probability: Probability of $\\(A\\)$ given $\\(B\\)$ occurred</p> \\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\] <p>Bayes' Theorem (fundamental for ML): $$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$</p> <p></p> <p>Example: Medical testing</p> <ul> <li>Disease prevalence: $\\(P(\\text{disease}) = 0.01\\)$ (1%)</li> <li>Test accuracy: $\\(P(\\text{positive | disease}) = 0.95\\)$</li> <li>False positive rate: $\\(P(\\text{positive | no disease}) = 0.05\\)$</li> </ul> <p>Even with a positive test, the actual probability of having the disease is only about 16% because the disease is rare!</p> <p>ML connection: Classification uses Bayes' theorem to compute $\\(P(\\text{class | features})\\)$ from $\\(P(\\text{features | class})\\)$.</p>"},{"location":"02-math/probability-statistics/#probability-distributions","title":"Probability Distributions","text":"<p>A probability distribution describes how probability is allocated across possible values.</p> <p>Types:</p> <ul> <li>Discrete: Probability Mass Function (PMF) - specific values have probabilities</li> <li>Continuous: Probability Density Function (PDF) - probabilities over ranges</li> </ul> <p>All probabilities sum (discrete) or integrate (continuous) to 1.</p> <p>Example: Fair die has uniform distribution</p> <ul> <li> \\[P(X = 1) = P(X = 2) = \\cdots = P(X = 6) = \\frac{1}{6}\\] </li> </ul>"},{"location":"02-math/probability-statistics/#probability-mass-function-pmf","title":"Probability Mass Function (PMF)","text":"<p>For discrete variables, PMF gives $\\(P(X = x)\\)$ for each value.</p> <p>Properties: - $\\(p_X(x) \\geq 0\\)$ for all $\\(x\\)$ - $\\(\\sum_x p_X(x) = 1\\)$</p> <p>Example: Number of heads in 3 coin flips</p> Heads (x) 0 1 2 3 P(X = x) 1/8 3/8 3/8 1/8 <p>Classification outputs are PMFs over classes.</p>"},{"location":"02-math/probability-statistics/#probability-density-function-pdf","title":"Probability Density Function (PDF)","text":"<p>For continuous variables, PDF $\\(f(x)\\)$ describes relative likelihood.</p> <p>Key difference: For continuous variables, $\\(P(X = x) = 0\\)$ for any specific $\\(x\\)$. We compute probabilities over intervals:</p> \\[ P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx \\] <p>Properties:</p> <ul> <li> \\[f(x) \\geq 0\\] </li> <li> \\[\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\\] </li> <li>Height $\\(f(x)\\)$ is NOT a probability (can exceed 1)</li> </ul> <p>Regression assumes errors follow a PDF (usually Gaussian/normal distribution).</p>"},{"location":"02-math/probability-statistics/#cumulative-distribution-function-cdf","title":"Cumulative Distribution Function (CDF)","text":"<p>The CDF gives probability that a random variable is less than or equal to a value:</p> \\[ F(x) = P(X \\leq x) \\] <p>Properties:</p> <ul> <li> \\[0 \\leq F(x) \\leq 1\\] </li> <li>$\\(F(x)\\)$ is non-decreasing</li> <li> \\[F(-\\infty) = 0$$, $$F(\\infty) = 1\\] </li> </ul> <p>Use: Compute probability over range $$ P(a &lt; X \\leq b) = F(b) - F(a) $$</p> <p>Example: For standard normal distribution</p> <ul> <li>$\\(F(0) = 0.5\\)$ (50% of values below 0)</li> <li>$\\(F(1) = 0.841\\)$ (84.1% of values below 1)</li> <li>$\\(P(-1 \\leq X \\leq 1) = F(1) - F(-1) = 0.683\\)$ (68.3%)</li> </ul>"},{"location":"02-math/probability-statistics/#percentiles","title":"Percentiles","text":"<p>The p-th percentile is the value below which p% of observations fall.</p> <p>Common percentiles:</p> <ul> <li>25th (Q1), 50th (median), 75th (Q3)</li> <li>Interquartile Range: $\\(IQR = Q3 - Q1\\)$</li> </ul> <p></p> <p>Outlier detection: Values outside $\\([Q1 - 1.5 \\times IQR, Q3 + 1.5 \\times IQR]\\)$</p> <p>ML connection: \"This model's 90th percentile error is 5 units\" means 90% of predictions have error \u2264 5.</p>"},{"location":"02-math/probability-statistics/#mean-variance-and-standard-deviation","title":"Mean, Variance, and Standard Deviation","text":"<p>Let's use a simple dataset to illustrate: Test scores: 70, 75, 80, 85, 90</p> <p>Mean (average): $$ \\mu = \\frac{70 + 75 + 80 + 85 + 90}{5} = \\frac{400}{5} = 80 $$</p> <p>The mean is the \"center\" or typical value.</p> <p>Variance (spread): $$ \\sigma^2 = \\frac{(70-80)^2 + (75-80)^2 + (80-80)^2 + (85-80)^2 + (90-80)^2}{5} $$ $$ = \\frac{100 + 25 + 0 + 25 + 100}{5} = \\frac{250}{5} = 50 $$</p> <p>Variance measures average squared distance from mean.</p> <p>Standard deviation: $$ \\sigma = \\sqrt{50} \\approx 7.07 $$</p> <p>Standard deviation is typical distance from mean, in original units.</p> <p>Interpretation: Most scores are within \u00b17 points of the average (80).</p> <p>Properties:</p> <ul> <li>Higher variance/std dev = more spread out data</li> <li>Zero variance = all values identical</li> <li>Units: variance is squared, std dev matches data units</li> </ul> <p>ML connection: </p> <ul> <li>Mean Squared Error (MSE) is the variance of prediction errors</li> <li>Standard deviation tells you typical prediction error magnitude</li> <li>Models try to minimize variance of errors</li> </ul>"},{"location":"02-math/probability-statistics/#common-probability-distributions","title":"Common Probability Distributions","text":""},{"location":"02-math/probability-statistics/#uniform-distribution","title":"Uniform Distribution","text":"<p>Intuition: All values equally likely in a range.</p> <p></p> <p>Use cases:</p> <ul> <li>Random initialization of neural network weights</li> <li>Generating test data</li> <li>Modeling \"no prior knowledge\" scenarios</li> </ul> <p>Example: Roll a fair die - each number has probability 1/6</p> <p>ML application: Dropout in neural networks randomly selects neurons uniformly.</p>"},{"location":"02-math/probability-statistics/#normal-gaussian-distribution","title":"Normal (Gaussian) Distribution","text":"<p>Intuition: Bell-shaped curve, most values near the mean, symmetric.</p> <p></p> <p>Use cases:</p> <ul> <li>Natural measurements (height, IQ)</li> <li>Modeling errors and noise</li> <li>Central Limit Theorem applications</li> </ul> <p>Parameters: Mean $\\(\\mu\\)$, variance $\\(\\sigma^2\\)$</p> <p>68-95-99.7 rule:</p> <ul> <li>68% of data within 1 standard deviation of mean</li> <li>95% within 2 standard deviations</li> <li>99.7% within 3 standard deviations</li> </ul> <p>ML applications:</p> <ul> <li>Linear regression assumes normal error distribution</li> <li>Gaussian Naive Bayes classifier</li> <li>Gaussian Processes</li> <li>Many algorithms assume normality for mathematical convenience</li> </ul>"},{"location":"02-math/probability-statistics/#bernoulli-distribution","title":"Bernoulli Distribution","text":"<p>Intuition: Single yes/no trial (success/failure).</p> <p></p> <p>Use cases:</p> <ul> <li>Coin flip (heads/tails)</li> <li>Binary classification (cat/not cat)</li> <li>Click/no click on ad</li> </ul> <p>Parameters: $\\(p\\)$ = probability of success</p> <p>ML connection: Logistic regression outputs Bernoulli probabilities for binary classification.</p>"},{"location":"02-math/probability-statistics/#binomial-distribution","title":"Binomial Distribution","text":"<p>Intuition: Number of successes in $\\(n\\)$ independent yes/no trials.</p> <p></p> <p>Use cases:</p> <ul> <li>Number of heads in 10 coin flips</li> <li>Number of customers who buy product</li> <li>Number of correct answers on multiple-choice test</li> </ul> <p>Parameters: $\\(n\\)$ (trials), $\\(p\\)$ (success probability)</p> <p>Mean: $\\(np\\)$, Variance: $\\(np(1-p)\\)$</p> <p>Example: Flip coin 10 times, expect 5 heads on average if fair.</p>"},{"location":"02-math/probability-statistics/#poisson-distribution","title":"Poisson Distribution","text":"<p>Intuition: Number of events occurring in fixed time/space interval when events happen independently at constant average rate.</p> <p></p> <p>Use cases:</p> <ul> <li>Number of emails received per hour</li> <li>Customer arrivals at store per day</li> <li>Website visits per minute</li> <li>Rare events</li> </ul> <p>Parameters: $\\(\\lambda\\)$ (average rate)</p> <p>ML connection: Poisson regression for count data (predicting number of occurrences).</p>"},{"location":"02-math/probability-statistics/#exponential-distribution","title":"Exponential Distribution","text":"<p>Intuition: Time between events in a Poisson process.</p> <p></p> <p>Use cases: - Time until next customer arrives - Lifetime of electronic component - Time between failures</p> <p>Parameters: $\\(\\lambda\\)$ (rate)</p> <p>Relationship: If events follow Poisson, time between events follows exponential.</p>"},{"location":"02-math/probability-statistics/#quick-reference","title":"Quick Reference","text":"Concept Definition ML Application Probability Likelihood measure, 0 to 1 Quantifying uncertainty Independent $\\(P(A \\cap B) = P(A)P(B)\\)$ Naive Bayes assumption Addition rule $\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)$ Multiple outcomes Conditional $\\(P(A\\|B) = \\frac{P(A \\cap B)}{P(B)}\\)$ Context-dependent prediction Bayes' theorem $\\(P(A\\|B) = \\frac{P(B\\|A)P(A)}{P(B)}\\)$ Classification Mean Center of distribution Expected value Variance Spread measure Error magnitude Normal distribution Bell curve Error modeling PMF Discrete probabilities Classification outputs PDF Continuous density Regression errors CDF Cumulative probability Percentiles, p-values"},{"location":"02-math/probability-statistics/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned how probability quantifies uncertainty, mastered probability rules, understood distributions as models of randomness, computed statistical measures, and connected concepts to ML applications.</p> <p>Best practices:</p> <ul> <li>Check distribution assumptions before applying methods</li> <li>Visualize data distributions to verify model fit</li> <li>Consider base rates (priors) in classification</li> <li>Use appropriate metrics for your distribution type</li> </ul> <p>Connections to ML:</p> <ul> <li>Classification outputs probability distributions (softmax)</li> <li>Regression assumes error distributions (usually normal)</li> <li>Uncertainty quantification uses probability</li> <li>Bayesian ML treats parameters as random variables</li> </ul> <p>External resources: - Khan Academy: Probability &amp; Statistics - interactive lessons - 3Blue1Brown: Bayes' Theorem - visual explanation - Seeing Theory - visual introduction to probability</p>"},{"location":"03-data/","title":"Data Overview","text":"<p>Turn raw data into answers. Short, practical lessons that take data from messy to model\u2011ready.</p> <p></p>"},{"location":"03-data/#data-fundamentals","title":"Data Fundamentals","text":"<p>Understanding the data pipeline</p> <p>Learn why data quality determines ML success and master the six-phase pipeline: collection, exploration, cleaning, transformation, splitting, and storage.</p> <p>You'll learn: Data types, quality dimensions, common challenges (imbalanced, missing, drift), and best practices for reproducible workflows</p>"},{"location":"03-data/#pandas","title":"Pandas","text":"<p>Load, explore, and reshape data fast</p> <p>Read files, select rows/columns, filter, group, merge, and tidy your datasets so analysis feels smooth, not painful.</p> <p>You\u2019ll learn: Importing data, cleaning columns, joins, groupby, and quick summaries</p>"},{"location":"03-data/#visualization","title":"Visualization","text":"<p>Make patterns visible</p> <p>Tell a clear story with plots that highlight trends, comparisons, and outliers using Matplotlib and Seaborn.</p> <p>You\u2019ll learn: Histograms, box plots, scatter plots, line charts, and choosing the right chart for the question</p>"},{"location":"03-data/#data-processing","title":"Data Processing","text":"<p>From messy to usable</p> <p>Handle missing values, fix types, engineer features, and prepare datasets that models can learn from.</p> <p>You\u2019ll learn: Imputation, encoding, scaling, datetime handling, and feature creation</p>"},{"location":"03-data/#data-collection","title":"Data collection","text":"<p>Get the data you need</p> <p>Pull data from files, APIs, and the web\u2014then store it in formats that are easy to use and share.</p> <p>You\u2019ll learn: CSV/Parquet basics, calling APIs, simple scraping, and organizing raw vs. processed data</p>"},{"location":"03-data/#sql-databases","title":"SQL databases","text":"<p>Query data with confidence</p> <p>Use SQL to select, join, aggregate, and filter data directly where it lives\u2014fast and reproducible.</p> <p>You\u2019ll learn: SELECT, WHERE, JOIN, GROUP BY, HAVING, and writing queries for analysis</p>"},{"location":"03-data/#nosql-mongodb","title":"NoSQL (MongoDB)","text":"<p>Work with flexible data</p> <p>Store and query JSON\u2011like documents for logs, events, and semi\u2011structured sources common in ML workflows.</p> <p>You\u2019ll learn: Inserting documents, filtering with queries, projections, and basic aggregations</p> <p>Start with a small dataset, answer one question with a plot, then clean and shape the data for a simple model. Repeat\u2014each pass makes the story (and the model) sharper.</p>"},{"location":"03-data/data-collection/","title":"Data Collection: Working with APIs","text":"<p>This tutorial covers how to retrieve data from web APIs using Python. You'll learn to make HTTP requests, handle common API patterns like pagination and rate limits, and transform JSON responses into usable datasets.</p> <p>Estimated time: 35 minutes</p>"},{"location":"03-data/data-collection/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: </p> <p>Most valuable data lives behind APIs, not in downloadable files.</p> <p>Real-world data sources include weather services, social media platforms, financial markets, government databases, and business tools. These services expose data through APIs (Application Programming Interfaces) that require specific request patterns, authentication, and response handling.</p> <p>Models need training data, and APIs provide access to fresh, structured information that CSV files can't match.</p> <p>Practical benefits: API skills let you collect current data, automate data pipelines, and access datasets that don't exist as files. You'll build systems that fetch updated information on demand rather than relying on static downloads.</p> <p>Professional context: Data scientists regularly integrate multiple API sources. Companies that master API collection build dynamic systems; those that rely only on manual downloads face outdated information and scaling problems.</p> <p>API access is standard practice. Every data role expects you to fetch and integrate external data.</p> <p></p>"},{"location":"03-data/data-collection/#core-concepts","title":"Core Concepts","text":""},{"location":"03-data/data-collection/#what-is-an-api","title":"What Is an API?","text":"<p>An API (Application Programming Interface) is a service that accepts structured requests and returns structured data. Instead of clicking through a website, you send HTTP requests directly to endpoints and receive responses in JSON or XML format.</p> <p>Key terminology:</p> <p>Endpoint: A specific URL that provides access to a resource (e.g., <code>https://api.service.com/users</code>)</p> <p>HTTP Methods: Actions you can perform:</p> <ul> <li>GET: Retrieve data (most common)</li> <li>POST: Send new data</li> <li>PUT/PATCH: Update existing data</li> <li>DELETE: Remove data</li> </ul> <p>Request: What you send to the API (URL, parameters, headers)</p> <p>Response: What the API returns (status code, data, headers)</p> <p>JSON: JavaScript Object Notation; the standard format for API data exchange</p> <p>Rate Limit: Maximum number of requests allowed per time period (e.g., 100 requests/hour)</p> <p>Authentication: Credentials proving you're authorized to access the API (API keys, tokens)</p>"},{"location":"03-data/data-collection/#the-request-response-cycle","title":"The Request-Response Cycle","text":"<p>Every API interaction follows the same pattern:</p> <p></p> <p>1. You send request \u2192 2. API processes \u2192 3. API returns response \u2192 4. You parse data</p> <p>Understanding this cycle helps you debug issues and handle errors systematically.</p>"},{"location":"03-data/data-collection/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"03-data/data-collection/#phase-1-making-your-first-api-request","title":"Phase 1: Making Your First API Request","text":"<p>Start simple: fetch one resource and examine the response.</p> <p>Install the requests library:</p> <pre><code>pip install requests\n</code></pre> <p>Basic GET request:</p> <pre><code>import requests\n\n# Define endpoint\nurl = \"https://swapi-api.hbtn.io/api/people/1/\"\n\n# Make request\nresponse = requests.get(url)\n\n# Check if successful\nprint(f\"Status Code: {response.status_code}\")\n\n# View raw response\nprint(f\"Response Text: {response.text}\")\n</code></pre> <p>Status codes tell you what happened:</p> Code Meaning Action 200 Success Parse the data 400 Bad Request Check your parameters 401 Unauthorized Verify authentication 404 Not Found Endpoint or resource doesn't exist 429 Too Many Requests You hit rate limit 500 Server Error API issue; try again later <p>Tip: Always check <code>response.status_code</code> before parsing data. Attempting to parse a failed request causes errors.</p>"},{"location":"03-data/data-collection/#phase-2-working-with-json-data","title":"Phase 2: Working with JSON Data","text":"<p>Most APIs return JSON, a structured format that Python handles natively.</p> <p>Converting JSON to Python:</p> <pre><code># Fetch data\nresponse = requests.get(\"https://swapi-api.hbtn.io/api/people/1/\")\n\n# Parse JSON into dictionary\nif response.status_code == 200:\n    data = response.json()          # Converts JSON string to Python dict\n    print(f\"Name: {data['name']}\")\n    print(f\"Height: {data['height']}\")\n    print(f\"Films: {data['films']}\")\nelse:\n    print(f\"Error: {response.status_code}\")\n</code></pre> <p>JSON structure maps to Python types:</p> <ul> <li>JSON object <code>{}</code> \u2192 Python dictionary</li> <li>JSON array <code>[]</code> \u2192 Python list</li> <li>JSON string \u2192 Python string</li> <li>JSON number \u2192 Python int/float</li> <li>JSON boolean \u2192 Python True/False</li> <li>JSON null \u2192 Python None</li> </ul> <p>Navigating nested JSON:</p> <pre><code># Access nested fields\ndata = response.json()\nhomeworld_url = data['homeworld']\nfilms_list = data['films']\n\n# Extract specific values{films_list}\")\n</code></pre>"},{"location":"03-data/data-collection/#phase-3-adding-parameters-to-requests","title":"Phase 3: Adding Parameters to Requests","text":"<p>APIs accept parameters to filter, search, or customize responses.</p> <p>Query parameters:</p> <pre><code># Search for characters named \"Luke\"\nurl = \"https://swapi-api.hbtn.io/api/people/\"\nparams = {\"search\": \"Luke\"}\n\nresponse = requests.get(url, params=params)\ndata = response.json()\n\nprint(f\"Results found: {data['count']}\")\nprint(f\"First result: {data['results']['name']}\")\n</code></pre> <p>The <code>params</code> argument automatically formats the URL: <pre><code>https://swapi-api.hbtn.io/api/people/?search=Luke\n</code></pre></p> <p>Multiple parameters:</p> <pre><code>params = {\n    \"search\": \"Skywalker\",\n    \"format\": \"json\"\n}\nresponse = requests.get(url, params=params)\n</code></pre> <p>Tip: Use dictionaries for parameters. The requests library handles URL encoding automatically.</p>"},{"location":"03-data/data-collection/#phase-4-handling-pagination","title":"Phase 4: Handling Pagination","text":"<p>APIs split large datasets across multiple pages. You must request each page to get complete data.</p> <p>Detecting pagination:</p> <pre><code>response = requests.get(\"https://swapi-api.hbtn.io/api/people/\")\ndata = response.json()\n\nprint(f\"Total results: {data['count']}\")\nprint(f\"Next page: {data['next']}\")\nprint(f\"Results on this page: {len(data['results'])}\")\n</code></pre> <p>Collecting all pages:</p> <pre><code>def fetch_all_pages(base_url):\n    all_results = []\n    url = base_url\n\n    while url:\n        response = requests.get(url)\n        if response.status_code != 200:\n            print(f\"Error: {response.status_code}\")\n            break\n\n        data = response.json()\n        all_results.extend(data['results'])  # Add this page's results\n        url = data['next']  # Get next page URL (None if last page)\n\n    return all_results\n\n# Fetch all Star Wars characters\ncharacters = fetch_all_pages(\"https://swapi-api.hbtn.io/api/people/\")\nprint(f\"Total characters collected: {len(characters)}\")\n</code></pre> <p>Pagination patterns vary by API:</p> <ul> <li>Link-based: API provides <code>next</code> URL (like SWAPI)</li> <li>Offset-based: You increment <code>?offset=20</code> or <code>?page=2</code></li> <li>Cursor-based: API provides token for next page</li> </ul> <p>Always read API documentation to understand the pagination method.</p>"},{"location":"03-data/data-collection/#phase-5-respecting-rate-limits","title":"Phase 5: Respecting Rate Limits","text":"<p>APIs restrict request frequency to prevent abuse and maintain performance.</p> <p>Common rate limit formats:</p> <ul> <li>100 requests per hour</li> <li>10 requests per second</li> <li>1000 requests per day</li> </ul> <p>Handling rate limits:</p> <pre><code>import time\n\ndef fetch_with_rate_limit(urls, delay=1):\n    \"\"\"Fetch multiple URLs with delay between requests.\"\"\"\n    results = []\n\n    for url in urls:\n        response = requests.get(url)\n\n        if response.status_code == 429:  # Rate limit hit\n            print(\"Rate limit reached. Waiting 60 seconds...\")\n            time.sleep(60)\n            response = requests.get(url)  # Retry\n\n        if response.status_code == 200:\n            results.append(response.json())\n\n        time.sleep(delay)  # Delay between requests\n\n    return results\n\n# Fetch multiple characters with 1-second delay\ncharacter_urls = [\n    \"https://swapi-api.hbtn.io/api/people/1/\",\n    \"https://swapi-api.hbtn.io/api/people/2/\",\n    \"https://swapi-api.hbtn.io/api/people/3/\"\n]\ncharacters = fetch_with_rate_limit(character_urls, delay=1)\n</code></pre> <p>Best practices:</p> <ul> <li>Read headers: Many APIs include rate limit info in response headers (<code>X-RateLimit-Remaining</code>)</li> <li>Add delays: Insert <code>time.sleep()</code> between requests</li> <li>Handle 429 responses: Wait and retry when rate limited</li> <li>Cache results: Store responses to avoid redundant requests</li> </ul>"},{"location":"03-data/data-collection/#phase-6-transforming-api-data-into-dataframes","title":"Phase 6: Transforming API Data into DataFrames","text":"<p>Once collected, convert JSON to pandas DataFrames for analysis.</p> <p>Basic conversion:</p> <pre><code>import pandas as pd\n\n# Fetch data\nresponse = requests.get(\"https://swapi-api.hbtn.io/api/people/\")\ndata = response.json()\n\n# Convert results to DataFrame\ndf = pd.DataFrame(data['results'])\nprint(df[['name', 'height', 'mass', 'birth_year']].head())\n</code></pre> <p>Handling nested fields:</p> <pre><code># Some fields contain lists or nested objects\n# Flatten or extract specific information\n\n# Extract first film URL for each character\ndff len(x) &gt; 0 else None)\n\n# Convert height to numeric (some values are \"unknown\")\ndf['height'] = pd.to_numeric(df['height'], errors='coerce')\n\n# Clean and prepare for analysis\ndf_clean = df[['name', 'height', 'mass', 'gender']].dropna()\n</code></pre> <p>Enriching data with related resources:</p> <pre><code>def get_homeworld_name(homeworld_url):\n    \"\"\"Fetch homeworld name from URL.\"\"\"\n    response = requests.get(homeworld_url)\n    if response.status_code == 200:\n        return response.json()['name']\n    return None\n\n# Add homeworld names\ndf['homeworld_name'] = df['homeworld'].apply(get_homeworld_name)\n</code></pre> <p>Warning: Fetching related resources for every row can trigger rate limits. Add delays or batch requests.</p>"},{"location":"03-data/data-collection/#common-api-challenges","title":"Common API Challenges","text":""},{"location":"03-data/data-collection/#authentication-required","title":"Authentication Required","text":"<p>Many APIs require credentials.</p> <p>API key in headers:</p> <pre><code>headers = {\"Authorization\": \"Bearer YOUR_API_KEY\"}\nresponse = requests.get(url, headers=headers)\n</code></pre> <p>API key in parameters:</p> <pre><code>params = {\"api_key\": \"YOUR_API_KEY\"}\nresponse = requests.get(url, params=params)\n</code></pre> <p>Always store API keys in environment variables, never in code:</p> <pre><code>import os\napi_key = os.environ.get(\"API_KEY\")\n</code></pre>"},{"location":"03-data/data-collection/#network-errors-and-timeouts","title":"Network Errors and Timeouts","text":"<p>Requests can fail due to network issues.</p> <p>Handling errors:</p> <pre><code>try:\n    response = requests.get(url, timeout=10)  # Wait max 10 seconds\n    response.raise_for_status()  # Raises exception for 4xx/5xx codes\n    data = response.json()\nexcept requests.exceptions.Timeout:\n    print(\"Request timed out\")\nexcept requests.exceptions.ConnectionError:\n    print(\"Connection failed\")\nexcept requests.exceptions.HTTPError as e:\n    print(f\"HTTP error: {e}\")\n</code></pre>"},{"location":"03-data/data-collection/#inconsistent-data-formats","title":"Inconsistent Data Formats","text":"<p>APIs sometimes return inconsistent structures.</p> <p>Defensive parsing:</p> <pre><code># Handle missing or inconsistent fields\nname = data.get('name', 'Unknown')\nheight = data.get('height', None)\nfilms = data.get('films', [])\n\n# Validate expected structure\nif 'results' in data and isinstance(data['results'], list):\n    process_results(data['results'])\nelse:\n    print(\"Unexpected response format\")\n</code></pre>"},{"location":"03-data/data-collection/#best-practices","title":"Best Practices","text":"<p>Read API documentation first: Spend 15 minutes understanding endpoints, parameters, authentication, and rate limits before writing code. Saves hours of debugging.</p> <p>Test with small requests: Fetch one resource successfully before building loops that make hundreds of requests.</p> <p>Handle errors gracefully: APIs fail. Always check status codes and include try-except blocks for network errors.</p> <p>Respect rate limits: Add delays between requests. Getting blocked wastes more time than being patient.</p> <p>Cache responses: Save API responses to files during development. Avoids re-requesting the same data during testing.</p> <pre><code>import json\n\n# Save response\nwith open('cache.json', 'w') as f:\n    json.dump(response.json(), f)\n\n# Load cached data\nwith open('cache.json', 'r') as f:\n    data = json.load(f)\n</code></pre> <p>Log your requests: Track what you've fetched to avoid duplicates and debug issues.</p> <p>Version API endpoints: APIs change. Note the API version you're using (e.g., <code>/v1/</code>, <code>/v2/</code>) to ensure reproducibility.</p>"},{"location":"03-data/data-collection/#quick-reference","title":"Quick Reference","text":""},{"location":"03-data/data-collection/#essential-requests-methods","title":"Essential requests Methods","text":"Method Purpose Example <code>requests.get(url)</code> Fetch data <code>response = requests.get(url)</code> <code>response.status_code</code> Check success <code>if response.status_code == 200:</code> <code>response.json()</code> Parse JSON <code>data = response.json()</code> <code>requests.get(url, params=dict)</code> Add parameters <code>requests.get(url, params={'search': 'Luke'})</code> <code>requests.get(url, headers=dict)</code> Add headers <code>requests.get(url, headers={'Authorization': 'Bearer KEY'})</code> <code>requests.get(url, timeout=10)</code> Set timeout <code>requests.get(url, timeout=10)</code>"},{"location":"03-data/data-collection/#pagination-pattern-template","title":"Pagination Pattern Template","text":"<pre><code>def fetch_all_pages(base_url):\n    results = []\n    url = base_url\n\n    while url:\n        response = requests.get(url)\n        if response.status_code == 200:\n            data = response.json()\n            results.extend(data['results'])\n            url = data.get('next')  # Or handle your API's pagination\n        else:\n            break\n\n    return results\n</code></pre>"},{"location":"03-data/data-collection/#error-handling-template","title":"Error Handling Template","text":"<pre><code>def safe_api_request(url, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, timeout=10)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            print(f\"Attempt {attempt + 1} failed: {e}\")\n            time.sleep(2 ** attempt)  # Exponential backoff\n    return None\n</code></pre>"},{"location":"03-data/data-collection/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned to make HTTP GET requests and parse JSON responses, handle pagination to collect complete datasets, respect rate limits and implement delays, transform API data into pandas DataFrames, and handle common errors and network issues.</p> <p>Critical insights:</p> <ul> <li>APIs are structured: Every API follows request-response patterns with consistent status codes</li> <li>Documentation matters: Reading docs prevents hours of trial-and-error debugging</li> <li>Be respectful: Rate limits exist for good reasons; always add delays</li> <li>Errors happen: Network issues are normal; build retry logic and error handling</li> </ul> <p>What's next:</p> <p>With API data collection skills, you can now build automated data pipelines that fetch fresh information daily. The next step is combining API data with other sources and preprocessing it for analysis.</p> <p>Practice resources:</p> <ul> <li>SWAPI - Star Wars API - free, no authentication required</li> <li>JSONPlaceholder - fake API for testing</li> <li>Public APIs List - hundreds of free APIs to practice with</li> </ul> <p>External resources:</p> <ul> <li>requests Documentation - comprehensive guide to the requests library</li> <li>HTTP Status Codes - understand what each code means</li> <li>REST API Tutorial - deeper dive into API design patterns</li> </ul> <p>Remember: APIs unlock real-time data. Master these patterns, and you'll build systems that stay current instead of relying on stale files. ```</p>"},{"location":"03-data/data-foundations/","title":"Data: The Foundation of Machine Learning","text":"<p>This tutorial introduces the essential role of data in machine learning and walks through the key phases of preparing data for modeling. You'll understand why data quality matters more than algorithm choice and learn the systematic approach to handling data from collection to model-ready format.</p> <p>Estimated time: 40 minutes</p>"},{"location":"03-data/data-foundations/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: </p> <p>The most sophisticated machine learning algorithm is worthless without good data. </p> <p>Models learn patterns from data, so if your data is biased, incomplete, or messy, your predictions will be unreliable regardless of model complexity. The phrase \"garbage in, garbage out\" is fundamental, because poor data quality leads directly to poor model performance.</p> <p>Practical benefits: Understanding data workflows enables you to spot quality issues early, choose appropriate preprocessing strategies, and build robust ML systems that work in production. </p> <p>Professional context: Industry surveys consistently show that data scientists spend 60-80% of their time on data preparation rather than modeling. Companies with strong data infrastructure outperform competitors regardless of algorithm sophistication. The most impactful ML practitioners aren't necessarily those who know the fanciest algorithms; they're the ones who understand their data deeply, handle it correctly, and ensure quality throughout the pipeline.</p> <p></p>"},{"location":"03-data/data-foundations/#core-concepts","title":"Core Concepts","text":""},{"location":"03-data/data-foundations/#what-is-data-in-ml-context","title":"What is Data in ML Context?","text":"<p>In machine learning, data consists of examples (also called samples, observations, or rows) with features (attributes, variables, or columns) and optionally labels (target values for supervised learning).</p> <p>Example: Predicting house prices</p> <ul> <li>Sample: One house</li> <li>Features: Square footage, number of bedrooms, location, age, etc.</li> <li>Label: Sale price (what we're trying to predict)</li> </ul>"},{"location":"03-data/data-foundations/#types-of-data","title":"Types of Data","text":"<p>Structured vs Unstructured:</p> <ul> <li>Structured: Organized in tables with defined columns (databases, spreadsheets)</li> <li>Unstructured: No predefined format (text, images, audio, video)</li> </ul> <p>Numerical vs Categorical:</p> <ul> <li>Numerical: Quantitative values (age: 25, temperature: 18.5\u00b0C)</li> <li>Categorical: Discrete categories (color: red/blue/green, city: Tirana/Durr\u00ebs)</li> </ul> <p>Time-series: Data points ordered by time (stock prices, sensor readings, web traffic)</p>"},{"location":"03-data/data-foundations/#data-quality-dimensions","title":"Data Quality Dimensions","text":"<p>Completeness: Are all expected values present? Missing data can bias results.</p> <p>Accuracy: Do values reflect reality? Measurement errors propagate through models.</p> <p>Consistency: Do values follow expected formats and rules? Inconsistencies cause processing failures.</p> <p>Relevance: Do features relate to your prediction task? Irrelevant features add noise.</p>"},{"location":"03-data/data-foundations/#the-data-pipeline-key-phases","title":"The Data Pipeline: Key Phases","text":"<p>The machine learning data pipeline is a series of steps that transform raw data into a format suitable for training models. Think of it as a factory assembly line where raw materials become finished products.</p> <p></p>"},{"location":"03-data/data-foundations/#phase-1-data-collection","title":"Phase 1: Data Collection","text":"<p>Purpose: Gather data from various sources to create your dataset.</p> <p>Common sources:</p> <ul> <li>Databases: SQL/NoSQL databases (MySQL, PostgreSQL, MongoDB)</li> <li>APIs: Web services providing data programmatically</li> <li>Web scraping: Extracting data from websites</li> <li>Files: CSV, JSON, Excel files</li> <li>Sensors/IoT: Real-time streaming data</li> <li>User input: Forms, surveys, clicks</li> </ul> <p>Key considerations:</p> <ul> <li>Representative samples: Data should reflect the real-world distribution you'll encounter</li> <li>Bias awareness: Who collected this data? What populations are missing?</li> <li>Volume: Do you have enough samples for your task? Rule of thumb: at least 10 samples per feature</li> </ul> <p>How you collect data determines what patterns your model can learn. </p> <p>If your training data only includes customers from one region, your model won't work well for other regions.</p> <p>Example: Building a spam detector</p> <ul> <li>Good collection: Emails from diverse users, timeframes, and domains</li> <li>Poor collection: Only emails from one company's inbox over one week</li> </ul>"},{"location":"03-data/data-foundations/#phase-2-data-exploration-eda","title":"Phase 2: Data Exploration (EDA)","text":"<p>Purpose: Understand what you have before making changes.</p> <p>Key questions:</p> <ul> <li>What does the data look like? (first few rows)</li> <li>What types are the features? (numerical, categorical, dates)</li> <li>What are the distributions? (histograms, box plots)</li> <li>Are there obvious outliers or anomalies?</li> <li>What patterns exist? (correlations, trends)</li> <li>What's missing? (null values, empty fields)</li> </ul> <p>Tools: Summary statistics (mean, median, std), visualizations (histograms, scatter plots), correlation matrices</p> <p>ML connection: Exploration guides your modeling choices. If features are highly correlated, some algorithms may struggle. If classes are imbalanced, accuracy might be misleading. If distributions are skewed, you might need transformations.</p> <p>Real example: Exploring customer data reveals that 95% are from age 25-35. This tells you your model might not work well for older customers\u2014you need more diverse data or age-specific models.</p>"},{"location":"03-data/data-foundations/#phase-3-data-cleaning","title":"Phase 3: Data Cleaning","text":"<p>Purpose: Fix or remove problems that would cause errors or bias.</p> <p>Common tasks:</p> <p>Handling missing values:</p> <ul> <li>Remove: Delete rows/columns with missing data (if small percentage)</li> <li>Impute: Fill with mean, median, mode, or predicted values</li> <li>Flag: Create indicator variable showing missingness</li> </ul> <p>Dealing with duplicates:</p> <ul> <li>Identify exact or near-duplicates</li> <li>Remove or merge duplicate records</li> </ul> <p>Fixing inconsistencies:</p> <ul> <li>Standardize formats (dates: <code>YYYY-MM-DD</code>, names: Title Case)</li> <li>Correct typos and data entry errors</li> <li>Resolve conflicting values</li> </ul> <p>Handling outliers:</p> <ul> <li>Detect using statistical methods (Z-score, IQR)</li> <li>Decide: Remove, cap, or keep based on domain knowledge</li> </ul> <p>ML connection: Dirty data creates noisy patterns that models learn incorrectly. One extreme outlier can skew an entire regression line. Missing values handled poorly can introduce bias or cause crashes during training.</p> <p></p>"},{"location":"03-data/data-foundations/#phase-4-data-transformation","title":"Phase 4: Data Transformation","text":"<p>Purpose: Convert data into forms that algorithms can process effectively.</p> <p>Common transformations:</p> <p>Scaling and normalization:</p> <ul> <li>Min-max scaling: Squash values to  range</li> <li>Standardization: Center at 0 with std dev of 1</li> <li>Why: Prevents features with large values from dominating distance-based algorithms</li> </ul> <p>Encoding categorical variables:</p> <ul> <li>Label encoding: Assign numbers (red=0, blue=1, green=2)</li> <li>One-hot encoding: Create binary columns for each category</li> <li>Why: Most ML algorithms require numerical input</li> </ul> <p>Feature engineering:</p> <ul> <li>Combining features: Create \"price per square foot\" from price and area</li> <li>Extracting components: Pull day-of-week from datetime</li> <li>Binning: Convert continuous age to age groups</li> <li>Why: New features can capture relationships better than original data</li> </ul> <p>ML connection: Proper scaling is critical for gradient descent (used in neural networks) and distance-based algorithms (k-NN, k-means). Without it, features with larger scales dominate, leading to poor performance.</p> <p>Example: In predicting house prices</p> <ul> <li>Feature 1: Square footage (500-5000)</li> <li>Feature 2: Number of bedrooms (1-5)</li> </ul> <p>Without scaling, square footage dominates. After standardization, both contribute equally.</p>"},{"location":"03-data/data-foundations/#phase-5-data-splitting","title":"Phase 5: Data Splitting","text":"<p>Purpose: Separate data to train models and evaluate them fairly.</p> <p>Standard split:</p> <ul> <li>Training set (70-80%): Used to train the model</li> <li>Validation set (10-15%): Used to tune hyperparameters and select models</li> <li>Test set (10-15%): Used only once at the end for final evaluation</li> </ul> <p>Why split? A model evaluated on training data appears better than it actually is (overfitting). Testing on unseen data reveals true performance.</p> <p>ML connection: Without proper splitting, you can't trust performance metrics. A model with 99% accuracy on training data might have 60% on new data. The test set simulates how your model will perform in production.</p> <p>Critical rule: NEVER look at test set during development. It's your final \"exam\"\u2014use it only once.</p> <p></p>"},{"location":"03-data/data-foundations/#phase-6-data-storage-versioning","title":"Phase 6: Data Storage &amp; Versioning","text":"<p>Purpose: Organize datasets for reproducibility and collaboration.</p> <p>Best practices:</p> <ul> <li>Keep raw data separate: Never modify original files</li> <li>Version datasets: Track changes like you track code (DVC, Git LFS)</li> <li>Document transformations: Record every step applied</li> <li>Use consistent naming: <code>data_raw.csv</code>, <code>data_cleaned.csv</code>, <code>data_train.csv</code></li> </ul> <p>ML connection: When model performance changes, you need to know if it's due to code changes or data changes. Versioning enables debugging, auditing, and reproducing results months later.</p>"},{"location":"03-data/data-foundations/#common-data-challenges","title":"Common Data Challenges","text":""},{"location":"03-data/data-foundations/#insufficient-data","title":"Insufficient Data","text":"<p>Problem: Too few samples for algorithms to learn patterns effectively.</p> <p>Impact: High variance, poor generalization, overfitting to noise.</p> <p>Solutions: Data augmentation, transfer learning, collect more data.</p>"},{"location":"03-data/data-foundations/#imbalanced-data","title":"Imbalanced Data","text":"<p>Problem: Unequal representation of classes (95% normal, 5% fraud).</p> <p>Impact: Models predict majority class for everything, achieving high accuracy but missing minority class entirely.</p> <p>Solutions: Resampling, cost-sensitive learning, different metrics (F1-score, not accuracy).</p>"},{"location":"03-data/data-foundations/#high-dimensionality","title":"High Dimensionality","text":"<p>Problem: Too many features relative to samples (curse of dimensionality).</p> <p>Impact: Models require exponentially more data, distance metrics become meaningless, increased computation.</p> <p>Solutions: Feature selection, dimensionality reduction (PCA), regularization.</p>"},{"location":"03-data/data-foundations/#data-drift","title":"Data Drift","text":"<p>Problem: Production data distribution differs from training data over time.</p> <p>Impact: Model performance degrades silently in production.</p> <p>Solutions: Continuous monitoring, periodic retraining, drift detection systems.</p>"},{"location":"03-data/data-foundations/#best-practices","title":"Best Practices","text":"<p>Start with exploration: Look at your data before deciding on approaches. One hour of exploration saves ten hours of debugging.</p> <p>Document everything: Record data sources, transformations, and decisions. Your future self will thank you.</p> <p>Version control datasets: Treat data like code. Track changes and tag versions used for specific models.</p> <p>Check for bias early: Examine demographic representation, temporal coverage, and geographic diversity before investing in modeling.</p> <p>Validate continuously: Don't wait until the end to check data quality. Validate at each pipeline stage.</p> <p>Keep raw data untouched: Always maintain original data. Apply transformations in code, never by editing files directly.</p> <p>Test pipeline with small samples: Verify your entire workflow on small data before processing millions of records.</p>"},{"location":"03-data/data-foundations/#quick-reference-data-pipeline-phases","title":"Quick Reference: Data Pipeline Phases","text":"Phase Key Question Tools/Techniques Next Tutorial Collection Where does data come from? APIs, databases, web scraping Data Collection Exploration What does data look like? Summary stats, visualizations Pandas, Visualization Cleaning What's wrong with data? Handle missing, duplicates, outliers Data Wrangling Transformation How to prepare for models? Scaling, encoding, feature engineering Pandas, NumPy Splitting How to evaluate fairly? Train/val/test split, cross-validation Pandas Storage How to organize and track? File structures, versioning, documentation Data Collection"},{"location":"03-data/data-foundations/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned why data quality determines ML success, understood the systematic data pipeline workflow, recognized common data challenges and their impacts, and connected proper data handling to model performance and production readiness.</p> <p>Critical insights:</p> <ul> <li>80/20 rule in practice: Most ML work is data preparation, not modeling</li> <li>Quality over quantity: Small, clean datasets often outperform large, messy ones</li> <li>Domain knowledge matters: Understanding context helps make better data decisions</li> <li>Iteration is normal: Data work is cyclical\u2014exploration reveals cleaning needs, cleaning enables better exploration</li> </ul> <p>Connections to previous topics:</p> <ul> <li>Statistics: Use mean, variance, distributions to understand data patterns</li> <li>Probability: Sampling strategies affect what patterns models can learn</li> <li>Linear algebra: Data is organized as matrices (rows = samples, columns = features)</li> </ul> <p>What's next in this section:</p> <p>Pandas Essentials: Load, filter, transform, and aggregate data efficiently</p> <p>Visualization: Create plots to explore patterns and communicate insights</p> <p>Data Wrangling: Handle missing values, merge datasets, reshape data structures</p> <p>Data Collection: APIs, web scraping, and database queries for gathering data</p> <p>SQL &amp; NoSQL: Query structured and unstructured data stores</p> <p>Each tutorial builds hands-on skills for specific pipeline phases. By the end, you'll have a complete workflow from raw data to model-ready datasets.</p> <p>External resources:</p> <ul> <li>Google's Machine Learning Crash Course: Data Preparation - practical data prep strategies</li> <li>Kaggle Learn: Data Cleaning - interactive data cleaning exercises</li> </ul> <p>Remember: Great models start with great data. Master the pipeline, and you'll build better ML systems than those who chase fancy algorithms with messy data.</p>"},{"location":"03-data/data-processing/","title":"Data Preprocessing: Preparing Data for Analysis","text":"<p>This tutorial covers the systematic process of preparing raw data for machine learning. You'll learn to identify data quality issues, apply appropriate corrections, and structure datasets so models can learn effectively from clean, consistent input.</p> <p>Estimated time: 50 minutes</p>"},{"location":"03-data/data-processing/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: </p> <p>Raw data is never ready for models.</p> <p>Real-world data arrives messy; missing values scatter throughout columns, categories use inconsistent spellings, numeric ranges vary wildly, and formats contradict each other. </p> <p>Models require clean, numeric matrices with no gaps or errors. </p> <p>The work between receiving data and training a model is data preprocessing, and it determines whether your model succeeds or fails.</p> <p>Practical benefits: Preprocessing skills let you spot problems early, fix them systematically, and deliver data that models can actually use. </p> <p>Professional context: Data scientists report spending 60-80% of their time on data preparation. Companies with rigorous preprocessing workflows build reliable systems; those that skip preprocessing steps face model failures in production. </p> <p>Preprocessing isn't optional. it's the foundation. Every successful data </p> <p></p>"},{"location":"03-data/data-processing/#core-concepts","title":"Core Concepts","text":""},{"location":"03-data/data-processing/#understanding-the-terminology","title":"Understanding the Terminology","text":"<p>Data work involves several related but distinct activities. Understanding what each term means helps you communicate clearly with colleagues and organize your work.</p> <p>Data Engineering: Building infrastructure to collect, move, store, and version data at scale. Data engineers create pipelines that deliver data from databases, APIs, and files to where analysts need it. This tutorial assumes data is already accessible.</p> <p>Data Preprocessing: The systematic process of cleaning errors, handling missing values, standardizing formats, and converting data into numeric forms that models require. This is the focus of this tutorial.</p> <p>Feature Engineering: Creating new variables from existing data to improve model predictions. This is a subset of preprocessing focused specifically on building better inputs.</p> <p>Data Cleansing: Fixing quality problems; correcting errors, removing duplicates, standardizing inconsistent formats, and handling missing values.</p> <p>Data Transformation: Converting data types and scaling numeric values so all features contribute appropriately to model training.</p> <p>Data Munging/Wrangling: Reshaping data structures; pivoting, melting, merging tables\u2014to get data into the right format.</p> <p>Exploratory Data Analysis (EDA): Examining data visually and statistically to understand distributions, spot outliers, identify patterns, and determine which preprocessing steps are needed.</p> <p></p>"},{"location":"03-data/data-processing/#the-preprocessing-workflow","title":"The Preprocessing Workflow","text":"<p>Preprocessing follows a systematic sequence, though you'll often loop back to earlier steps as you discover new issues.</p> <p>1. Explore \u2192 2. Clean \u2192 3. Transform \u2192 4. Validate \u2192 5. Split \u2192 6. Apply</p> <p>Each phase has specific goals and standard techniques. Skip a phase, and you'll likely face problems during training or deployment.</p>"},{"location":"03-data/data-processing/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"03-data/data-processing/#phase-1-exploratory-analysis-guides-preprocessing","title":"Phase 1: Exploratory Analysis Guides Preprocessing","text":"<p>Discover what preprocessing your data needs before making changes.</p> <p>Start by examining data visually and numerically to identify issues. Each pattern you find suggests specific preprocessing steps.</p> <p>Quick EDA workflow:</p> <pre><code>import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load data\ndf = pd.read_csv('raw_data.csv')\n\n# Structure check\nprint(f\"Shape: {df.shape}\")\nprint(f\"\\nData types:\\n{df.dtypes}\")\nprint(f\"\\nFirst rows:\\n{df.head()}\")\n\n# Missing values\nprint(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n\n# Numeric distributions\nprint(f\"\\nNumeric summary:\\n{df.describe()}\")\n\n# Categorical distributions\nfor col in df.select_dtypes(include='object').columns:\n    print(f\"\\n{col} value counts:\\n{df[col].value_counts()}\")\n</code></pre> <p>Visual checks:</p> <pre><code># Distribution of numeric features\ndf.hist(figsize=(12, 8), bins=30)\nplt.tight_layout()\nplt.show()\n\n# Box plots reveal outliers\nfor col in df.select_dtypes(include='number').columns:\n    plt.figure(figsize=(6, 4))\n    sns.boxplot(x=df[col])\n    plt.title(f'{col} Distribution')\n    plt.show()\n\n# Correlation heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm')\nplt.title('Feature Correlations')\nplt.show()\n</code></pre> <p>Pattern \u2192 Preprocessing action mapping:</p> What EDA Shows Preprocessing Needed Skewed distribution (histogram) Log or power transformation Extreme outliers (box plot) Capping, removal, or investigation Missing values &gt;5% Imputation strategy needed Inconsistent categories Standardization required Wide numeric ranges Scaling required High correlation (&gt;0.9) Consider removing redundant features <p>For comprehensive EDA techniques, see the Visualization tutorial.</p>"},{"location":"03-data/data-processing/#phase-2-data-cleansing","title":"Phase 2: Data Cleansing","text":"<p>Purpose: Fix quality problems that cause model errors or introduce bias.</p>"},{"location":"03-data/data-processing/#21-handling-missing-values","title":"2.1 Handling Missing Values","text":"<p>Missing data is common in real datasets. Your strategy depends on how much is missing and why.</p> <p>Detection:</p> <pre><code># Missing value summary\nmissing_summary = pd.DataFrame({\n    'column': df.columns,\n    'missing_count': df.isnull().sum(),\n    'missing_pct': (df.isnull().sum() / len(df)) * 100\n})\nmissing_summary = missing_summary[missing_summary['missing_count'] &gt; 0]\nprint(missing_summary.sort_values('missing_pct', ascending=False))\n</code></pre> <p>Decision framework:</p> <ul> <li>Less than 5% missing: Usually safe to drop those rows</li> <li>5-30% missing: Impute with statistical measures</li> <li>More than 30% missing: Consider dropping the column or using advanced methods</li> <li>Missing has pattern: Investigate why values are missing</li> </ul> <p>Strategies:</p> <pre><code># Strategy 1: Drop rows (only if missing is minimal)\ndf_clean = df.dropna(subset=['critical_column'])\n\n# Strategy 2: Fill numeric with median (robust to outliers)\ndf['age'].fillna(df['age'].median(), inplace=True)\n\n# Strategy 3: Fill categorical with mode\ndf['category'].fillna(df['category'].mode(), inplace=True)\n\n# Strategy 4: Fill based on groups\ndf['salary'] = df.groupby('department')['salary'].transform(\n    lambda x: x.fillna(x.median())\n)\n\n# Strategy 5: Create missing indicator\ndf['age_was_missing'] = df['age'].isnull().astype(int)\ndf['age'].fillna(df['age'].median(), inplace=True)\n</code></pre> <p>For pandas mechanics, see the Pandas tutorial - Handling Missing Data.</p>"},{"location":"03-data/data-processing/#22-removing-duplicates","title":"2.2 Removing Duplicates","text":"<p>Detection:</p> <pre><code># Exact duplicates\nduplicates = df.duplicated()\nprint(f\"Exact duplicates: {duplicates.sum()}\")\n\n# Duplicates on specific columns\nduplicates_subset = df.duplicated(subset=['customer_id', 'date'])\nprint(f\"Duplicate transactions: {duplicates_subset.sum()}\")\n\n# View duplicates\ndf[df.duplicated(keep=False)].sort_values('customer_id')\n</code></pre> <p>Removal:</p> <pre><code># Remove exact duplicates, keep first occurrence\ndf_clean = df.drop_duplicates()\n\n# Remove based on subset of columns\ndf_clean = df.drop_duplicates(subset=['customer_id', 'date'], keep='first')\n</code></pre> <p>When duplicates are valid: Sometimes duplicate rows represent real repeated events. Check with domain experts before removing.</p>"},{"location":"03-data/data-processing/#23-fixing-inconsistencies","title":"2.3 Fixing Inconsistencies","text":"<p>Standardizing text:</p> <pre><code># Lowercase and strip whitespace\ndf['city'] = df['city'].str.lower().str.strip()\n\n# Fix common typos\ndf['city'] = df['city'].replace({\n    'new york': 'new_york',\n    'ny': 'new_york',\n    'newyork': 'new_york'\n})\n\n# Standardize date formats\ndf['date'] = pd.to_datetime(df['date'], errors='coerce')\n</code></pre> <p>Resolving contradictions:</p> <pre><code># Example: End date before start date\ninvalid_dates = df[df['end_date'] &lt; df['start_date']]\nprint(f\"Invalid date ranges: {len(invalid_dates)}\")\n\n# Fix by investigation or removal\ndf = df[df['end_date'] &gt;= df['start_date']]\n</code></pre>"},{"location":"03-data/data-processing/#24-handling-outliers","title":"2.4 Handling Outliers","text":"<p>Detection methods:</p> <pre><code># Method 1: Z-score (assumes normal distribution)\nfrom scipy import stats\nz_scores = np.abs(stats.zscore(df['price']))\noutliers_z = df[z_scores &gt; 3]\n\n# Method 2: IQR (more robust)\nQ1 = df['price'].quantile(0.25)\nQ3 = df['price'].quantile(0.75)\nIQR = Q3 - Q1\noutliers_iqr = df[(df['price'] &lt; Q1 - 1.5*IQR) | (df['price'] &gt; Q3 + 1.5*IQR)]\n\nprint(f\"Outliers found: {len(outliers_iqr)}\")\n</code></pre> <p>Treatment options:</p> <pre><code># Option 1: Remove outliers\ndf_no_outliers = df[(df['price'] &gt;= Q1 - 1.5*IQR) &amp; (df['price'] &lt;= Q3 + 1.5*IQR)]\n\n# Option 2: Cap outliers\ndf['price_capped'] = df['price'].clip(lower=Q1 - 1.5*IQR, upper=Q3 + 1.5*IQR)\n\n# Option 3: Keep outliers but flag them\ndf['is_outlier'] = ((df['price'] &lt; Q1 - 1.5*IQR) | (df['price'] &gt; Q3 + 1.5*IQR)).astype(int)\n</code></pre> <p>When to keep outliers: Domain knowledge is critical. A $10M house sale might be valid in certain markets. Always investigate before removing.</p>"},{"location":"03-data/data-processing/#phase-3-data-transformation","title":"Phase 3: Data Transformation","text":"<p>Convert clean data into numeric formats with appropriate scales.</p>"},{"location":"03-data/data-processing/#31-scaling-numeric-features","title":"3.1 Scaling Numeric Features","text":"<p>Different features often have vastly different ranges. Scaling puts them on comparable scales.</p> <p>Why scaling matters: Features with large values (e.g., income: \\(20,000-\\)150,000) can dominate features with small values (e.g., number of children: 0-5) during model training. Scaling prevents this.</p> <p>Scaling techniques:</p> <pre><code>from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n\n# Min-Max Scaling: Scales to  range\nscaler = MinMaxScaler()\ndf[['age', 'income']] = scaler.fit_transform(df[['age', 'income']])\n\n# Standardization: Mean=0, Std=1\nscaler = StandardScaler()\ndf[['age', 'income']] = scaler.fit_transform(df[['age', 'income']])\n\n# Robust Scaling: Uses median and IQR (good with outliers)\nscaler = RobustScaler()\ndf[['age', 'income']] = scaler.fit_transform(df[['age', 'income']])\n</code></pre> <p>Before and after comparison:</p> <pre><code># Visualize scaling effect\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Before scaling\naxes.hist(df_original['income'], bins=30)\naxes.set_title('Before Scaling')\naxes.set_xlabel('Income')\n\n# After scaling\naxes.hist(df['income'], bins=30)\naxes.set_title('After Scaling')\naxes.set_xlabel('Scaled Income')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Critical warning: Fit scalers only on training data, then apply to validation and test sets. Fitting on all data causes data leakage.</p> <pre><code># CORRECT approach\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)  # Fit on training only\nX_test_scaled = scaler.transform(X_test)        # Apply to test\n\n# WRONG approach\n# scaler.fit_transform(X)  # Don't fit on entire dataset!\n</code></pre>"},{"location":"03-data/data-processing/#32-encoding-categorical-variables","title":"3.2 Encoding Categorical Variables","text":"<p>Models require numeric input, but many features are categorical. Encoding converts categories to numbers.</p> <p>One-Hot Encoding (most common):</p> <p>Creates binary columns for each category.</p> <pre><code># Using pandas\ndf_encoded = pd.get_dummies(df, columns=['city', 'color'], drop_first=True)\n\n# Using sklearn\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(sparse=False, drop='first')\nencoded = encoder.fit_transform(df[['city', 'color']])\nencoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out())\n</code></pre> <p>Example:</p> Color \u2192 Color_Blue Color_Green Color_Red Red 0 0 1 Blue 1 0 0 Green 0 1 0 <p>Warning: One-hot encoding creates many columns with high-cardinality features (e.g., 50 unique cities \u2192 50 new columns). This causes the \"curse of dimensionality.\"</p> <p>Label Encoding:</p> <p>Assigns integers to categories. Use only for ordinal data (categories with natural order).</p> <pre><code># Ordinal example: education level has order\nfrom sklearn.preprocessing import LabelEncoder\n\n# Define order\neducation_order = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\ndf['education_encoded'] = df['education'].map(education_order)\n\n# Or use LabelEncoder (but it assigns arbitrary order)\nencoder = LabelEncoder()\ndf['city_encoded'] = encoder.fit_transform(df['city'])\n</code></pre> <p>Danger: Using label encoding on nominal categories (no natural order) misleads models into thinking higher numbers mean \"more\" of something.</p>"},{"location":"03-data/data-processing/#33-feature-engineering-basics","title":"3.3 Feature Engineering Basics","text":"<p>Creating new features from existing data often improves model performance more than choosing better models.</p> <p>Common transformations:</p> <pre><code># Ratios and rates\ndf['price_per_sqft'] = df['price'] / df['square_feet']\ndf['debt_to_income'] = df['debt'] / df['income']\n\n# Binning continuous variables\ndf['age_group'] = pd.cut(df['age'], bins=,[2][11][12][13][14]\n                          labels=['&lt;18', '18-35', '36-50', '51-65', '65+'])\n\n# Date/time features\ndf['date'] = pd.to_datetime(df['date'])\ndf['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\ndf['day_of_week'] = df['date'].dt.dayofweek\ndf['is_weekend'] = df['day_of_week'].isin().astype(int)\n\n# Interaction features\ndf['price_x_quality'] = df['price'] * df['quality_score']\n\n# Polynomial features\ndf['age_squared'] = df['age'] ** 2\n</code></pre> <p>For more on creating columns, see the Pandas tutorial - Creating New Columns.</p>"},{"location":"03-data/data-processing/#phase-4-avoiding-bias-in-preprocessing","title":"Phase 4: Avoiding Bias in Preprocessing","text":"<p>Recognize and prevent bias introduced during data preparation.</p> <p>Preprocessing choices can inadvertently introduce bias that degrades model performance or creates unfair outcomes. Three main sources require attention:</p>"},{"location":"03-data/data-processing/#41-data-leakage","title":"4.1 Data Leakage","text":"<p>Problem: Using information that won't be available when making predictions.</p> <p>Common causes:</p> <pre><code># WRONG: Scaling before splitting\nscaler.fit(X)  # Fit on entire dataset\nX_train, X_test = train_test_split(X, test_size=0.2)\n\n# CORRECT: Split first, then scale\nX_train, X_test = train_test_split(X, test_size=0.2)\nscaler.fit(X_train)  # Fit only on training\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</code></pre> <p>Target leakage example:</p> <pre><code># WRONG: Creating feature that contains target information\ndf['has_churned'] = (df['last_activity'] == df['churn_date']).astype(int)\n# Problem: 'churn_date' is only known after customer churns!\n\n# CORRECT: Use information available before prediction\ndf['days_since_last_activity'] = (df['today'] - df['last_activity']).dt.days\n</code></pre>"},{"location":"03-data/data-processing/#42-sampling-bias","title":"4.2 Sampling Bias","text":"<p>Problem: Training and test sets don't represent the same population.</p> <p>Solutions:</p> <pre><code># Use stratified splitting for imbalanced classes\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# For time-series: Use temporal splitting (never random)\ntrain_cutoff = df['date'].quantile(0.8)\ntrain = df[df['date'] &lt;= train_cutoff]\ntest = df[df['date'] &gt; train_cutoff]\n</code></pre>"},{"location":"03-data/data-processing/#43-preprocessing-induced-bias","title":"4.3 Preprocessing-Induced Bias","text":"<p>Problem: Preprocessing choices that favor majority groups or create unfair patterns.</p> <p>Example: Imputation bias</p> <pre><code># Problem: Filling missing income with overall median\n# If group A has missing values but different income distribution,\n# this biases predictions for group A\ndf['income'].fillna(df['income'].median(), inplace=True)\n\n# Better: Fill based on relevant groups\ndf['income'] = df.groupby('region')['income'].transform(\n    lambda x: x.fillna(x.median())\n)\n</code></pre> <p>Proxy variables: Some features indirectly encode protected attributes (e.g., ZIP code strongly correlates with race). Be aware of these relationships.</p>"},{"location":"03-data/data-processing/#phase-5-the-complete-preprocessing-pipeline","title":"Phase 5: The Complete Preprocessing Pipeline","text":"<p>Purpose: Apply all steps in the correct order.</p> <p>Here's the standard sequence for a complete preprocessing workflow:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\n\n# 1. Load data\ndf = pd.read_csv('raw_data.csv')\n\n# 2. Initial exploration\nprint(df.info())\nprint(df.describe())\nprint(df.isnull().sum())\n\n# 3. Remove duplicates\ndf = df.drop_duplicates()\n\n# 4. Handle missing values (before splitting)\n# Drop columns with &gt;50% missing\nhigh_missing = df.columns[df.isnull().mean() &gt; 0.5]\ndf = df.drop(columns=high_missing)\n\n# 5. Fix data types\ndf['date'] = pd.to_datetime(df['date'])\ndf['category'] = df['category'].astype('category')\n\n# 6. Remove obvious outliers (domain-specific)\ndf = df[df['age'] &gt; 0]\ndf = df[df['price'] &gt; 0]\n\n# 7. Feature engineering (before splitting)\ndf['price_per_unit'] = df['price'] / df['units']\ndf['year'] = df['date'].dt.year\n\n# 8. Separate features and target\nX = df.drop('target', axis=1)\ny = df['target']\n\n# 9. CRITICAL: Split data BEFORE fitting any transformations\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 10. Impute missing values (fit on training only)\nimputer = SimpleImputer(strategy='median')\nnumeric_cols = X_train.select_dtypes(include='number').columns\nX_train[numeric_cols] = imputer.fit_transform(X_train[numeric_cols])\nX_test[numeric_cols] = imputer.transform(X_test[numeric_cols])\n\n# 11. Encode categorical variables (fit on training only)\ncategorical_cols = X_train.select_dtypes(include='object').columns\nencoder = OneHotEncoder(sparse=False, drop='first', handle_unknown='ignore')\nX_train_cat = encoder.fit_transform(X_train[categorical_cols])\nX_test_cat = encoder.transform(X_test[categorical_cols])\n\n# 12. Scale numeric features (fit on training only)\nscaler = StandardScaler()\nX_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\nX_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n\n# 13. Combine encoded and scaled features\nX_train_final = np.hstack([X_train[numeric_cols].values, X_train_cat])\nX_test_final = np.hstack([X_test[numeric_cols].values, X_test_cat])\n\n# 14. Verify shapes\nprint(f\"Training shape: {X_train_final.shape}\")\nprint(f\"Test shape: {X_test_final.shape}\")\n\n# 15. Save preprocessing objects for production use\nimport joblib\njoblib.dump(imputer, 'imputer.pkl')\njoblib.dump(encoder, 'encoder.pkl')\njoblib.dump(scaler, 'scaler.pkl')\n</code></pre> <p>Key sequence points:</p> <ol> <li>Explore before changing anything</li> <li>Remove duplicates early</li> <li>Basic cleaning before feature engineering</li> <li>Split data BEFORE fitting transformations</li> <li>Fit all transformations on training data only</li> <li>Apply transformations to test data</li> <li>Save fitted transformers for production</li> </ol>"},{"location":"03-data/data-processing/#common-preprocessing-challenges","title":"Common Preprocessing Challenges","text":""},{"location":"03-data/data-processing/#imbalanced-classes","title":"Imbalanced Classes","text":"<p>Problem: One class has far more samples (e.g., 95% class A, 5% class B).</p> <p>Impact: Models predict majority class for everything, achieving high accuracy but missing minority class entirely.</p> <p>Solutions:</p> <pre><code># Check balance\nprint(y.value_counts(normalize=True))\n\n# Option 1: Oversample minority class\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n\n# Option 2: Undersample majority class\nfrom imblearn.under_sampling import RandomUnderSampler\nundersampler = RandomUnderSampler(random_state=42)\nX_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n</code></pre>"},{"location":"03-data/data-processing/#high-dimensional-data","title":"High-Dimensional Data","text":"<p>Problem: Too many features relative to samples.</p> <p>Impact: Models require more data, training slows, and performance suffers.</p> <p>Solutions:</p> <pre><code># Option 1: Remove low-variance features\nfrom sklearn.feature_selection import VarianceThreshold\nselector = VarianceThreshold(threshold=0.01)\nX_reduced = selector.fit_transform(X)\n\n# Option 2: Remove highly correlated features\ncorr_matrix = X.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\nto_drop = [column for column in upper.columns if any(upper[column] &gt; 0.95)]\nX_reduced = X.drop(columns=to_drop)\n\n# Option 3: Use feature importance from a model\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X_train, y_train)\nimportances = pd.Series(rf.feature_importances_, index=X_train.columns)\ntop_features = importances.nlargest(20).index\nX_reduced = X[top_features]\n</code></pre>"},{"location":"03-data/data-processing/#memory-issues-with-large-datasets","title":"Memory Issues with Large Datasets","text":"<p>Problem: Dataset doesn't fit in memory.</p> <p>Solutions:</p> <pre><code># Option 1: Load in chunks\nchunk_size = 10000\nfor chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):\n    # Process each chunk\n    processed = preprocess_chunk(chunk)\n    # Save or aggregate results\n\n# Option 2: Use data types to reduce memory\ndf['age'] = df['age'].astype('int8')  # Instead of int64\ndf['category'] = df['category'].astype('category')  # Instead of object\n\n# Option 3: Select relevant columns only\nrelevant_cols = ['col1', 'col2', 'col3']\ndf = pd.read_csv('large_file.csv', usecols=relevant_cols)\n</code></pre>"},{"location":"03-data/data-processing/#best-practices","title":"Best Practices","text":"<p>Always explore before preprocessing: Spend 30 minutes examining data before making any changes. Understanding what you have prevents mistakes.</p> <p>Document every decision: Write comments explaining why you chose specific thresholds, imputation methods, or transformations. Future you will need this.</p> <p>Split data early: Separate training and test sets immediately after loading. Never fit transformations on test data.</p> <p>Validate after each step: Check data shape, null counts, and sample values after every preprocessing operation to catch errors immediately.</p> <p>Keep raw data untouched: Never modify original files. Apply all changes in code for reproducibility.</p> <p>Version preprocessing code with models: A model is worthless without knowing how its input was prepared. Track them together.</p> <p>Test pipeline with small samples: Run your full workflow on 1,000 rows before processing millions. Catch bugs fast.</p> <p>Save fitted transformers: Production systems need the exact scalers, encoders, and imputers used during training. Save them as files.</p>"},{"location":"03-data/data-processing/#quick-reference","title":"Quick Reference","text":""},{"location":"03-data/data-processing/#preprocessing-decision-guide","title":"Preprocessing Decision Guide","text":"Issue When to Use Method Missing &lt;5% Small, random gaps Drop rows Missing 5-30% Moderate gaps Impute with median/mode Missing &gt;30% Extensive gaps Drop column or advanced imputation Skewed distribution Long right tail Log transformation Outliers present Extreme values visible IQR capping or removal Wide numeric ranges Features on different scales StandardScaler or MinMaxScaler Categorical features Any non-numeric categories One-hot encoding High cardinality (&gt;50 categories) Too many unique values Group rare categories or target encoding Imbalanced classes One class &lt;10% of data SMOTE or class weights"},{"location":"03-data/data-processing/#preprocessing-sequence-checklist","title":"Preprocessing Sequence Checklist","text":"<ul> <li>Load and explore data (<code>.info()</code>, <code>.describe()</code>, <code>.head()</code>)</li> <li>Check for missing values (<code>.isnull().sum()</code>)</li> <li>Remove duplicates (<code>.drop_duplicates()</code>)</li> <li>Fix data types (<code>.astype()</code>, <code>pd.to_datetime()</code>)</li> <li>Handle outliers (based on domain knowledge)</li> <li>Engineer features (ratios, date parts, interactions)</li> <li>Split into train/test sets</li> <li>Impute missing values (fit on train only)</li> <li>Encode categorical variables (fit on train only)</li> <li>Scale numeric features (fit on train only)</li> <li>Validate final data (check shapes, null counts)</li> <li>Save preprocessing objects (<code>.pkl</code> files)</li> </ul>"},{"location":"03-data/data-processing/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned to identify data quality issues through exploration, apply systematic cleaning techniques for missing values, duplicates, and outliers, encode categorical variables and scale numeric features appropriately, recognize and prevent common sources of bias, and build complete preprocessing pipelines with correct sequencing.</p> <p>Critical insights:</p> <ul> <li>Preprocessing determines model success: Clean, properly scaled data matters more than choosing the fanciest model</li> <li>Split data before fitting: Fit all transformations on training data only to prevent data leakage</li> <li>Document decisions: Your preprocessing choices need justification and reproducibility</li> <li>Iteration is normal: Expect to revisit earlier steps as you discover new issues</li> </ul> <p>Connections to previous tutorials:</p> <ul> <li>Data Foundations: This tutorial implements the cleaning and transformation phases described there</li> <li>Pandas: Your primary tool for executing every preprocessing operation</li> <li>Visualization: Essential for discovering preprocessing needs and validating results</li> </ul> <p>What's next:</p> <p>With clean, preprocessed data, you're ready to train models. The next tutorials cover model selection, training workflows, and performance evaluation.</p> <p>External resources:</p> <ul> <li>scikit-learn Preprocessing Documentation - comprehensive guide to transformation methods</li> <li>Kaggle Learn: Data Cleaning - interactive exercises with real datasets</li> <li>Feature Engineering Book - advanced feature creation techniques</li> </ul> <p>Remember: Great models start with great preprocessing. Master these techniques, and you'll build reliable systems that work in production, not just in notebooks.</p>"},{"location":"03-data/mongo-install/","title":"Setting Up MongoDB on WSL","text":"<p>Prerequisites:</p> <ul> <li>Windows 10/11 with WSL2 installed</li> <li>Ubuntu or Debian distribution in WSL</li> <li>VS Code installed (optional)</li> </ul>"},{"location":"03-data/mongo-install/#1-update-system","title":"1. Update System","text":"<p>Update package lists to ensure latest versions and avoid compatibility issues.</p> <pre><code>sudo apt-get update\nsudo apt-get upgrade -y\n</code></pre>"},{"location":"03-data/mongo-install/#2-import-mongodb-repository-key","title":"2. Import MongoDB Repository Key","text":"<p>MongoDB isn't in Ubuntu's default repositories. Import MongoDB's official GPG key to verify authenticity.</p> <pre><code>curl -fsSL https://www.mongodb.org/static/pgp/server-7.0.asc | \\\n   sudo gpg -o /usr/share/keyrings/mongodb-server-7.0.gpg --dearmor\n</code></pre>"},{"location":"03-data/mongo-install/#3-add-mongodb-repository","title":"3. Add MongoDB Repository","text":"<p>Tell Ubuntu's package manager where to find MongoDB packages.</p> <pre><code>echo \"deb [ arch=amd64,arm64 signed-by=/usr/share/keyrings/mongodb-server-7.0.gpg ] https://repo.mongodb.org/apt/ubuntu jammy/mongodb-org/7.0 multiverse\" | \\\n   sudo tee /etc/apt/sources.list.d/mongodb-org-7.0.list\n\nsudo apt-get update\n</code></pre>"},{"location":"03-data/mongo-install/#4-install-mongodb","title":"4. Install MongoDB","text":"<p>Install the complete MongoDB database system including server, shell, and tools.</p> <pre><code>sudo apt-get install -y mongodb-org\n</code></pre> <p>Verify installation:</p> <pre><code>mongod --version\nmongosh --version\n</code></pre>"},{"location":"03-data/mongo-install/#5-create-data-directory","title":"5. Create Data Directory","text":"<p>MongoDB requires a directory to store database files. Default location is <code>/data/db</code>.</p> <pre><code># Create directory\nsudo mkdir -p /data/db\n\n# Set ownership to your user\nsudo chown -R $USER:$USER /data/db\n\n# Verify permissions\nls -ld /data/db\n</code></pre> <p>Expected output: <code>drwxr-xr-x ... /data/db</code></p>"},{"location":"03-data/mongo-install/#6-start-mongodb-server","title":"6. Start MongoDB Server","text":"<p>Method 1: Using systemctl</p> <pre><code># Start MongoDB service\nsudo systemctl start mongod\n\n# Check status\nsudo systemctl status mongod\n\n# Enable auto-start on boot (optional)\nsudo systemctl enable mongod\n</code></pre> <p>Look for \"Active: active (running)\" in status output.</p> <p>Method 2: Manual start (if systemctl doesn't work)</p> <pre><code>sudo mongod --dbpath /data/db --fork --logpath /var/log/mongodb.log\n</code></pre> <p>Flags: - <code>--dbpath /data/db</code> - Data storage location - <code>--fork</code> - Runs in background - <code>--logpath</code> - Log file location</p>"},{"location":"03-data/mongo-install/#7-connect-to-mongodb-shell","title":"7. Connect to MongoDB Shell","text":"<p>Launch MongoDB shell to interact with databases.</p> <pre><code>mongosh\n</code></pre> <p>Expected output: <pre><code>Connecting to: mongodb://127.0.0.1:27017/\nUsing MongoDB: 7.0.x\ntest&gt;\n</code></pre></p> <p>The <code>test&gt;</code> prompt indicates connection success.</p>"},{"location":"03-data/mongo-install/#8-test-mongodb-operations","title":"8. Test MongoDB Operations","text":"<p>Run these commands inside mongosh:</p> <p>Check current database: <pre><code>db\n</code></pre></p> <p>Create database: <pre><code>use testdb\n</code></pre></p> <p>Insert document: <pre><code>db.users.insertOne({ \n  name: \"Alice\", \n  age: 28, \n  city: \"Tirana\"\n})\n</code></pre></p> <p>Query documents: <pre><code>db.users.find()\n</code></pre></p> <p>Insert multiple documents: <pre><code>db.users.insertMany([\n  { name: \"Bob\", age: 32, city: \"Durr\u00ebs\" },\n  { name: \"Carol\", age: 25, city: \"Vlor\u00eb\" }\n])\n</code></pre></p> <p>Count documents: <pre><code>db.users.countDocuments()\n</code></pre></p> <p>Query with filter: <pre><code>db.users.find({ age: { $gte: 30 } })\n</code></pre></p> <p>Update document: <pre><code>db.users.updateOne(\n  { name: \"Alice\" },\n  { $set: { age: 29 } }\n)\n</code></pre></p> <p>Delete document: <pre><code>db.users.deleteOne({ name: \"Bob\" })\n</code></pre></p> <p>Show collections: <pre><code>show collections\n</code></pre></p> <p>Show databases: <pre><code>show dbs\n</code></pre></p> <p>Exit shell: <pre><code>exit\n</code></pre></p>"},{"location":"03-data/mongo-install/#9-stop-mongodb-server","title":"9. Stop MongoDB Server","text":"<pre><code>sudo systemctl stop mongod\n</code></pre> <p>Or if started manually: <pre><code>sudo pkill mongod\n</code></pre></p>"},{"location":"03-data/mongo-install/#10-vs-code-integration-optional","title":"10. VS Code Integration (Optional)","text":"<p>Install extension:</p> <ol> <li>Open VS Code</li> <li>Extensions (Ctrl+Shift+X)</li> <li>Search \"MongoDB for VS Code\"</li> <li>Install official extension</li> </ol> <p>Connect:</p> <ol> <li>Click MongoDB icon in sidebar</li> <li>Add Connection</li> <li>Enter: <code>mongodb://localhost:27017</code></li> <li>Connect</li> </ol> <p>Features: Browse databases, run queries, view documents, export/import data.</p>"},{"location":"03-data/mongo-install/#verification-commands","title":"Verification Commands","text":"<pre><code># Check versions\nmongod --version\nmongosh --version\n\n# Verify service\nsudo systemctl status mongod\n\n# Test connection\nmongosh --eval \"db.version()\"\n</code></pre>"},{"location":"03-data/mongo-install/#essential-commands","title":"Essential Commands","text":"<pre><code># Start\nsudo systemctl start mongod\n\n# Connect\nmongosh\n\n# Stop\nsudo systemctl stop mongod\n\n# Status\nsudo systemctl status mongod\n</code></pre>"},{"location":"03-data/mongo/","title":"MongoDB Introduction: NoSQL Document Databases","text":"<p>This tutorial introduces MongoDB and NoSQL databases. You'll learn what NoSQL means, how it differs from SQL, when to choose document storage, and how to perform CRUD operations (Create, Read, Update, Delete) in MongoDB using practical examples.</p> <p>Estimated time: 45 minutes</p>"},{"location":"03-data/mongo/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: </p> <p>Not all data fits neatly into tables.</p> <p>Real-world data is messy. Social media posts with varying numbers of tags, product catalogs where items have different attributes, IoT sensors generating unpredictable JSON payloads, and user profiles with flexible schemas don't map cleanly to rigid SQL tables. Adding columns for every possible field creates sparse tables full of NULLs.</p> <p>NoSQL databases solve these problems. They embrace flexibility, allowing each record to have its own structure. MongoDB stores data as JSON-like documents, making it natural to work with modern web applications that already speak JSON.</p> <p>Practical benefits: MongoDB skills let you build applications with evolving schemas, handle unstructured data from APIs, scale horizontally across servers, and integrate seamlessly with JavaScript/Python ecosystems. Many startups and tech companies choose MongoDB for rapid prototyping and flexible data models.</p> <p>Professional context: NoSQL databases power high-traffic applications like e-commerce catalogs, content management systems, real-time analytics, and mobile app backends. Understanding when to use NoSQL vs SQL is a crucial architectural decision.</p> <p>Choose the right tool for the job\u2014SQL for structured transactions, NoSQL for flexible documents.</p>"},{"location":"03-data/mongo/#core-concepts","title":"Core Concepts","text":""},{"location":"03-data/mongo/#what-is-nosql","title":"What Is NoSQL?","text":"<p>NoSQL = \"Not Only SQL\" (not \"No SQL\")</p> <p>NoSQL databases provide alternative data models to traditional relational databases. They sacrifice some relational features (like complex joins and strict schemas) to gain flexibility, scalability, and performance for specific use cases.</p> <p>Key characteristics:</p> <ul> <li>Schema flexibility: Documents can have different structures</li> <li>Horizontal scaling: Add more servers instead of upgrading hardware</li> <li>Denormalization: Store related data together rather than splitting across tables</li> <li>High availability: Built-in replication and distribution</li> </ul> <p>Common misconception: NoSQL doesn't mean \"no structure\"; it means \"flexible structure.\"</p>"},{"location":"03-data/mongo/#sql-vs-nosql-key-differences","title":"SQL vs NoSQL: Key Differences","text":"Aspect SQL (Relational) NoSQL (MongoDB) Data Model Tables with rows/columns Collections with documents (JSON) Schema Fixed, predefined Dynamic, flexible Relationships Foreign keys, JOINs Embedded documents or references Scaling Vertical (bigger servers) Horizontal (more servers) Transactions Strong ACID guarantees Eventual consistency (varies) Query Language SQL Query API (JavaScript-like) Best For Financial systems, structured data Content management, catalogs, logs <p>Example comparison:</p> <p>SQL structure: <pre><code>-- Two tables with foreign key\nCustomers: id, name, email\nOrders: id, customer_id, order_date, amount\n</code></pre></p> <p>NoSQL structure: <pre><code>// Single document with embedded data\n{\n  \"_id\": \"cust123\",\n  \"name\": \"John Doe\",\n  \"email\": \"john@example.com\",\n  \"orders\": [\n    { \"order_date\": \"2025-01-15\", \"amount\": 99.99 },\n    { \"order_date\": \"2025-02-20\", \"amount\": 149.50 }\n  ]\n}\n</code></pre></p>"},{"location":"03-data/mongo/#what-is-acid","title":"What Is ACID?","text":"<p>ACID = Atomicity, Consistency, Isolation, Durability</p> <p>These properties guarantee reliable database transactions:</p> <ol> <li>Atomicity: All operations in a transaction succeed or all fail (no partial updates)</li> <li>Consistency: Database moves from one valid state to another (constraints maintained)</li> <li>Isolation: Concurrent transactions don't interfere with each other</li> <li>Durability: Committed data survives system failures</li> </ol> <p>SQL databases: Strong ACID by default (every operation is a transaction)</p> <p>NoSQL databases (including MongoDB): </p> <ul> <li>Traditionally prioritized performance over strict ACID</li> <li>MongoDB 4.0+ supports multi-document ACID transactions</li> <li>Single-document operations are always atomic</li> </ul> <p>Trade-off: Traditional NoSQL favored eventual consistency for speed and scalability. Modern versions like MongoDB now offer both options.</p>"},{"location":"03-data/mongo/#what-is-document-storage","title":"What Is Document Storage?","text":"<p>Document databases store data as documents (JSON, BSON, XML). Each document is a self-contained record with key-value pairs.</p> <p>Example document: <pre><code>{\n  \"_id\": ObjectId(\"507f1f77bcf86cd799439011\"),\n  \"name\": \"Alice Johnson\",\n  \"email\": \"alice@company.com\",\n  \"age\": 28,\n  \"skills\": [\"Python\", \"SQL\", \"Docker\"],\n  \"address\": {\n    \"city\": \"Tirana\",\n    \"country\": \"Albania\"\n  },\n  \"hire_date\": ISODate(\"2025-01-15T00:00:00Z\"),\n  \"active\": true\n}\n</code></pre></p> <p>Why documents?</p> <ul> <li>Natural fit for objects: Maps directly to JSON/Python dictionaries</li> <li>No schema migration: Add fields without ALTER TABLE</li> <li>Nested structures: Embed related data (address, arrays)</li> <li>Variable fields: Different documents can have different fields</li> </ul> <p>MongoDB uses BSON (Binary JSON) internally for efficiency while exposing JSON interface.</p>"},{"location":"03-data/mongo/#nosql-types","title":"NoSQL Types","text":"<p>NoSQL encompasses multiple database types, each optimized for different use cases:</p> <p>1. Document Databases (MongoDB, CouchDB)</p> <ul> <li>Store JSON-like documents</li> <li>Best for: Content management, catalogs, user profiles</li> </ul> <p>2. Key-Value Stores (Redis, DynamoDB)</p> <ul> <li>Simple key \u2192 value mapping</li> <li>Best for: Caching, session storage, real-time data</li> </ul> <p>3. Column-Family Stores (Cassandra, HBase)</p> <ul> <li>Store data in columns rather than rows</li> <li>Best for: Time-series data, analytics, massive scale</li> </ul> <p>4. Graph Databases (Neo4j, Amazon Neptune)</p> <ul> <li>Store nodes and relationships</li> <li>Best for: Social networks, recommendation engines, fraud detection</li> </ul> <p>This tutorial focuses on MongoDB: The most popular document database.</p>"},{"location":"03-data/mongo/#benefits-of-nosql-databases","title":"Benefits of NoSQL Databases","text":"<p>Flexibility: Add fields to documents without downtime or migrations. Perfect for evolving requirements.</p> <pre><code>// No problem adding new field to one document\n{ \"name\": \"Bob\", \"email\": \"bob@co.com\", \"department\": \"Sales\" }\n</code></pre> <p>When to choose NoSQL:</p> <ul> <li>Rapid development with changing requirements</li> <li>Hierarchical or nested data structures</li> <li>Need to scale horizontally</li> <li>Working with JSON APIs</li> <li>Real-time analytics on large datasets</li> </ul> <p>When to stick with SQL:</p> <ul> <li>Complex transactions (banking, accounting)</li> <li>Strong consistency requirements</li> <li>Data with many relationships requiring joins</li> <li>Existing SQL ecosystem and expertise</li> </ul>"},{"location":"03-data/mongo/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"03-data/mongo/#1-mongodb-basics-and-setup","title":"1. MongoDB Basics and Setup","text":"<p>Installing MongoDB:</p> <pre><code># macOS with Homebrew\nbrew tap mongodb/brew\nbrew install mongodb-community\n\n# Ubuntu/Debian\nsudo apt-get install mongodb\n\n# Windows: Download installer from mongodb.com\n</code></pre> <p>Starting MongoDB:</p> <pre><code># Start MongoDB service\nmongod\n\n# Connect with MongoDB Shell\nmongosh\n</code></pre> <p>Cloud alternative: Use MongoDB Atlas (free tier available) at mongodb.com/atlas</p> <p>Basic MongoDB terminology:</p> SQL Term MongoDB Equivalent Database Database Table Collection Row Document Column Field Index Index JOIN Embedding or $lookup"},{"location":"03-data/mongo/#2-creating-databases-and-collections","title":"2. Creating Databases and Collections","text":"<p>In MongoDB, databases and collections are created automatically when you insert data.</p> <p>Switch to (or create) database:</p> <pre><code>// Switch to database (creates if doesn't exist)\nuse company_data\n\n// Check current database\ndb\n\n// Show all databases\nshow dbs\n// Note: Empty databases don't appear until data is added\n</code></pre> <p>Collections are created implicitly:</p> <pre><code>// No need to explicitly create collection\n// It's created automatically on first insert\ndb.employees.insertOne({\n  name: \"John Doe\",\n  email: \"john@company.com\"\n})\n\n// Show all collections in current database\nshow collections\n\n// Explicitly create collection (optional)\ndb.createCollection(\"customers\")\n</code></pre> <p>Dropping database/collection:</p> <pre><code>// Drop current database\ndb.dropDatabase()\n\n// Drop collection\ndb.employees.drop()\n</code></pre>"},{"location":"03-data/mongo/#3-inserting-documents","title":"3. Inserting Documents","text":"<p>Insert single document:</p> <pre><code>// insertOne() adds a single document\ndb.employees.insertOne({\n  first_name: \"Alice\",\n  last_name: \"Johnson\",\n  email: \"alice.j@company.com\",\n  age: 28,\n  department: \"Engineering\",\n  skills: [\"Python\", \"MongoDB\", \"Docker\"],\n  hire_date: new Date(\"2025-01-15\"),\n  salary: 75000,\n  active: true\n})\n\n// MongoDB automatically generates _id if not provided\n// Returns: { acknowledged: true, insertedId: ObjectId(\"...\") }\n</code></pre> <p>Insert multiple documents:</p> <pre><code>// insertMany() adds array of documents\ndb.employees.insertMany([\n  {\n    first_name: \"Bob\",\n    last_name: \"Smith\",\n    email: \"bob.smith@company.com\",\n    age: 35,\n    department: \"Sales\",\n    skills: [\"Negotiation\", \"CRM\"],\n    hire_date: new Date(\"2024-06-10\"),\n    salary: 68000,\n    active: true\n  },\n  {\n    first_name: \"Carol\",\n    last_name: \"Martinez\",\n    email: \"carol.m@company.com\",\n    age: 42,\n    department: \"Engineering\",\n    skills: [\"JavaScript\", \"React\", \"Node.js\"],\n    hire_date: new Date(\"2023-03-22\"),\n    salary: 92000,\n    active: true\n  },\n  {\n    first_name: \"David\",\n    last_name: \"Chen\",\n    email: \"david.chen@company.com\",\n    age: 29,\n    department: \"Marketing\",\n    skills: [\"SEO\", \"Content\", \"Analytics\"],\n    hire_date: new Date(\"2024-11-05\"),\n    salary: 62000,\n    active: true\n  }\n])\n\n// Returns: { acknowledged: true, insertedIds: { '0': ObjectId(\"...\"), '1': ObjectId(\"...\"), ... } }\n</code></pre> <p>Nested documents:</p> <pre><code>// Documents can contain nested objects and arrays\ndb.employees.insertOne({\n  first_name: \"Emma\",\n  last_name: \"Wilson\",\n  email: \"emma.w@company.com\",\n  age: 31,\n  department: \"HR\",\n  address: {\n    street: \"123 Main St\",\n    city: \"Tirana\",\n    country: \"Albania\",\n    postal_code: \"1001\"\n  },\n  projects: [\n    { name: \"Recruitment Portal\", role: \"Lead\", start_date: new Date(\"2024-01-01\") },\n    { name: \"Training Program\", role: \"Coordinator\", start_date: new Date(\"2024-06-15\") }\n  ],\n  salary: 70000\n})\n</code></pre>"},{"location":"03-data/mongo/#4-querying-documents","title":"4. Querying Documents","text":"<p>MongoDB uses a rich query API instead of SQL.</p> <p>Find all documents:</p> <pre><code>// Find all (like SELECT *)\ndb.employees.find()\n\n// Pretty print\ndb.employees.find().pretty()\n\n// Count documents\ndb.employees.countDocuments()\n</code></pre> <p>Find with criteria (WHERE equivalent):</p> <pre><code>// Single condition\ndb.employees.find({ department: \"Engineering\" })\n\n// Multiple conditions (AND)\ndb.employees.find({ \n  department: \"Engineering\", \n  salary: { $gte: 80000 } \n})\n\n// OR condition\ndb.employees.find({\n  $or: [\n    { department: \"Engineering\" },\n    { department: \"Sales\" }\n  ]\n})\n\n// IN operator\ndb.employees.find({\n  department: { $in: [\"Engineering\", \"Sales\", \"Marketing\"] }\n})\n</code></pre> <p>Comparison operators:</p> Operator Meaning Example <code>$eq</code> Equals <code>{ age: { $eq: 30 } }</code> <code>$ne</code> Not equals <code>{ active: { $ne: false } }</code> <code>$gt</code> Greater than <code>{ salary: { $gt: 70000 } }</code> <code>$gte</code> Greater or equal <code>{ age: { $gte: 30 } }</code> <code>$lt</code> Less than <code>{ age: { $lt: 40 } }</code> <code>$lte</code> Less or equal <code>{ salary: { $lte: 80000 } }</code> <code>$in</code> In array <code>{ dept: { $in: [\"IT\", \"HR\"] } }</code> <code>$nin</code> Not in array <code>{ status: { $nin: [\"inactive\"] } }</code> <p>Projection (select specific fields):</p> <pre><code>// Show only first_name and email (1 = include, 0 = exclude)\ndb.employees.find(\n  { department: \"Engineering\" },\n  { first_name: 1, email: 1, _id: 0 }\n)\n\n// Exclude specific fields\ndb.employees.find(\n  {},\n  { salary: 0, _id: 0 }\n)\n</code></pre> <p>Sorting:</p> <pre><code>// Sort by salary ascending (1 = ascending, -1 = descending)\ndb.employees.find().sort({ salary: 1 })\n\n// Sort by department ascending, then salary descending\ndb.employees.find().sort({ department: 1, salary: -1 })\n</code></pre> <p>Limiting and skipping:</p> <pre><code>// Get first 5 documents\ndb.employees.find().limit(5)\n\n// Skip first 10, then get 5 (pagination)\ndb.employees.find().skip(10).limit(5)\n\n// Combine: sort, skip, limit\ndb.employees.find()\n  .sort({ hire_date: -1 })\n  .skip(10)\n  .limit(5)\n</code></pre> <p>Pattern matching (LIKE equivalent):</p> <pre><code>// Regex for pattern matching\n// Find emails ending with @company.com\ndb.employees.find({\n  email: { $regex: \"@company.com$\" }\n})\n\n// Case-insensitive search\ndb.employees.find({\n  first_name: { $regex: \"^a\", $options: \"i\" }  // Starts with 'a' or 'A'\n})\n</code></pre> <p>Querying nested fields:</p> <pre><code>// Dot notation for nested fields\ndb.employees.find({\n  \"address.city\": \"Tirana\"\n})\n\n// Query array elements\ndb.employees.find({\n  skills: \"Python\"  // Finds if Python is in skills array\n})\n\n// Array contains all\ndb.employees.find({\n  skills: { $all: [\"Python\", \"MongoDB\"] }\n})\n</code></pre> <p>Find one document:</p> <pre><code>// Returns single document (or null)\ndb.employees.findOne({ email: \"alice.j@company.com\" })\n\n// Useful for getting by ID\ndb.employees.findOne({ _id: ObjectId(\"507f1f77bcf86cd799439011\") })\n</code></pre>"},{"location":"03-data/mongo/#5-updating-documents","title":"5. Updating Documents","text":"<p>Update single document:</p> <pre><code>// updateOne() modifies first matching document\ndb.employees.updateOne(\n  { email: \"alice.j@company.com\" },  // Filter\n  { $set: { salary: 82000 } }        // Update\n)\n\n// Returns: { acknowledged: true, matchedCount: 1, modifiedCount: 1 }\n</code></pre> <p>Update operators:</p> Operator Purpose Example <code>$set</code> Set field value <code>{ $set: { age: 30 } }</code> <code>$unset</code> Remove field <code>{ $unset: { temp_field: \"\" } }</code> <code>$inc</code> Increment number <code>{ $inc: { salary: 5000 } }</code> <code>$mul</code> Multiply <code>{ $mul: { quantity: 2 } }</code> <code>$rename</code> Rename field <code>{ $rename: { \"name\": \"full_name\" } }</code> <code>$push</code> Add to array <code>{ $push: { skills: \"Docker\" } }</code> <code>$pull</code> Remove from array <code>{ $pull: { skills: \"Java\" } }</code> <code>$addToSet</code> Add if not exists <code>{ $addToSet: { tags: \"new\" } }</code> <p>Update multiple documents:</p> <pre><code>// updateMany() modifies all matching documents\ndb.employees.updateMany(\n  { department: \"Engineering\" },\n  { $inc: { salary: 5000 } }  // Give 5k raise to all engineers\n)\n\n// Update all documents\ndb.employees.updateMany(\n  {},\n  { $set: { reviewed: false } }\n)\n</code></pre> <p>Update with multiple operators:</p> <pre><code>db.employees.updateOne(\n  { email: \"bob.smith@company.com\" },\n  {\n    $set: { department: \"Sales Management\" },\n    $inc: { salary: 10000 },\n    $push: { skills: \"Leadership\" }\n  }\n)\n</code></pre> <p>Upsert (update or insert):</p> <pre><code>// If document doesn't exist, create it\ndb.employees.updateOne(\n  { email: \"new.person@company.com\" },\n  { \n    $set: { \n      first_name: \"New\",\n      last_name: \"Person\",\n      department: \"IT\",\n      salary: 60000\n    }\n  },\n  { upsert: true }  // Creates if not found\n)\n</code></pre> <p>Replace entire document:</p> <pre><code>// replaceOne() replaces entire document (except _id)\ndb.employees.replaceOne(\n  { email: \"old@company.com\" },\n  {\n    first_name: \"Updated\",\n    last_name: \"Employee\",\n    email: \"new@company.com\",\n    department: \"Finance\",\n    salary: 75000\n  }\n)\n// Warning: This removes all fields not in replacement document\n</code></pre>"},{"location":"03-data/mongo/#6-deleting-documents","title":"6. Deleting Documents","text":"<p>Delete single document:</p> <pre><code>// deleteOne() removes first matching document\ndb.employees.deleteOne({\n  email: \"person@company.com\"\n})\n\n// Returns: { acknowledged: true, deletedCount: 1 }\n</code></pre> <p>Delete multiple documents:</p> <pre><code>// deleteMany() removes all matching documents\ndb.employees.deleteMany({\n  active: false\n})\n\n// Delete all documents in collection\ndb.employees.deleteMany({})  // Dangerous!\n</code></pre> <p>Delete with conditions:</p> <pre><code>// Delete employees hired before 2023\ndb.employees.deleteMany({\n  hire_date: { $lt: new Date(\"2023-01-01\") }\n})\n\n// Delete by multiple criteria\ndb.employees.deleteMany({\n  department: \"Temp\",\n  salary: { $lt: 50000 }\n})\n</code></pre>"},{"location":"03-data/mongo/#7-aggregation-pipeline","title":"7. Aggregation Pipeline","text":"<p>Aggregation performs complex data processing (like SQL GROUP BY, JOINs).</p> <p>Basic aggregation:</p> <pre><code>// Calculate average salary by department\ndb.employees.aggregate([\n  {\n    $group: {\n      _id: \"$department\",\n      avg_salary: { $avg: \"$salary\" },\n      count: { $sum: 1 }\n    }\n  },\n  {\n    $sort: { avg_salary: -1 }\n  }\n])\n</code></pre> <p>Aggregation stages:</p> Stage Purpose SQL Equivalent <code>$match</code> Filter documents WHERE <code>$group</code> Group by field GROUP BY <code>$sort</code> Sort results ORDER BY <code>$project</code> Select/reshape fields SELECT <code>$limit</code> Limit results LIMIT <code>$skip</code> Skip documents OFFSET <code>$lookup</code> Join collections JOIN <code>$unwind</code> Deconstruct arrays - <p>Complex aggregation example:</p> <pre><code>// Find top 3 highest paid employees per department\ndb.employees.aggregate([\n  // Stage 1: Filter active employees\n  {\n    $match: { active: true }\n  },\n  // Stage 2: Sort by department and salary\n  {\n    $sort: { department: 1, salary: -1 }\n  },\n  // Stage 3: Group by department and get top 3\n  {\n    $group: {\n      _id: \"$department\",\n      top_earners: { \n        $push: {\n          name: { $concat: [\"$first_name\", \" \", \"$last_name\"] },\n          salary: \"$salary\"\n        }\n      }\n    }\n  },\n  // Stage 4: Limit to top 3 per department\n  {\n    $project: {\n      department: \"$_id\",\n      top_earners: { $slice: [\"$top_earners\", 3] }\n    }\n  }\n])\n</code></pre> <p>Common aggregation operators:</p> <pre><code>// Count, sum, average, min, max\ndb.employees.aggregate([\n  {\n    $group: {\n      _id: null,  // Group all documents together\n      total_employees: { $sum: 1 },\n      total_payroll: { $sum: \"$salary\" },\n      avg_salary: { $avg: \"$salary\" },\n      min_salary: { $min: \"$salary\" },\n      max_salary: { $max: \"$salary\" }\n    }\n  }\n])\n</code></pre>"},{"location":"03-data/mongo/#common-mongodb-challenges","title":"Common MongoDB Challenges","text":""},{"location":"03-data/mongo/#handling-missing-fields","title":"Handling Missing Fields","text":"<p>Problem: Documents can have different fields. Querying missing fields returns nothing.</p> <pre><code>// Check if field exists\ndb.employees.find({ phone: { $exists: true } })\n\n// Check if field doesn't exist\ndb.employees.find({ phone: { $exists: false } })\n\n// Provide default with $ifNull in aggregation\ndb.employees.aggregate([\n  {\n    $project: {\n      name: \"$first_name\",\n      phone: { $ifNull: [\"$phone\", \"No phone provided\"] }\n    }\n  }\n])\n</code></pre>"},{"location":"03-data/mongo/#understanding-_id-vs-custom-ids","title":"Understanding _id vs Custom IDs","text":"<p>Problem: MongoDB auto-generates <code>_id</code> as ObjectId, but sometimes you want custom IDs.</p> <pre><code>// Auto-generated ObjectId\ndb.users.insertOne({ name: \"Alice\" })\n// _id: ObjectId(\"507f1f77bcf86cd799439011\")\n\n// Custom ID\ndb.users.insertOne({ _id: \"user_123\", name: \"Bob\" })\n// _id: \"user_123\"\n\n// Query by ObjectId (must wrap in ObjectId())\ndb.users.findOne({ _id: ObjectId(\"507f1f77bcf86cd799439011\") })\n\n// Query by custom ID\ndb.users.findOne({ _id: \"user_123\" })\n</code></pre>"},{"location":"03-data/mongo/#avoiding-accidental-updates","title":"Avoiding Accidental Updates","text":"<p>Problem: Forgetting <code>$set</code> replaces entire document.</p> <pre><code>// WRONG: This replaces entire document with just { salary: 80000 }\ndb.employees.updateOne(\n  { email: \"alice@co.com\" },\n  { salary: 80000 }  // Missing $set!\n)\n\n// CORRECT: Use $set to update specific fields\ndb.employees.updateOne(\n  { email: \"alice@co.com\" },\n  { $set: { salary: 80000 } }\n)\n</code></pre>"},{"location":"03-data/mongo/#quick-reference","title":"Quick Reference","text":""},{"location":"03-data/mongo/#essential-mongodb-commands","title":"Essential MongoDB Commands","text":"Command Purpose Example <code>use &lt;db&gt;</code> Switch/create database <code>use company</code> <code>show dbs</code> List databases <code>show dbs</code> <code>show collections</code> List collections <code>show collections</code> <code>db.collection.insertOne()</code> Insert document <code>db.users.insertOne({name: \"Alice\"})</code> <code>db.collection.find()</code> Query documents <code>db.users.find({age: {$gt: 25}})</code> <code>db.collection.updateOne()</code> Update document <code>db.users.updateOne({_id: 1}, {$set: {age: 30}})</code> <code>db.collection.deleteOne()</code> Delete document <code>db.users.deleteOne({_id: 1})</code> <code>db.collection.countDocuments()</code> Count documents <code>db.users.countDocuments()</code> <code>db.collection.drop()</code> Delete collection <code>db.users.drop()</code>"},{"location":"03-data/mongo/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned what NoSQL means and when to use it, how MongoDB differs from SQL databases, what ACID properties ensure, how document storage works with JSON-like structures, the four types of NoSQL databases, how to perform CRUD operations in MongoDB, and how to use aggregation for complex queries.</p> <p>Critical insights:</p> <ul> <li>NoSQL isn't anti-SQL: It's a complementary tool for different use cases</li> <li>Flexibility has trade-offs: Gain schema freedom, lose strict consistency guarantees</li> <li>Denormalization is normal: Embed related data instead of always splitting into references</li> <li>Indexes matter: Even flexible databases need optimization</li> </ul> <p>When to use MongoDB:</p> <ul> <li>Rapid prototyping with evolving requirements</li> <li>Hierarchical/nested data (product catalogs, user profiles)</li> <li>Content management systems</li> <li>Real-time analytics and logging</li> <li>Applications already using JSON</li> </ul> <p>When to use SQL:</p> <ul> <li>Financial transactions requiring strong consistency</li> <li>Complex multi-table relationships and joins</li> <li>Fixed schemas with strict validation</li> <li>Existing SQL infrastructure and expertise</li> </ul> <p>What's next:</p> <p>With MongoDB fundamentals mastered, explore replica sets (high availability), sharding (horizontal scaling), change streams (real-time data), and integrating MongoDB with Python using PyMongo or with Node.js using the native driver.</p> <p>Practice resources:</p> <ul> <li>MongoDB University - free official courses with certifications</li> <li>MongoDB Documentation - comprehensive guides</li> <li>M001: MongoDB Basics - hands-on course</li> </ul> <p>External resources:</p> <ul> <li>MongoDB Compass - GUI for MongoDB</li> <li>MongoDB Atlas - free cloud database hosting</li> <li>PyMongo Documentation - Python integration</li> </ul> <p>Remember: SQL and NoSQL aren't competitors; they're tools for different jobs. Choose based on your data structure, consistency needs, and scalability requirements. Master both to become a versatile data professional.</p>"},{"location":"03-data/pandas/","title":"Pandas Essentials: Data Manipulation in Python","text":"<p>This tutorial teaches you to work with tabular data using pandas, Python's most powerful data analysis library. You'll learn the fundamental operations that transform messy CSV files into clean, model-ready datasets through practical examples and clear explanations.</p> <p>Estimated time: 60 minutes</p>"},{"location":"03-data/pandas/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: Real-world data arrives in spreadsheets, databases, and CSV files\u2014tables with hundreds of columns, thousands of rows, missing values, mixed types, and unclear patterns. Without pandas, loading and manipulating this data requires hundreds of lines of code, nested loops, and manual error checking. Simple tasks like \"show me customers over age 25 from Tirana\" become programming challenges.</p> <p>Practical benefits: Pandas reduces complex data operations to single readable commands. What takes 50 lines in pure Python becomes one line in pandas. It handles the messy reality of real data\u2014missing values, mixed types, date parsing, string operations\u2014with built-in methods. Pandas makes data exploration fast enough to be interactive, cleaning operations repeatable, and feature engineering productive. You spend time discovering insights instead of debugging loops.</p> <p>Professional context: Every data scientist uses pandas daily\u2014it's as fundamental as knowing Git or SQL. Before neural networks or gradient boosting, data must be cleaned and shaped correctly. Pandas is the universal tool for this work. Job interviews test pandas proficiency because it's essential infrastructure. Open any data science notebook on Kaggle or GitHub, and you'll see pandas in the first cell. </p> <p>PLACEHOLDER</p>"},{"location":"03-data/pandas/#prerequisites-learning-objectives","title":"Prerequisites &amp; Learning Objectives","text":"<p>Required knowledge: - Python basics (lists, dictionaries, loops, functions) - Basic statistics (mean, median, sum, count) - Understanding of tables and spreadsheets (rows, columns, cells)</p> <p>Required tools: - Python 3.x installed - pandas library (<code>pip install pandas</code>) - Jupyter Notebook recommended for interactive exploration</p> <p>Learning outcomes: - Understand DataFrames and Series as pandas data structures - Load data from CSV files and inspect basic properties - Select specific columns and filter rows based on conditions - Handle missing values through removal or imputation - Create new columns from existing data (feature engineering) - Group data by categories and compute aggregate statistics - Connect each operation to machine learning pipeline phases</p> <p>High-level approach: Learn pandas through progressive examples, starting with loading data and building to complex grouping operations, always connecting concepts to practical ML workflows.</p>"},{"location":"03-data/pandas/#core-concepts","title":"Core Concepts","text":""},{"location":"03-data/pandas/#what-is-pandas","title":"What is Pandas?","text":"<p>Pandas is a Python library that provides high-performance data structures and analysis tools for working with structured (tabular) data. Think of it as \"Excel but programmable, faster, and more powerful.\"</p> <p>Two core data structures:</p> <p>DataFrame: A 2D labeled table with rows and columns (like a spreadsheet or SQL table). Each column can have a different data type.</p> <p>Series: A single column (or row) with an index. Essentially a 1D array with labels.</p> <p>Why \"pandas\"? The name comes from \"panel data,\" an econometrics term for multidimensional datasets, but it's commonly used for all tabular data.</p>"},{"location":"03-data/pandas/#why-pandas-for-machine-learning","title":"Why Pandas for Machine Learning?","text":"<p>Speed: Built on top of NumPy (C-optimized arrays), pandas inherits NumPy's performance while adding labels and intuitive syntax.</p> <p>Real-world data handling: Deals with missing values, mixed types, string operations, date parsing, which are all common in production datasets.</p> <p>Readable syntax: Operations read like English questions: \"give me rows where age is greater than 25.\"</p> <p>Integration: Works well with <code>scikit-learn</code>, <code>matplotlib</code>, and other ML libraries.</p> <p>Key insight: While NumPy provides fast numerical arrays, pandas adds structure and labels that make data manipulation intuitive. You think in terms of \"customers\" and \"purchases,\" not \"row 47, column 3.\"</p> <p>PLACEHOLDER</p>"},{"location":"03-data/pandas/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"03-data/pandas/#understanding-dataframes-and-series","title":"Understanding DataFrames and Series","text":"<p>Before loading real data, understand pandas' two fundamental structures.</p> <p>DataFrame basics:</p> <pre><code>import pandas as pd\n\n# Create a DataFrame from a dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n    'age': [25, 30, 35, 28],\n    'city': ['Tirana', 'Durr\u00ebs', 'Tirana', 'Vlor\u00eb'],\n    'salary': [45000, 52000, 61000, 48000]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n</code></pre> <p>Expected output: <pre><code>      name  age    city  salary\n0    Alice   25  Tirana   45000\n1      Bob   30  Durr\u00ebs   52000\n2  Charlie   35  Tirana   61000\n3    Diana   28   Vlor\u00eb   48000\n</code></pre></p> <p>Key observations: - Left column (0, 1, 2, 3) is the index (row labels) - Top row contains column names - Each column can have different types (strings, integers) - Data organized in rows (samples) and columns (features)</p> <p>Basic properties:</p> <pre><code># Shape: (rows, columns)\nprint(f\"Shape: {df.shape}\")  # (4, 4)\n\n# Column names\nprint(f\"Columns: {df.columns.tolist()}\")\n\n# Data types\nprint(df.dtypes)\n</code></pre> <p>Series: Single column</p> <pre><code># Extract one column (returns a Series)\nages = df['age']\nprint(type(ages))  # &lt;class 'pandas.core.series.Series'&gt;\nprint(ages)\n</code></pre> <p>ML connection: In machine learning, DataFrames represent datasets where rows are samples (observations, data points) and columns are features (variables, attributes). A customer dataset might have 10,000 rows (customers) and 20 columns (age, location, purchase history, etc.).</p>"},{"location":"03-data/pandas/#loading-data-from-files","title":"Loading Data from Files","text":"<p>Real data comes from files, not dictionaries. Learn to import common formats.</p> <p>Loading CSV (most common):</p> <pre><code>import pandas as pd\n\n# Load CSV file\ndf = pd.read_csv('data.csv')\n\n# Common parameters\ndf = pd.read_csv('data.csv',\n                 sep=',',           # delimiter (default is comma)\n                 header=0,          # which row contains column names\n                 index_col=None,    # which column to use as index\n                 encoding='utf-8')  # character encoding\n</code></pre> <p>First look at data:</p> <pre><code># First 5 rows\nprint(df.head())\n\n# Last 5 rows\nprint(df.tail())\n\n# First N rows\nprint(df.head(10))\n</code></pre> <p>Dataset overview:</p> <pre><code># Concise summary: columns, types, non-null counts\nprint(df.info())\n\n# Statistical summary of numerical columns\nprint(df.describe())\n\n# Number of rows and columns\nprint(f\"Shape: {df.shape}\")\nprint(f\"{df.shape[0]} rows, {df.shape[1]} columns\")\n</code></pre> <p>Example output from <code>.info()</code>: <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   total_bill  244 non-null    float64\n 1   tip         244 non-null    float64\n 2   sex         244 non-null    object \n 3   smoker      244 non-null    object \n 4   day         244 non-null    object \n 5   time        244 non-null    object \n 6   size        244 non-null    int64  \ndtypes: float64(2), int64(1), object(4)\n</code></pre></p> <p>Reading other formats:</p> <pre><code># Excel file\ndf = pd.read_excel('data.xlsx', sheet_name='Sheet1')\n\n# From URL\nurl = 'https://example.com/data.csv'\ndf = pd.read_csv(url)\n</code></pre> <p>ML connection: This is the Data Collection phase of the ML pipeline. Always start by loading data and inspecting it with <code>.head()</code>, <code>.info()</code>, and <code>.describe()</code> before any analysis or modeling.</p>"},{"location":"03-data/pandas/#selecting-columns","title":"Selecting Columns","text":"<p>Extract specific columns to focus your analysis or prepare features for modeling.</p> <p>Single column (returns Series):</p> <pre><code># Select 'age' column\nages = df['age']\nprint(type(ages))  # pandas.core.series.Series\n\n# Also works with dot notation (if column name has no spaces)\nages = df.age\n</code></pre> <p>Multiple columns (returns DataFrame):</p> <pre><code># Select multiple columns - pass a list\nsubset = df[['name', 'age', 'city']]\nprint(subset.head())\n\n# Order matters - columns appear in the order you specify\nreordered = df[['city', 'name', 'age']]\n</code></pre> <p>Column names:</p> <pre><code># List all column names\nprint(df.columns)\n\n# Convert to regular Python list\nprint(df.columns.tolist())\n\n# Number of columns\nprint(len(df.columns))\n</code></pre> <p>Dropping columns:</p> <pre><code># Drop one column\ndf_smaller = df.drop('salary', axis=1)\n\n# Drop multiple columns\ndf_smaller = df.drop(['age', 'salary'], axis=1)\n\n# Drop columns in place (modifies original DataFrame)\ndf.drop('salary', axis=1, inplace=True)\n</code></pre> <p>Why select columns?</p> <ul> <li>Focus on relevant features</li> <li>Reduce memory usage</li> <li>Prepare specific features for modeling</li> <li>Create different views of the same data</li> </ul> <p>ML connection: Feature selection is critical in machine learning. Not all columns are useful for predictions. Selecting relevant features improves model performance and reduces training time.</p>"},{"location":"03-data/pandas/#filtering-rows-selection-by-condition","title":"Filtering Rows (Selection by Condition)","text":"<p>Select rows that meet specific criteria\u2014one of pandas' most powerful features.</p> <p>Single condition:</p> <pre><code># People older than 30\nolder = df[df['age'] &gt; 30]\n\n# People from Tirana\nfrom_tirana = df[df['city'] == 'Tirana']\n\n# Salaries greater than or equal to 50000\nhigh_earners = df[df['salary'] &gt;= 50000]\n</code></pre> <p>Multiple conditions with AND (<code>&amp;</code>):</p> <pre><code># People from Tirana AND age &gt; 25\nresult = df[(df['city'] == 'Tirana') &amp; (df['age'] &gt; 25)]\n\n# IMPORTANT: Parentheses around each condition are required!\n</code></pre> <p>Multiple conditions with OR (<code>|</code>):</p> <pre><code># People from Tirana OR Durr\u00ebs\nresult = df[(df['city'] == 'Tirana') | (df['city'] == 'Durr\u00ebs')]\n\n# Age less than 25 OR greater than 35\nresult = df[(df['age'] &lt; 25) | (df['age'] &gt; 35)]\n</code></pre> <p>Using <code>.isin()</code> for multiple values:</p> <pre><code># People from Tirana, Durr\u00ebs, or Vlor\u00eb\ncities = ['Tirana', 'Durr\u00ebs', 'Vlor\u00eb']\nresult = df[df['city'].isin(cities)]\n</code></pre> <p>Using <code>.query()</code> method (more readable):</p> <pre><code># Same as above but cleaner syntax\nresult = df.query('age &gt; 30')\nresult = df.query('city == \"Tirana\" and age &gt; 25')\nresult = df.query('city in [\"Tirana\", \"Durr\u00ebs\"]')\n</code></pre> <p>Negation (NOT):</p> <pre><code># People NOT from Tirana\nresult = df[df['city'] != 'Tirana']\n\n# Using tilde (~) for negation\nresult = df[~(df['city'] == 'Tirana')]\n</code></pre> <p>Counting filtered results:</p> <pre><code># How many people are over 30?\ncount = len(df[df['age'] &gt; 30])\nprint(f\"{count} people are over 30\")\n</code></pre> <p>ML connection: Filtering is essential for:</p> <ul> <li>Removing outliers before training</li> <li>Creating training/test splits by condition</li> <li>Analyzing subgroups (stratified analysis)</li> <li>Generating class-specific statistics</li> </ul>"},{"location":"03-data/pandas/#handling-missing-data","title":"Handling Missing Data","text":"<p>Real-world data contains missing values. Pandas provides tools to detect and handle them.</p> <p>Detecting missing values:</p> <pre><code># Check for any missing values\nprint(df.isnull())  # Returns Boolean DataFrame\n\n# Count missing values per column\nprint(df.isnull().sum())\n\n# Count total missing values\nprint(df.isnull().sum().sum())\n\n# Which rows have ANY missing value?\nrows_with_missing = df[df.isnull().any(axis=1)]\n</code></pre> <p>Example output: <pre><code>name      0\nage       2\ncity      1\nsalary    0\ndtype: int64\n</code></pre></p> <p>Strategy 1: Drop missing values</p> <pre><code># Drop rows with ANY missing value\ndf_clean = df.dropna()\n\n# Drop rows where specific columns have missing values\ndf_clean = df.dropna(subset=['age', 'salary'])\n\n# Drop columns with ANY missing value\ndf_clean = df.dropna(axis=1)\n\n# Drop only if ALL values in row are missing\ndf_clean = df.dropna(how='all')\n</code></pre> <p>Strategy 2: Fill missing values</p> <pre><code># Fill with a specific value\ndf['age'].fillna(0, inplace=True)\n\n# Fill with mean\ndf['age'].fillna(df['age'].mean(), inplace=True)\n\n# Fill with median (better for skewed distributions)\ndf['salary'].fillna(df['salary'].median(), inplace=True)\n\n# Fill with mode (most common value)\ndf['city'].fillna(df['city'].mode()[0], inplace=True)\n\n# Forward fill (use previous valid value)\ndf['age'].fillna(method='ffill', inplace=True)\n\n# Backward fill (use next valid value)\ndf['age'].fillna(method='bfill', inplace=True)\n</code></pre> <p>Fill all columns at once:</p> <pre><code># Fill all numeric columns with their means\nnumeric_cols = df.select_dtypes(include=['number']).columns\nfor col in numeric_cols:\n    df[col].fillna(df[col].mean(), inplace=True)\n</code></pre> <p>ML connection: Machine learning models cannot process missing values. During the </p> <p>Data Cleaning phase, you must either:</p> <ul> <li>Remove rows/columns with missing data (acceptable if &lt; 5% of data)</li> <li>Impute (fill) missing values with statistical measures</li> <li>Use advanced imputation methods (KNN, iterative)</li> </ul> <p>The choice affects model performance\u2014test different strategies.</p>"},{"location":"03-data/pandas/#creating-new-columns","title":"Creating New Columns","text":"<p>Feature engineering creates new columns from existing data to improve model predictions.</p> <p>Simple calculations:</p> <pre><code># Create age in months\ndf['age_months'] = df['age'] * 12\n\n# Calculate salary per year of age\ndf['salary_per_age'] = df['salary'] / df['age']\n\n# Boolean column\ndf['is_tirana'] = df['city'] == 'Tirana'\n</code></pre> <p>Using functions with <code>.apply()</code>:</p> <pre><code># Define a function\ndef categorize_age(age):\n    if age &lt; 25:\n        return 'Young'\n    elif age &lt; 35:\n        return 'Middle'\n    else:\n        return 'Senior'\n\n# Apply to create new column\ndf['age_category'] = df['age'].apply(categorize_age)\n\n# Using lambda (inline function)\ndf['age_category'] = df['age'].apply(\n    lambda x: 'Young' if x &lt; 25 else 'Middle' if x &lt; 35 else 'Senior'\n)\n</code></pre> <p>Combining multiple columns:</p> <pre><code># Create full description\ndf['description'] = df['name'] + ' from ' + df['city']\n\n# Conditional based on multiple columns\ndf['high_earner_from_capital'] = (\n    (df['salary'] &gt; 50000) &amp; (df['city'] == 'Tirana')\n)\n</code></pre> <p>Using <code>.map()</code> for replacements:</p> <pre><code># Map cities to regions\ncity_to_region = {\n    'Tirana': 'Central',\n    'Durr\u00ebs': 'Coastal',\n    'Vlor\u00eb': 'Coastal',\n    'Shkod\u00ebr': 'North'\n}\ndf['region'] = df['city'].map(city_to_region)\n</code></pre> <p>Renaming columns:</p> <pre><code># Rename specific columns\ndf.rename(columns={'salary': 'annual_salary', 'age': 'years_old'}, inplace=True)\n\n# Rename all columns (useful for cleaning names)\ndf.columns = [col.lower().replace(' ', '_') for col in df.columns]\n</code></pre> <p>ML connection: Feature engineering is often the most impactful way to improve model performance. Good features can make a simple model outperform a complex one with raw features. Common transformations:</p> <ul> <li>Ratios (revenue per customer)</li> <li>Categories from continuous values (age groups)</li> <li>Combinations (interactions between features)</li> <li>Time-based features (day of week, month, season)</li> </ul>"},{"location":"03-data/pandas/#sorting-and-ranking","title":"Sorting and Ranking","text":"<p>Order data to find patterns, extremes, or prepare for specific analyses.</p> <p>Sort by single column:</p> <pre><code># Sort by age (ascending - default)\ndf_sorted = df.sort_values('age')\n\n# Sort by age (descending)\ndf_sorted = df.sort_values('age', ascending=False)\n\n# Sort in place\ndf.sort_values('age', inplace=True)\n</code></pre> <p>Sort by multiple columns:</p> <pre><code># Sort by city, then by age within each city\ndf_sorted = df.sort_values(['city', 'age'])\n\n# Different order for each column\ndf_sorted = df.sort_values(['city', 'age'], \n                           ascending=[True, False])\n</code></pre> <p>Reset index after sorting:</p> <pre><code># After sorting, index might be out of order\ndf_sorted = df.sort_values('age').reset_index(drop=True)\n</code></pre> <p>Finding top/bottom values:</p> <pre><code># Top 5 earners\ntop_5 = df.nlargest(5, 'salary')\n\n# Bottom 5 by age\nyoungest_5 = df.nsmallest(5, 'age')\n</code></pre> <p>Ranking:</p> <pre><code># Rank salaries (1 = lowest)\ndf['salary_rank'] = df['salary'].rank()\n\n# Rank in descending order (1 = highest)\ndf['salary_rank'] = df['salary'].rank(ascending=False)\n\n# Handle ties with average ranks\ndf['salary_rank'] = df['salary'].rank(method='average')\n</code></pre> <p>ML connection: Sorting is useful for:</p> <ul> <li>Finding outliers (highest/lowest values)</li> <li>Time-series data (ensuring chronological order)</li> <li>Ranking customers by value</li> <li>Identifying top performers for analysis</li> </ul>"},{"location":"03-data/pandas/#grouping-and-aggregating","title":"Grouping and Aggregating","text":"<p>Group data by categories and compute summary statistics\u2014one of pandas' most powerful features.</p> <p>Basic grouping:</p> <pre><code># Average salary by city\navg_salary = df.groupby('city')['salary'].mean()\nprint(avg_salary)\n\n# Count people per city\ncounts = df.groupby('city').size()\n# or\ncounts = df.groupby('city')['name'].count()\n</code></pre> <p>Expected output: <pre><code>city\nDurr\u00ebs    52000.0\nTirana    53000.0\nVlor\u00eb     48000.0\nName: salary, dtype: float64\n</code></pre></p> <p>Multiple aggregations:</p> <pre><code># Multiple statistics for salary\nsummary = df.groupby('city')['salary'].agg(['mean', 'median', 'min', 'max', 'std'])\nprint(summary)\n\n# Different aggregations for different columns\nsummary = df.groupby('city').agg({\n    'salary': ['mean', 'sum'],\n    'age': ['mean', 'min', 'max'],\n    'name': 'count'  # count of names = count of people\n})\n</code></pre> <p>Grouping by multiple columns:</p> <pre><code># Average salary by city AND age category\navg_by_city_age = df.groupby(['city', 'age_category'])['salary'].mean()\nprint(avg_by_city_age)\n\n# Convert to DataFrame with unstack()\nresult = df.groupby(['city', 'age_category'])['salary'].mean().unstack()\n</code></pre> <p>Filtering groups:</p> <pre><code># Only cities with more than 2 people\nlarge_cities = df.groupby('city').filter(lambda x: len(x) &gt; 2)\n\n# Cities where average salary exceeds 50000\nhigh_salary_cities = df.groupby('city').filter(\n    lambda x: x['salary'].mean() &gt; 50000\n)\n</code></pre> <p>Applying custom functions:</p> <pre><code>def salary_range(group):\n    return group['salary'].max() - group['salary'].min()\n\n# Apply custom function to each group\nranges = df.groupby('city').apply(salary_range)\n</code></pre> <p>ML connection: Grouping creates aggregate features that are highly predictive:</p> <ul> <li>Customer lifetime value (sum of purchases per customer)</li> <li>Average session duration (mean time per user)</li> <li>Purchase frequency (count of orders per month)</li> <li>Conversion rate by segment (ratio of conversions to visits)</li> </ul> <p>These aggregated features often perform better than raw transaction data.</p>"},{"location":"03-data/pandas/#value-counts-and-unique-values","title":"Value Counts and Unique Values","text":"<p>Understand the distribution of categorical variables.</p> <p>Value counts:</p> <pre><code># Count occurrences of each city\ncity_counts = df['city'].value_counts()\nprint(city_counts)\n\n# With percentages\ncity_pcts = df['city'].value_counts(normalize=True) * 100\nprint(city_pcts)\n\n# Include missing values in count\ncounts_with_na = df['city'].value_counts(dropna=False)\n</code></pre> <p>Expected output: <pre><code>city\nTirana    2\nDurr\u00ebs    1\nVlor\u00eb     1\nName: count, dtype: int64\n</code></pre></p> <p>Unique values:</p> <pre><code># List all unique cities\ncities = df['city'].unique()\nprint(cities)  # array(['Tirana', 'Durr\u00ebs', 'Vlor\u00eb'], dtype=object)\n\n# Count of unique values\nn_cities = df['city'].nunique()\nprint(f\"Number of unique cities: {n_cities}\")\n</code></pre> <p>Cross-tabulation:</p> <pre><code># Count combinations of two categorical variables\ncrosstab = pd.crosstab(df['city'], df['age_category'])\nprint(crosstab)\n</code></pre> <p>ML connection: Understanding category distributions helps with:</p> <ul> <li>Class imbalance detection: If 95% of samples are one class, model will be biased</li> <li>Feature encoding decisions: Categories with few samples might need special handling</li> <li>Stratification: Ensuring train/test splits represent all categories proportionally</li> </ul>"},{"location":"03-data/pandas/#data-types-and-conversion","title":"Data Types and Conversion","text":"<p>Proper data types improve performance, enable appropriate operations, and reduce memory usage.</p> <p>Check current types:</p> <pre><code># View data types of all columns\nprint(df.dtypes)\n\n# Check type of specific column\nprint(df['age'].dtype)\n</code></pre> <p>Convert types:</p> <pre><code># Convert to integer\ndf['age'] = df['age'].astype('int')\n\n# Convert to float\ndf['salary'] = df['salary'].astype('float')\n\n# Convert to string\ndf['id'] = df['id'].astype('str')\n\n# Convert to category (saves memory for repeated values)\ndf['city'] = df['city'].astype('category')\n</code></pre> <p>Numeric conversions with error handling:</p> <pre><code># Convert to numeric, coercing errors to NaN\ndf['age'] = pd.to_numeric(df['age'], errors='coerce')\n\n# Convert to numeric, raising error if impossible\ndf['age'] = pd.to_numeric(df['age'], errors='raise')\n</code></pre> <p>Categorical optimization:</p> <pre><code># Regular string column\ndf['city'] = df['city'].astype('object')  # default\nprint(df.memory_usage())\n\n# Convert to categorical (more efficient for repeated values)\ndf['city'] = df['city'].astype('category')\nprint(df.memory_usage())  # Typically much smaller\n</code></pre> <p>Why it matters:</p> <ul> <li>Memory: Categorical types use less memory than strings</li> <li>Speed: Operations on proper types are faster</li> <li>Correctness: Some operations require specific types</li> </ul> <p>ML connection: Proper data types are essential for:</p> <ul> <li>Encoding: Categorical variables need categorical type before one-hot encoding</li> <li>Numerical operations: Age stored as string can't be used in calculations</li> <li>Memory efficiency: Large datasets require careful type management</li> </ul>"},{"location":"03-data/pandas/#exporting-data","title":"Exporting Data","text":"<p>Save processed data for later use or sharing.</p> <p>Save to CSV:</p> <pre><code># Basic export\ndf.to_csv('output.csv')\n\n# Without row index\ndf.to_csv('output.csv', index=False)\n\n# With specific encoding\ndf.to_csv('output.csv', index=False, encoding='utf-8')\n\n# Only specific columns\ndf[['name', 'age']].to_csv('subset.csv', index=False)\n</code></pre> <p>Save to Excel:</p> <pre><code># Single sheet\ndf.to_excel('output.xlsx', index=False, sheet_name='Data')\n\n# Multiple sheets\nwith pd.ExcelWriter('output.xlsx') as writer:\n    df.to_excel(writer, sheet_name='Raw Data', index=False)\n    summary.to_excel(writer, sheet_name='Summary')\n</code></pre> <p>Other formats:</p> <pre><code># Parquet (efficient for large datasets)\ndf.to_parquet('output.parquet')\n\n# JSON\ndf.to_json('output.json', orient='records')\n\n# SQL database\n# df.to_sql('table_name', connection, if_exists='replace')\n</code></pre> <p>ML connection: Saving processed data is part of the Storage phase:</p> <ul> <li>Save cleaned data separately from raw data</li> <li>Export train/test splits for reproducibility</li> <li>Store engineered features for model training</li> <li>Version datasets alongside code</li> </ul>"},{"location":"03-data/pandas/#quick-reference","title":"Quick Reference","text":"Task Code Purpose Load CSV <code>pd.read_csv('file.csv')</code> Import data First rows <code>.head()</code>, <code>.tail()</code> Preview data Info <code>.info()</code>, <code>.describe()</code> Understand structure Select column <code>df['column']</code> Extract feature Select columns <code>df[['col1', 'col2']]</code> Multiple features Filter rows <code>df[df['age'] &gt; 25]</code> Conditional selection Missing values <code>.isnull().sum()</code> Detect nulls Drop nulls <code>.dropna()</code> Remove missing Fill nulls <code>.fillna(value)</code> Impute missing New column <code>df['new'] = calculation</code> Feature engineering Apply function <code>.apply(func)</code> Transform values Sort <code>.sort_values('col')</code> Order data Group <code>.groupby('col').mean()</code> Aggregate statistics Value counts <code>.value_counts()</code> Count categories Unique values <code>.unique()</code>, <code>.nunique()</code> Distinct values Data types <code>.dtypes</code>, <code>.astype()</code> Type information/conversion Save CSV <code>.to_csv('file.csv')</code> Export data"},{"location":"03-data/pandas/#common-workflows","title":"Common Workflows","text":"<p>Workflow 1: Load \u2192 Explore \u2192 Clean</p> <pre><code># Load data\ndf = pd.read_csv('data.csv')\n\n# Explore\nprint(df.info())\nprint(df.describe())\nprint(df.isnull().sum())\n\n# Clean\ndf = df.dropna()  # Remove missing\ndf = df[df['age'] &gt; 0]  # Remove invalid values\n</code></pre> <p>Workflow 2: Filter \u2192 Group \u2192 Analyze</p> <pre><code># Filter relevant subset\nactive_users = df[df['status'] == 'active']\n\n# Group and compute statistics\nsummary = active_users.groupby('region')['revenue'].agg(['sum', 'mean', 'count'])\n\n# Sort results\nsummary = summary.sort_values('sum', ascending=False)\n</code></pre> <p>Workflow 3: Feature Engineering \u2192 Export</p> <pre><code># Create derived features\ndf['revenue_per_customer'] = df['revenue'] / df['customers']\ndf['month'] = pd.to_datetime(df['date']).dt.month\n\n# Select relevant columns\ndf_model = df[['revenue_per_customer', 'month', 'region', 'target']]\n\n# Export for modeling\ndf_model.to_csv('model_data.csv', index=False)\n</code></pre>"},{"location":"03-data/pandas/#best-practices","title":"Best Practices","text":"<p>Always explore first: Before making changes, use <code>.head()</code>, <code>.info()</code>, <code>.describe()</code>, and <code>.value_counts()</code> to understand your data.</p> <p>Use <code>.copy()</code> for safety: When creating modified DataFrames, use <code>df_new = df.copy()</code> to avoid unintended changes to the original.</p> <p>Chain methods carefully: <code>df.groupby('city').mean().sort_values('salary')</code> is readable, but break complex chains into steps for debugging.</p> <p>Check after transformations: After each operation, verify it worked\u2014check shape, null counts, and sample values.</p> <p>Keep raw data unchanged: Never modify original files. Apply all transformations in code for reproducibility.</p> <p>Document transformations: Comment your code explaining why you made each transformation decision.</p> <p>Test with small samples: Before processing millions of rows, test your workflow on the first 1000 rows to catch errors quickly.</p>"},{"location":"03-data/pandas/#ml-pipeline-integration","title":"ML Pipeline Integration","text":"<p>Pandas operations map directly to machine learning pipeline phases:</p> Pipeline Phase Pandas Operations Purpose Collection <code>pd.read_csv()</code>, <code>pd.read_excel()</code>, <code>pd.read_sql()</code> Load data from sources Exploration <code>.head()</code>, <code>.info()</code>, <code>.describe()</code>, <code>.value_counts()</code> Understand structure and distributions Cleaning <code>.dropna()</code>, <code>.fillna()</code>, filtering, deduplication Remove errors and handle missing values Transformation New columns, <code>.apply()</code>, <code>.map()</code>, <code>.astype()</code> Engineer features and prepare for modeling Splitting Boolean indexing, <code>.sample()</code> Create train/validation/test sets Storage <code>.to_csv()</code>, <code>.to_parquet()</code> Save processed data"},{"location":"03-data/pandas/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned to load data from CSV files, explore structure and distributions, select and filter subsets, handle missing values systematically, create new features through calculations and functions, group data by categories and compute aggregates, and connect every operation to machine learning workflows.</p> <p>Critical insights:</p> <ul> <li>DataFrames are the universal data structure for tabular data in Python ML ecosystem</li> <li>80% of ML work is data preparation: pandas is your primary tool for this phase</li> <li>Operations are composable: chain methods to build powerful data transformations</li> <li>Always validate: check results after each transformation to catch errors early</li> <li>Type matters: proper data types enable correct operations and reduce memory</li> </ul> <p>Real-world readiness: You can now:</p> <ul> <li>Load messy CSV files and make sense of their structure</li> <li>Clean data by detecting and handling missing values</li> <li>Filter datasets to analyze specific subgroups</li> <li>Engineer features that improve model predictions</li> <li>Group data to discover patterns and generate insights</li> <li>Prepare datasets in the exact format ML models expect</li> </ul> <p>Connections to previous topics:</p> <ul> <li>NumPy: Pandas is built on NumPy\u2014DataFrames are collections of NumPy arrays with labels</li> <li>Statistics: Use <code>.describe()</code>, <code>.groupby()</code>, and aggregations to compute statistical summaries</li> <li>Linear Algebra: Each DataFrame column is a vector; operations mirror vector/matrix operations</li> </ul> <p>External resources:</p> <ul> <li>Pandas Official Documentation - comprehensive reference with examples</li> <li>Pandas Cheat Sheet - quick command reference (2 pages)</li> <li>Kaggle Learn: Pandas - interactive micro-lessons with immediate feedback</li> <li>Python for Data Analysis (Book) - written by pandas creator Wes McKinney</li> </ul> <p>Next tutorial: Visualization with Matplotlib and Seaborn transform the insights you discovered in pandas into clear, compelling charts that communicate patterns and support decisions.</p>"},{"location":"03-data/sql/","title":"SQL Introduction: Working with Relational Databases","text":"<p>This tutorial introduces SQL (Structured Query Language) and relational databases. You'll learn to create databases and tables, query data with SELECT statements, modify records with INSERT/UPDATE/DELETE, and use functions and subqueries to solve complex data problems.</p> <p>Estimated time: 40 minutes</p>"},{"location":"03-data/sql/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: </p> <p>Data without structure becomes unusable at scale.</p> <p>Real-world data needs organization. When you have thousands of customer records, product inventories, or transaction logs, spreadsheets break down. You need a system that stores data efficiently, enforces relationships between tables, handles concurrent access, and retrieves specific records instantly.</p> <p>Databases solve these problems. They provide structure, speed, and reliability that flat files can't match.</p> <p>Practical benefits: SQL skills let you extract insights from production databases, build data pipelines that feed analytics tools, and communicate with backend engineers about data requirements. Every data role involves querying databases.</p> <p>Professional context: SQL ranks among the most requested skills in data job postings. Companies store operational data in relational databases, and analysts spend significant time writing queries to access it. Understanding SQL fundamentals is non-negotiable for data work.</p> <p>SQL is the language of data. Master it! </p>"},{"location":"03-data/sql/#core-concepts","title":"Core Concepts","text":""},{"location":"03-data/sql/#understanding-databases","title":"Understanding Databases","text":"<p>Database: An organized collection of data stored electronically. Databases manage how data is stored, accessed, updated, and protected.</p> <p>Database Management System (DBMS): Software that creates and manages databases. Examples include MySQL, PostgreSQL, Oracle, SQL Server, and SQLite.</p> <p>Why databases matter: They provide structured storage with built-in rules (no duplicate IDs, required fields, data type enforcement), fast retrieval using indexes, concurrent access allowing multiple users simultaneously, and data integrity through relationships and constraints.</p>"},{"location":"03-data/sql/#what-is-a-relational-database","title":"What Is a Relational Database?","text":"<p>Relational databases organize data into tables (also called relations) with rows and columns. Tables connect to each other through shared key values, forming relationships.</p> <p>Key characteristics:</p> <ul> <li>Tables: Data organized in rows (records) and columns (fields)</li> <li>Relationships: Tables link via foreign keys referencing primary keys</li> <li>Schema: Predefined structure defining table layouts and data types</li> <li>ACID properties: Atomicity, Consistency, Isolation, Durability guarantee reliable transactions</li> </ul> <p>Example structure:</p> <pre><code>Customers Table          Orders Table\n----------------        ----------------\ncustomer_id (PK)        order_id (PK)\nname                    customer_id (FK) \u2192 links to Customers\nemail                   order_date\n                        total_amount\n</code></pre> <p>The <code>customer_id</code> in Orders references <code>customer_id</code> in Customers, creating a relationship.</p>"},{"location":"03-data/sql/#what-does-sql-stand-for","title":"What Does SQL Stand For?","text":"<p>SQL = Structured Query Language</p> <p>SQL is the standard language for interacting with relational databases. It lets you define structure (CREATE tables), manipulate data (INSERT, UPDATE, DELETE), query data (SELECT), and control access (GRANT permissions).</p> <p>SQL is declarative: You describe what you want, not how to get it. The database figures out the optimal execution plan.</p>"},{"location":"03-data/sql/#what-is-mysql","title":"What Is MySQL?","text":"<p>MySQL is one of the most popular open-source relational database management systems. It's fast, reliable, widely supported, and free for most use cases.</p> <p>When MySQL is used: Web applications, content management systems, e-commerce platforms, data warehousing, and logging systems commonly use MySQL.</p> <p>Alternatives: PostgreSQL (more features), SQLite (lightweight, no server), SQL Server (Microsoft), Oracle (enterprise).</p> <p>For this tutorial: Examples use MySQL syntax, but core concepts apply to all SQL databases with minor syntax variations.</p>"},{"location":"03-data/sql/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"03-data/sql/#1-creating-a-database","title":"1: Creating a Database","text":"<p>Databases contain tables; tables contain data. Start by creating the container.</p> <p>Creating a database:</p> <pre><code>-- Create a new database\nCREATE DATABASE company_data;\n\n-- View all databases\nSHOW DATABASES;\n\n-- Select database to use\nUSE company_data;\n</code></pre> <p>Best practice: Use lowercase with underscores for database names. Avoid spaces and special characters.</p> <p>Checking current database:</p> <pre><code>SELECT DATABASE();\n</code></pre> <p>Dropping (deleting) a database:</p> <pre><code>-- WARNING: This permanently deletes all data\nDROP DATABASE company_data;\n</code></pre>"},{"location":"03-data/sql/#2-creating-tables","title":"2: Creating Tables","text":"<p>Tables define structure with columns, data types, and constraints.</p> <p>Basic table creation:</p> <pre><code>CREATE TABLE employees (\n    employee_id INT PRIMARY KEY AUTO_INCREMENT,\n    first_name VARCHAR(50) NOT NULL,\n    last_name VARCHAR(50) NOT NULL,\n    email VARCHAR(100) UNIQUE,\n    hire_date DATE,\n    salary DECIMAL(10, 2),\n    department_id INT\n);\n</code></pre> <p>Key components:</p> <ul> <li>Column name: <code>first_name</code>, <code>salary</code></li> <li>Data type: <code>INT</code> (integer), <code>VARCHAR(50)</code> (text up to 50 chars), <code>DATE</code>, <code>DECIMAL(10,2)</code> (numbers with 2 decimal places)</li> <li>Constraints: </li> <li><code>PRIMARY KEY</code> - unique identifier for each row</li> <li><code>AUTO_INCREMENT</code> - automatically generates sequential numbers</li> <li><code>NOT NULL</code> - value required</li> <li><code>UNIQUE</code> - no duplicates allowed</li> </ul> <p>Common data types:</p> Type Description Example INT Whole numbers 42, -100, 0 VARCHAR(n) Variable-length text 'John', 'New York' TEXT Large text blocks Product descriptions DATE Date (YYYY-MM-DD) '2025-01-15' DATETIME Date and time '2025-01-15 14:30:00' DECIMAL(p,s) Precise numbers 99.99, 12345.67 BOOLEAN True/False TRUE, FALSE <p>Viewing table structure:</p> <pre><code>DESCRIBE employees;\n-- or\nSHOW COLUMNS FROM employees;\n</code></pre> <p>Altering existing tables:</p> <pre><code>-- Add a new column\nALTER TABLE employees \nADD COLUMN phone_number VARCHAR(15);\n\n-- Modify column definition\nALTER TABLE employees \nMODIFY COLUMN salary DECIMAL(12, 2);\n\n-- Rename column\nALTER TABLE employees \nCHANGE COLUMN phone_number contact_number VARCHAR(15);\n\n-- Drop column\nALTER TABLE employees \nDROP COLUMN contact_number;\n</code></pre>"},{"location":"03-data/sql/#3-selecting-data","title":"3: Selecting Data","text":"<p>SELECT retrieves data from tables. It's the most common SQL operation.</p> <p>Basic SELECT syntax:</p> <pre><code>-- Get all columns and rows\nSELECT * FROM employees;\n\n-- Get specific columns\nSELECT first_name, last_name, salary FROM employees;\n\n-- Get unique values\nSELECT DISTINCT department_id FROM employees;\n</code></pre> <p>Filtering with WHERE:</p> <pre><code>-- Single condition\nSELECT * FROM employees \nWHERE department_id = 5;\n\n-- Multiple conditions\nSELECT first_name, last_name, salary \nFROM employees \nWHERE salary &gt; 50000 AND department_id = 5;\n\n-- Pattern matching\nSELECT * FROM employees \nWHERE email LIKE '%@company.com';\n\n-- Range\nSELECT * FROM employees \nWHERE salary BETWEEN 40000 AND 80000;\n\n-- List matching\nSELECT * FROM employees \nWHERE department_id IN (1, 3, 5);\n</code></pre> <p>Sorting results:</p> <pre><code>-- Sort ascending (default)\nSELECT first_name, last_name, salary \nFROM employees \nORDER BY salary;\n\n-- Sort descending\nSELECT first_name, last_name, salary \nFROM employees \nORDER BY salary DESC;\n\n-- Multiple columns\nSELECT * FROM employees \nORDER BY department_id, last_name;\n</code></pre> <p>Limiting results:</p> <pre><code>-- Get first 10 rows\nSELECT * FROM employees \nLIMIT 10;\n\n-- Skip first 20, then get 10\nSELECT * FROM employees \nLIMIT 10 OFFSET 20;\n</code></pre>"},{"location":"03-data/sql/#4-inserting-data","title":"4: Inserting Data","text":"<p>INSERT adds new rows to tables.</p> <p>Inserting single row:</p> <pre><code>INSERT INTO employees (first_name, last_name, email, hire_date, salary, department_id)\nVALUES ('John', 'Doe', 'john.doe@company.com', '2025-01-15', 65000.00, 3);\n</code></pre> <p>Inserting multiple rows:</p> <pre><code>INSERT INTO employees (first_name, last_name, email, hire_date, salary, department_id)\nVALUES \n    ('Jane', 'Smith', 'jane.smith@company.com', '2025-02-01', 70000.00, 2),\n    ('Bob', 'Johnson', 'bob.j@company.com', '2025-02-15', 55000.00, 3),\n    ('Alice', 'Williams', 'alice.w@company.com', '2025-03-01', 80000.00, 1);\n</code></pre> <p>Tip: If you include all columns in order, you can omit column names:</p> <pre><code>INSERT INTO employees \nVALUES (NULL, 'Mike', 'Brown', 'mike@company.com', '2025-03-15', 60000.00, 2);\n-- NULL for AUTO_INCREMENT primary key\n</code></pre>"},{"location":"03-data/sql/#5-updating-data","title":"5: Updating Data","text":"<p>UPDATE modifies existing rows.</p> <p>Basic update:</p> <pre><code>-- Update single record\nUPDATE employees \nSET salary = 75000.00 \nWHERE employee_id = 5;\n\n-- Update multiple columns\nUPDATE employees \nSET salary = 68000.00, department_id = 4 \nWHERE employee_id = 3;\n\n-- Update multiple rows\nUPDATE employees \nSET salary = salary * 1.05 \nWHERE department_id = 2;\n</code></pre> <p>Warning: Always include WHERE clause unless you want to update ALL rows.</p> <pre><code>-- DANGER: Updates every row in table\nUPDATE employees SET salary = 50000.00;\n\n-- SAFE: Updates only matching rows\nUPDATE employees SET salary = 50000.00 WHERE employee_id = 10;\n</code></pre>"},{"location":"03-data/sql/#6-deleting-data","title":"6: Deleting Data","text":"<p>DELETE removes rows from tables.</p> <p>Basic deletion:</p> <pre><code>-- Delete specific row\nDELETE FROM employees \nWHERE employee_id = 15;\n\n-- Delete multiple rows\nDELETE FROM employees \nWHERE hire_date &lt; '2020-01-01';\n\n-- Delete with conditions\nDELETE FROM employees \nWHERE department_id = 7 AND salary &lt; 40000;\n</code></pre> <p>Warning: DELETE without WHERE removes all rows.</p> <pre><code>-- DANGER: Deletes entire table contents\nDELETE FROM employees;\n\n-- SAFER: Use WHERE to target specific rows\nDELETE FROM employees WHERE employee_id = 20;\n</code></pre> <p>Difference from DROP:</p> <ul> <li>DELETE: Removes rows, keeps table structure</li> <li>DROP TABLE: Deletes entire table and structure</li> <li>TRUNCATE: Removes all rows faster than DELETE (but can't undo)</li> </ul>"},{"location":"03-data/sql/#7-using-subqueries","title":"7: Using Subqueries","text":"<p>Subqueries are queries nested inside other queries, useful for complex filtering and calculations.</p> <p>Subquery in WHERE clause:</p> <pre><code>-- Find employees earning above average\nSELECT first_name, last_name, salary \nFROM employees \nWHERE salary &gt; (SELECT AVG(salary) FROM employees);\n</code></pre> <p>Subquery with IN:</p> <pre><code>-- Find employees in departments located in 'New York'\nSELECT first_name, last_name \nFROM employees \nWHERE department_id IN (\n    SELECT department_id \n    FROM departments \n    WHERE location = 'New York'\n);\n</code></pre> <p>Subquery in SELECT (correlated):</p> <pre><code>-- Show each employee with their department's average salary\nSELECT \n    first_name, \n    last_name, \n    salary,\n    (SELECT AVG(salary) \n     FROM employees e2 \n     WHERE e2.department_id = e1.department_id) AS dept_avg_salary\nFROM employees e1;\n</code></pre> <p>When to use subqueries:</p> <ul> <li>Filter based on aggregated data</li> <li>Compare values across tables</li> <li>Perform calculations for each row based on related data</li> </ul> <p>Alternative: JOINs often perform better than subqueries for relating tables.</p>"},{"location":"03-data/sql/#8-using-mysql-functions","title":"8: Using MySQL Functions","text":"<p>MySQL provides built-in functions for calculations, text manipulation, and data transformation.</p> <p>Aggregate functions:</p> <pre><code>-- Count rows\nSELECT COUNT(*) FROM employees;\n\n-- Count non-null values\nSELECT COUNT(email) FROM employees;\n\n-- Sum values\nSELECT SUM(salary) FROM employees;\n\n-- Average\nSELECT AVG(salary) FROM employees;\n\n-- Min and Max\nSELECT MIN(salary), MAX(salary) FROM employees;\n\n-- Aggregate with grouping\nSELECT department_id, AVG(salary) AS avg_salary, COUNT(*) AS employee_count\nFROM employees\nGROUP BY department_id;\n</code></pre> <p>String functions:</p> <pre><code>-- Concatenate\nSELECT CONCAT(first_name, ' ', last_name) AS full_name \nFROM employees;\n\n-- Uppercase/lowercase\nSELECT UPPER(first_name), LOWER(email) \nFROM employees;\n\n-- Substring\nSELECT SUBSTRING(email, 1, 5) AS email_prefix \nFROM employees;\n\n-- Length\nSELECT first_name, LENGTH(first_name) AS name_length \nFROM employees;\n</code></pre> <p>Date functions:</p> <pre><code>-- Current date and time\nSELECT NOW(), CURDATE(), CURTIME();\n\n-- Extract parts\nSELECT \n    hire_date,\n    YEAR(hire_date) AS hire_year,\n    MONTH(hire_date) AS hire_month,\n    DAY(hire_date) AS hire_day\nFROM employees;\n\n-- Date arithmetic\nSELECT \n    hire_date,\n    DATE_ADD(hire_date, INTERVAL 90 DAY) AS probation_end\nFROM employees;\n\n-- Date difference\nSELECT \n    first_name,\n    DATEDIFF(CURDATE(), hire_date) AS days_employed\nFROM employees;\n</code></pre> <p>Mathematical functions:</p> <pre><code>-- Rounding\nSELECT salary, ROUND(salary, -3) AS rounded_salary \nFROM employees;\n\n-- Absolute value\nSELECT ABS(-42);\n\n-- Power\nSELECT POWER(2, 3);  -- Returns 8\n</code></pre>"},{"location":"03-data/sql/#common-sql-challenges","title":"Common SQL Challenges","text":""},{"location":"03-data/sql/#null-values","title":"NULL Values","text":"<p>Problem: NULL represents missing data and behaves differently than regular values.</p> <pre><code>-- Wrong: This doesn't work as expected\nSELECT * FROM employees WHERE email = NULL;\n\n-- Correct: Use IS NULL / IS NOT NULL\nSELECT * FROM employees WHERE email IS NULL;\nSELECT * FROM employees WHERE email IS NOT NULL;\n\n-- Handling NULLs in calculations\nSELECT first_name, COALESCE(phone_number, 'No phone') AS contact\nFROM employees;\n</code></pre>"},{"location":"03-data/sql/#avoiding-accidental-data-loss","title":"Avoiding Accidental Data Loss","text":"<p>Problem: DELETE or UPDATE without WHERE affects all rows.</p> <p>Solution: Always test with SELECT first:</p> <pre><code>-- Step 1: Test your WHERE clause\nSELECT * FROM employees WHERE department_id = 7;\n\n-- Step 2: If results look correct, change to DELETE\nDELETE FROM employees WHERE department_id = 7;\n</code></pre>"},{"location":"03-data/sql/#understanding-distinct-vs-group-by","title":"Understanding DISTINCT vs GROUP BY","text":"<p>Both reduce duplicates but serve different purposes:</p> <pre><code>-- DISTINCT: Returns unique combinations\nSELECT DISTINCT department_id FROM employees;\n\n-- GROUP BY: Aggregates data\nSELECT department_id, COUNT(*) \nFROM employees \nGROUP BY department_id;\n</code></pre> <p>Use <code>DISTINCT</code> for simple deduplication, <code>GROUP BY</code> when calculating aggregates.</p>"},{"location":"03-data/sql/#best-practices","title":"Best Practices","text":"<p>Always use WHERE with UPDATE and DELETE: Protect against accidentally modifying all rows. Test your WHERE clause with SELECT first.</p> <p>Name things clearly: Use descriptive names like <code>customer_email</code> instead of <code>ce</code>. Future you will thank past you.</p> <p>Use consistent naming conventions: Pick a style (snake_case or camelCase) and stick with it throughout your database.</p> <p>Index frequently queried columns: Add indexes to columns used in <code>WHERE</code> clauses and <code>JOIN</code>s to speed up queries. Primary keys are automatically indexed.</p> <pre><code>CREATE INDEX idx_employee_department ON employees(department_id);\n</code></pre> <p>Back up before structural changes: Always back up data before running ALTER TABLE or DROP statements.</p> <p>Comment complex queries: Add comments to explain business logic:</p> <pre><code>-- Calculate bonus for employees hired in 2024 with sales &gt; $100k\nSELECT \n    employee_id,\n    salary * 0.10 AS bonus  -- 10% of salary\nFROM employees \nWHERE YEAR(hire_date) = 2024;\n</code></pre> <p>Use transactions for multi-step operations: Ensure all-or-nothing execution:</p> <pre><code>START TRANSACTION;\n\nUPDATE accounts SET balance = balance - 100 WHERE account_id = 1;\nUPDATE accounts SET balance = balance + 100 WHERE account_id = 2;\n\nCOMMIT;  -- Or ROLLBACK if something went wrong\n</code></pre>"},{"location":"03-data/sql/#quick-reference","title":"Quick Reference","text":""},{"location":"03-data/sql/#essential-sql-commands","title":"Essential SQL Commands","text":"Command Purpose Example <code>CREATE DATABASE</code> Create new database <code>CREATE DATABASE company;</code> <code>USE</code> Select database <code>USE company;</code> <code>CREATE TABLE</code> Define new table <code>CREATE TABLE users (...);</code> <code>SELECT</code> Query data <code>SELECT * FROM users;</code> <code>WHERE</code> Filter rows <code>WHERE age &gt; 25</code> <code>ORDER BY</code> Sort results <code>ORDER BY name DESC</code> <code>INSERT INTO</code> Add rows <code>INSERT INTO users VALUES (...);</code> <code>UPDATE</code> Modify rows <code>UPDATE users SET age = 30 WHERE id = 5;</code> <code>DELETE</code> Remove rows <code>DELETE FROM users WHERE id = 10;</code> <code>ALTER TABLE</code> Modify table <code>ALTER TABLE users ADD COLUMN email VARCHAR(100);</code>"},{"location":"03-data/sql/#common-operators","title":"Common Operators","text":"Operator Meaning Example <code>=</code> Equals <code>WHERE status = 'active'</code> <code>!=</code> or <code>&lt;&gt;</code> Not equals <code>WHERE status != 'inactive'</code> <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code> Comparison <code>WHERE salary &gt; 50000</code> <code>BETWEEN</code> Range <code>WHERE age BETWEEN 25 AND 35</code> <code>IN</code> Match list <code>WHERE dept IN (1, 2, 3)</code> <code>LIKE</code> Pattern match <code>WHERE name LIKE 'J%'</code> <code>IS NULL</code> Check for NULL <code>WHERE email IS NULL</code> <code>AND</code>, <code>OR</code>, <code>NOT</code> Logical <code>WHERE age &gt; 25 AND dept = 5</code>"},{"location":"03-data/sql/#basic-query-template","title":"Basic Query Template","text":"<pre><code>SELECT column1, column2, aggregate_function(column3)\nFROM table_name\nWHERE condition\nGROUP BY column1\nHAVING aggregate_condition\nORDER BY column1 DESC\nLIMIT 10;\n</code></pre>"},{"location":"03-data/sql/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned what databases and relational databases are, how to create databases and tables with proper structure, how to query data with SELECT and filter with WHERE, how to insert, update, and delete records safely, how subqueries enable complex filtering, and how to use built-in MySQL functions.</p> <p>Critical insights:</p> <ul> <li>SQL is declarative: Describe what you want, not how to get it</li> <li>Structure matters: Well-designed tables with proper data types prevent problems</li> <li>Test before executing: Always verify WHERE clauses with SELECT before UPDATE/DELETE</li> <li>Functions extend capabilities: Built-in functions handle common tasks efficiently</li> </ul> <p>What's next:</p> <p>With SQL fundamentals mastered, you're ready to explore JOINs (combining multiple tables), indexes (optimizing query performance), views (saving complex queries), and stored procedures (reusable SQL logic). You can also start integrating SQL queries into Python workflows using libraries like <code>pandas.read_sql()</code> or SQLAlchemy.</p> <p>Practice resources:</p> <ul> <li>SQLBolt - interactive SQL lessons with instant feedback</li> <li>Mode SQL Tutorial - real datasets for practice</li> <li>LeetCode Database Problems - SQL challenges</li> </ul> <p>External resources:</p> <ul> <li>MySQL Official Documentation - comprehensive reference</li> <li>W3Schools SQL Tutorial - quick examples and testing</li> <li>SQL Style Guide - formatting conventions</li> </ul> <p>Remember: SQL is the foundation of data work. Master these basics, and you'll access, manipulate, and analyze data from any relational database with confidence.</p>"},{"location":"03-data/visualization/","title":"Data Visualization: Seeing Patterns in Data","text":"<p>This tutorial introduces the essential role of visualization in machine learning workflows and guides you through choosing and creating effective visualizations. You'll understand why seeing your data visually is crucial for building successful models and learn the systematic approach to exploring data through charts and graphs.</p> <p>Estimated time: 45 minutes</p> <p>Hands-on practice: For practical code examples using the Albania House Prices dataset, see the companion Jupyter notebook.</p>"},{"location":"03-data/visualization/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: </p> <p>You can't understand what you can't see. </p> <p>Data hidden in tables remains abstract, making it nearly impossible to spot patterns, outliers, or relationships that directly impact model performance. A correlation of 0.85 between features means little until you see the actual scatter plot. An imbalanced dataset with 95% of one class isn't obvious from summary statistics alone, but a bar chart makes it immediately clear.</p> <p>Professional context: Data scientists create dozens of exploratory plots before selecting model features. Companies that visualize data pipelines catch errors 3-5x faster than those relying on numerical validation alone. The ability to choose the right visualization and communicate insights visually separates junior practitioners from senior data scientists. Stakeholders rarely read statistical reports, but they always remember compelling visualizations.</p> <p></p>"},{"location":"03-data/visualization/#core-concepts","title":"Core Concepts","text":""},{"location":"03-data/visualization/#what-is-data-visualization","title":"What is Data Visualization?","text":"<p>Data visualization is the graphical representation of information, transforming numerical values into visual forms like charts, graphs, and plots. In machine learning contexts, visualization serves three purposes:</p> <ul> <li>Exploratory Analysis: Discovering patterns, distributions, and relationships before modeling</li> <li>Diagnostic Analysis: Understanding why models succeed or fail through residual plots, confusion matrices, and learning curves</li> <li>Communication: Presenting findings to technical and non-technical audiences</li> </ul>"},{"location":"03-data/visualization/#the-visualization-pipeline","title":"The Visualization Pipeline","text":"<p>Data \u2192 Chart Type Selection \u2192 Encoding \u2192 Rendering \u2192 Interpretation</p> <p></p> <p>Each step requires decisions: What question am I answering? Which chart type reveals this pattern best? How should I map data to visual properties? What customizations improve clarity?</p>"},{"location":"03-data/visualization/#visual-encoding-channels","title":"Visual Encoding Channels","text":"<p>Visual encoding maps data values to visual properties:</p> <ul> <li>Position: Most accurate encoding (x-axis, y-axis placement)</li> <li>Length: Highly effective (bar heights, line lengths)</li> <li>Angle: Moderately effective (pie slices)</li> <li>Area: Less accurate (bubble sizes)</li> <li>Color: Good for categories, challenging for continuous values</li> <li>Shape: Effective for categorical distinctions</li> </ul> <p>The hierarchy matters because position encodings (scatter plots) communicate information more accurately than area encodings (bubble charts), affecting how viewers interpret your data.</p>"},{"location":"03-data/visualization/#matplotlib-vs-seaborn-complementary-tools","title":"Matplotlib vs Seaborn: Complementary Tools","text":"<p>Matplotlib provides complete control over every plot element, requiring explicit commands for each component. Think of it as the foundation layer.</p> <p>Seaborn builds on Matplotlib, offering high-level interfaces for statistical visualizations with better default aesthetics and built-in statistical computations.</p> <p>When to use each: Use Matplotlib for custom plots, precise control, and fundamental chart types. Use Seaborn for statistical analysis, quick exploratory plots, and when you need beautiful defaults with minimal code.</p>"},{"location":"03-data/visualization/#visualization-types-and-their-purposes","title":"Visualization Types and Their Purposes","text":"<p>Choosing the right visualization depends on your data structure and the question you're answering. Using the wrong chart type obscures insights or misleads viewers.</p>"},{"location":"03-data/visualization/#distribution-visualizations","title":"Distribution Visualizations","text":"<p>Understand how values are spread across a variable.</p> <p></p>"},{"location":"03-data/visualization/#histogram","title":"Histogram","text":"<p>Shows frequency distribution by grouping continuous data into bins. Reveals shape (normal, skewed, bimodal), spread, and outliers.</p> <p>Use it when exploring numerical features like age, price, or income to understand central tendency and variability.</p>"},{"location":"03-data/visualization/#box-plot","title":"Box Plot","text":"<p>Displays five-number summary (min, Q1, median, Q3, max) plus outliers. Compact representation ideal for comparing distributions across categories.</p> <p>Use when comparing numerical distributions across groups (price by zone, salary by department). It can help to quickly identifies outliers that might require handling. Reveals class imbalance when box plots for different target classes differ dramatically.</p>"},{"location":"03-data/visualization/#violin-plot","title":"Violin Plot","text":"<p>Combines box plot with kernel density estimation, showing distribution shape through width variations.</p> <p>Use when you need both summary statistics and distribution shape, especially for multimodal distributions that box plots miss.</p> <p>It helps to reveal complex distributional patterns that inform feature engineering and model selection decisions.</p>"},{"location":"03-data/visualization/#comparison-visualizations","title":"Comparison Visualizations","text":"<p>Compare values across categories or groups.</p>"},{"location":"03-data/visualization/#bar-chart","title":"Bar Chart","text":"<p>Uses rectangular bars to represent values across discrete categories. Bar heights encode magnitudes, making comparisons immediate.</p> <p>Use when comparing metrics across categories (sales by region, model accuracy by algorithm).</p> <p>Compares model performance metrics across different algorithms or hyperparameter settings. Horizontal bars work well for many categories or long labels.</p>"},{"location":"03-data/visualization/#grouped-bar-chart","title":"Grouped Bar Chart","text":"<p>Places multiple bars side-by-side for each category, enabling multi-variable comparisons.</p> <p>Use when comparing multiple metrics simultaneously (precision and recall across classifiers).</p> <p>Compares training vs validation performance across models, revealing overfitting when bars differ significantly.</p>"},{"location":"03-data/visualization/#relationship-visualizations","title":"Relationship Visualizations","text":"<p>Reveal relationships, correlations, and patterns between variables.</p>"},{"location":"03-data/visualization/#scatter-plot","title":"Scatter Plot","text":"<p>Displays individual data points using two numerical variables for x and y positions. Reveals correlations, clusters, and outliers.</p> <p>Use when exploring relationships between continuous variables (size vs price, age vs salary).</p> <p>It validates linear regression assumptions. Strong linear patterns suggest predictive relationships. Clusters indicate potential for classification. Outliers far from trends require investigation.</p>"},{"location":"03-data/visualization/#line-plot","title":"Line Plot","text":"<p>Connects data points with lines, emphasizing trends and changes over sequences (often time).</p> <p>Use when showing trends over time (stock prices, model performance across epochs, website traffic).</p> <p>Tracks training and validation loss/accuracy across epochs. Diverging lines signal overfitting.</p>"},{"location":"03-data/visualization/#heatmap","title":"Heatmap","text":"<p>Uses color intensity to represent values in a matrix format. Particularly effective for correlation matrices.</p> <p>Use when visualizing relationships between many variables simultaneously (correlation matrix, confusion matrix).</p> <p>Identifies highly correlated features that might cause multicollinearity. Confusion matrices reveal which classes models confuse most frequently.</p>"},{"location":"03-data/visualization/#regression-plot","title":"Regression Plot","text":"<p>Scatter plot with fitted regression line and confidence interval bands.</p> <p>Use when assessing linear relationships and prediction confidence.</p> <p>Visualizes regression model fit quality. Wide confidence bands indicate high uncertainty requiring more data or better features.</p>"},{"location":"03-data/visualization/#composition-visualizations","title":"Composition Visualizations","text":"<p>Show how parts contribute to a whole.</p>"},{"location":"03-data/visualization/#pie-chart","title":"Pie Chart","text":"<p>Circular chart divided into slices representing proportions of a whole. Most effective for 3-5 categories.</p> <p>Use when showing simple proportions where exact values matter less than relative sizes (market share, budget allocation).</p> <p>It visualizes class distribution in classification problems. Reveals severe imbalance requiring resampling techniques.</p> <p>However, human perception struggles with angle comparisons. Use sparingly and only when showing parts of 100%.</p>"},{"location":"03-data/visualization/#stacked-bar-chart","title":"Stacked Bar Chart","text":"<p>Bars divided into segments representing component contributions.</p> <p>Use when comparing both totals and component breakdowns across categories.</p> <p>It shows how feature importance varies across different model versions or datasets.</p>"},{"location":"03-data/visualization/#the-visualization-workflow","title":"The Visualization Workflow","text":""},{"location":"03-data/visualization/#phase-1-define-your-question","title":"Phase 1: Define Your Question","text":"<p>Clarify what you're trying to understand before creating visualizations.</p> <p>Key questions:</p> <ul> <li>What pattern am I looking for? (distribution, relationship, comparison, trend)</li> <li>What decisions will this visualization inform?</li> <li>Who is the audience? (technical team vs business stakeholders)</li> </ul> <p>ML connection: Exploratory analysis requires different visualizations than model diagnostics or stakeholder presentations. Asking \"Why did this model fail on these samples?\" guides you toward residual plots and confusion matrices, not histograms.</p> <p>Example questions:</p> <ul> <li>\"Are features correlated?\" \u2192 Heatmap</li> <li>\"Is the target class balanced?\" \u2192 Bar chart or pie chart</li> <li>\"Does feature X predict target Y?\" \u2192 Scatter plot or regression plot</li> <li>\"How are values distributed?\" \u2192 Histogram or box plot</li> </ul>"},{"location":"03-data/visualization/#phase-2-choose-the-right-visualization","title":"Phase 2: Choose the Right Visualization","text":"<p>Match visualization type to data structure and question.</p> <p>Decision framework:</p> <ul> <li>One numerical variable: Histogram, box plot, violin plot</li> <li>One categorical variable: Bar chart, pie chart</li> <li>Two numerical variables: Scatter plot, line plot (if sequential), regression plot</li> <li>One numerical + one categorical: Box plot, violin plot, grouped bar chart</li> <li>Two categorical variables: Heatmap, stacked bar chart, count plot</li> <li>Multiple numerical variables: Correlation heatmap, pair plot, parallel coordinates</li> </ul>"},{"location":"03-data/visualization/#phase-3-create-the-visualization","title":"Phase 3: Create the Visualization","text":"<p>Implement the chosen visualization with appropriate libraries and parameters.</p> <ul> <li>Matplotlib approach: Build plots layer-by-layer with explicit control over every element.</li> <li>Seaborn approach: Use high-level functions that handle statistical computations and styling automatically.</li> </ul> <p>Key implementation considerations:</p> <ul> <li>Figure size: Larger plots for presentations, smaller for reports</li> <li>Color choices: Colorblind-friendly palettes, meaningful color schemes</li> <li>Labels and titles: Clear, descriptive text without jargon</li> <li>Axis scales: Linear, logarithmic, or custom based on data range</li> </ul>"},{"location":"03-data/visualization/#phase-4-customize-for-clarity","title":"Phase 4: Customize for Clarity","text":"<p>Improve readability and eliminate ambiguity.</p> <p>Essential customizations:</p> <ul> <li>Titles and labels: Every plot needs descriptive title and labeled axes with units</li> <li>Legends: Required when multiple categories or series appear</li> <li>Gridlines: Subtle gridlines aid value reading without cluttering</li> <li>Annotations: Highlight specific points or regions of interest</li> <li>Color schemes: Use consistent, accessible palettes</li> <li>Font sizes: Readable at intended viewing distance</li> </ul>"},{"location":"03-data/visualization/#phase-5-interpret-and-act","title":"Phase 5: Interpret and Act","text":"<p>Extract insights that guide next steps in your ML workflow.</p> <p>Interpretation questions:</p> <ul> <li>What patterns are visible? (clusters, trends, outliers)</li> <li>What surprises appear? (unexpected relationships, missing values)</li> <li>What actions should follow? (data cleaning, feature engineering, model selection)</li> </ul> <p>ML connection: Visualization insights directly inform pipeline decisions:</p> <ul> <li>Skewed distributions \u2192 Apply transformations (log, square root)</li> <li>Outliers \u2192 Investigate causes, decide to keep/remove/cap</li> <li>Missing value patterns \u2192 Choose imputation strategy</li> <li>Class imbalance \u2192 Apply resampling or adjust class weights</li> <li>Feature correlations \u2192 Remove redundant features or apply PCA</li> <li>Non-linear relationships \u2192 Engineer polynomial features or choose non-linear models</li> </ul>"},{"location":"03-data/visualization/#common-visualization-challenges","title":"Common Visualization Challenges","text":""},{"location":"03-data/visualization/#overplotting","title":"Overplotting","text":"<p>Problem: Too many data points overlap, obscuring density and patterns.</p> <p>Impact: Cannot see true data distribution, clusters appear as uniform blobs.</p> <p>Solutions:</p> <ul> <li>Transparency (alpha): Make points semi-transparent to show density</li> <li>Sampling: Plot random subset for initial exploration</li> <li>Binning: Use hexbin plots or 2D histograms</li> <li>Jitter: Add small random noise to separate overlapping points</li> </ul>"},{"location":"03-data/visualization/#choosing-colors","title":"Choosing Colors","text":"<p>Problem: Color choices that aren't colorblind-safe or don't convey meaning appropriately.</p> <p>Impact: ~8% of men and ~0.5% of women can't distinguish red-green, making many default palettes useless to them.</p> <p>Solutions:</p> <ul> <li>Use colorblind-safe palettes: Viridis, Cividis, colorbrewer schemes</li> <li>Don't rely solely on color: Use shapes or line styles too</li> <li>Sequential palettes: For continuous values (light to dark)</li> <li>Diverging palettes: For values with meaningful zero (negative to positive)</li> <li>Qualitative palettes: For categorical data</li> </ul> <p>Model comparison plots become meaningless if stakeholders can't distinguish the colors representing different algorithms.</p>"},{"location":"03-data/visualization/#scale-and-range-issues","title":"Scale and Range Issues","text":"<p>Problem: Inappropriate axis scales that hide patterns or mislead.</p> <p>Impact: Linear scales compress high values, making trends invisible. Truncated axes exaggerate small differences.</p> <p>Solutions:</p> <ul> <li>Log scales: For data spanning orders of magnitude</li> <li>Start axes at zero: For bar charts and comparisons</li> <li>Full range: Show complete data range unless justified otherwise</li> <li>Breaks: Indicate discontinuous axes explicitly</li> </ul> <p>ML connection: Learning curves plotted on inappropriate scales can hide early overfitting or make converged training appear unstable.</p>"},{"location":"03-data/visualization/#too-much-information","title":"Too Much Information","text":"<p>Problem: Cramming multiple messages into one visualization, creating confusion.</p> <p>Impact: Viewers miss key insights when overwhelmed by complexity.</p> <p>Solutions:</p> <ul> <li>One message per plot: Focus each visualization on single insight</li> <li>Progressive disclosure: Start simple, add complexity gradually</li> <li>Small multiples: Create grid of related simple plots rather than one complex plot</li> <li>Annotations: Guide viewers to important patterns</li> </ul> <p>Model diagnostic dashboards should separate concerns: one plot for learning curves, another for confusion matrix, another for feature importance. Combining them dilutes each message.</p>"},{"location":"03-data/visualization/#best-practices","title":"Best Practices","text":"<p>Start simple, add complexity only when necessary: Begin with basic plot, add elements that enhance understanding. Remove anything that doesn't serve the message.</p> <p>Label everything explicitly: Titles, axis labels, legends, and units should be clear without external context. Future you will forget what <code>variable_3</code> means.</p> <p>Choose appropriate chart types: Bar charts for comparisons, scatter plots for relationships, histograms for distributions. Wrong chart type obscures rather than reveals.</p> <p>Use consistent styling: Maintain color schemes, fonts, and layouts across related visualizations. Consistency reduces cognitive load.</p> <p>Consider colorblindness: Use colorblind-safe palettes. Never encode information only through color.</p> <p>Size appropriately: Match figure size to viewing context. Presentation slides need larger fonts than research papers.</p> <p>Annotate insights: Highlight patterns, outliers, or thresholds directly on plots. Don't make viewers search for the message.</p> <p>Test with audience: Show drafts to representative viewers. What's obvious to you may confuse others.</p> <p>Version and save source code: Save visualization code, not just images. Reproducibility requires the ability to regenerate plots with updated data.</p> <p>Know when to use 3D: Almost never. 3D charts look impressive but make reading values nearly impossible. Flat projections usually communicate better.</p>"},{"location":"03-data/visualization/#quick-reference-visualization-selection-guide","title":"Quick Reference: Visualization Selection Guide","text":"Data Structure Question Type Best Visualization Library Recommendation One numerical Distribution shape Histogram, Box plot Matplotlib, Seaborn One categorical Category counts Bar chart, Pie chart Matplotlib, Seaborn Two numerical Relationship strength Scatter plot, Regression plot Matplotlib, Seaborn Two numerical (time) Trend over time Line plot Matplotlib Numerical + Categorical Distribution by group Box plot, Violin plot Seaborn Many numerical Correlations Heatmap, Pair plot Seaborn Actual vs Predicted Model accuracy Scatter plot, Residual plot Matplotlib Classification errors Confusion patterns Confusion matrix heatmap Seaborn Training progress Overfitting detection Learning curves Matplotlib"},{"location":"03-data/visualization/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned why visualization is fundamental to ML success, understood how to choose appropriate chart types for different data structures and questions, recognized common visualization pitfalls and solutions, and connected effective visualization practices to model development and communication.</p> <p>Critical insights:</p> <ul> <li>See before you model: Visualization prevents costly modeling mistakes by revealing data issues early</li> <li>Match chart to question: Different analytical questions require different visualization types</li> <li>Clarity over complexity: Simple, well-labeled plots communicate better than elaborate designs</li> <li>Iterate visually: Create dozens of exploratory plots to understand data deeply</li> </ul> <p>Connections to previous topics:</p> <ul> <li>Data Foundations: Visualization reveals the data quality issues identified in data pipelines</li> <li>Statistics: Distributions, correlations, and statistical tests become concrete through visualization</li> <li>Pandas: DataFrames integrate seamlessly with visualization libraries for rapid exploration</li> </ul> <p>External resources:</p> <ul> <li>Matplotlib Official Documentation - comprehensive reference and tutorials</li> <li>Seaborn Gallery - visual examples of all plot types</li> <li>The Data Visualisation Catalogue - guide to choosing chart types</li> <li>ColorBrewer - colorblind-safe palette generator</li> </ul> <p>Remember: Data visualization isn't about making pretty pictures\u2014it's about seeing patterns that guide better decisions. Master visualization, and you'll build better models because you'll understand your data deeply.</p>"},{"location":"04-core-ml/","title":"Core Machine Learning Overview","text":"<p>Build solid ML skills step by step. Clear goals, small projects, and habits that make models reliable in the real world.</p> <p></p>"},{"location":"04-core-ml/#introduction-to-machine-learning","title":"Introduction to Machine Learning","text":"<p>What ML is and when to use it</p> <p>Understand the difference between ML, AI, and Deep Learning. Learn when ML makes sense and when traditional programming is better. Preview the full workflow before diving into algorithms.</p> <p>You'll learn: ML definitions, real-world applications, when to use ML, prerequisites check</p>"},{"location":"04-core-ml/#ml-lifecycle","title":"ML Lifecycle","text":"<p>From problem to production</p> <p>See the complete journey: define the problem, collect and prepare data, train models, evaluate with proper metrics, deploy, and monitor. Learn to iterate\u2014measure, improve, repeat.</p> <p>You'll learn: Problem framing, data preparation, experiment tracking, model evaluation, deployment basics, monitoring</p>"},{"location":"04-core-ml/#regression","title":"Regression","text":"<p>Predict continuous values</p> <p>Master Linear Regression from the ground up. Understand cost functions, training processes, and evaluation metrics. Learn to spot and fix overfitting.</p> <p>You'll learn: Linear regression, polynomial regression, cost functions (MSE), R\u00b2, RMSE, MAE, bias-variance tradeoff</p>"},{"location":"04-core-ml/#classification","title":"Classification","text":"<p>Predict categories</p> <p>Build classifiers that generalize. From Logistic Regression to Decision Trees and Random Forests. Master evaluation metrics and confusion matrices.</p> <p>You'll learn: Logistic regression, decision trees, random forests, accuracy, precision, recall, F1, ROC-AUC</p>"},{"location":"04-core-ml/#supervised-advanced","title":"Supervised Advanced","text":"<p>Expand your toolkit</p> <p>Add KNN, Naive Bayes, and SVM to your arsenal. Compare algorithms and learn when to use each. Build intuition for model selection.</p> <p>You'll learn: k-Nearest Neighbors, Naive Bayes, Support Vector Machines, algorithm comparison, model selection</p>"},{"location":"04-core-ml/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Find patterns without labels</p> <p>Discover hidden structure in data. Cluster similar items and compress features while keeping signal. Master evaluation without ground truth.</p> <p>You'll learn: K-means clustering, hierarchical clustering, DBSCAN, PCA, dimensionality reduction, evaluation strategies</p> <p>Start simple, measure honestly, and improve in small steps. A clear baseline plus good metrics beats fancy algorithms every time.</p>"},{"location":"04-core-ml/clustering/","title":"A General Guide to Clustering Analysis","text":"<p>This guide provides a comprehensive overview of cluster analysis, the unsupervised ML approach for discovering natural groupings in data. It covers the core concepts, compares different families of algorithms, and explains the methods used to validate their results.</p> <p>For a hands-on application of these concepts, see the accompanying tutorial: Clustering the Iris Dataset with K-Means.</p> <p>Estimated time: 40\u201345 minutes</p>"},{"location":"04-core-ml/clustering/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement:</p> <p>How can we find meaningful structure in our data when we don't have predefined labels to guide us?</p> <p>Cluster analysis answers this by identifying and characterizing hidden segments. This is a foundational capability for any data-driven organization, enabling teams to move from a chaotic sea of data points to an organized set of actionable insights.</p> <p></p> <p>Practical benefits:</p> <ul> <li>Discover Structure: Understand the natural \"shapes\" and densities in your data (e.g., customer behavior archetypes, product usage patterns).</li> <li>Simplify Complexity: Summarize thousands or millions of items into a handful of understandable groups.</li> <li>Enable Targeted Actions: Apply different strategies to different segments (e.g., targeted marketing, personalized support).</li> </ul>"},{"location":"04-core-ml/clustering/#foundational-concepts","title":"Foundational Concepts","text":""},{"location":"04-core-ml/clustering/#what-is-a-multimodal-distribution","title":"What is a Multimodal Distribution?","text":"<p>A distribution is multimodal if it has two or more distinct peaks (or \"modes\"). In the context of data, each peak represents a high-density area where data points are concentrated. The existence of multiple modes is a strong signal that the underlying data is not a single homogeneous group, but rather a mix of several subgroups. Cluster analysis is fundamentally the act of identifying and separating these modes.</p> <p></p>"},{"location":"04-core-ml/clustering/#what-is-a-cluster","title":"What is a Cluster?","text":"<p>A cluster is a group of data points that are more similar to each other than to points in other groups. While the concept is intuitive, the precise definition depends on the algorithm used:</p> <ul> <li>Density-based: A cluster is a dense region of points separated by low-density areas.</li> <li>Centroid-based: A cluster is a set of points whose center of mass is a shared prototype (the centroid).</li> <li>Contiguity-based: A cluster is a set of points that are connected to one another.</li> </ul> <p></p>"},{"location":"04-core-ml/clustering/#what-is-cluster-analysis","title":"What is Cluster Analysis?","text":"<p>Cluster analysis is the formal process of partitioning a dataset into clusters. It encompasses the entire workflow, from data preparation and algorithm selection to determining the number of clusters and interpreting the final results.</p>"},{"location":"04-core-ml/clustering/#hard-vs-soft-clustering","title":"\u201cHard\u201d vs. \u201cSoft\u201d Clustering","text":"<p>This is a critical distinction in how an item's membership in a group is defined.</p> <ul> <li>Hard Clustering (e.g., K-Means): Each data point is assigned exclusively to one and only one cluster. The output is a definite, unambiguous assignment. This is like sorting mail into distinct bins.</li> <li>Soft Clustering (e.g., GMM): Each data point is assigned a probability or likelihood of belonging to each cluster. A point can be 70% in Cluster A, 25% in Cluster B, and 5% in Cluster C. This is useful for representing uncertainty and overlap, especially for points that lie on the border between groups.</li> </ul>"},{"location":"04-core-ml/clustering/#centroid-based-clustering-k-means","title":"Centroid-Based Clustering: K-Means","text":"<p>K-Means is the most common and straightforward clustering algorithm. It is a hard clustering method that groups data by minimizing distances to cluster centers.</p>"},{"location":"04-core-ml/clustering/#what-is-k-means-clustering","title":"What is K-Means Clustering?","text":"<p>The algorithm's goal is to partition data into K predefined clusters. It works as follows:</p> <ol> <li>Initialize: Randomly place K centroids in the feature space.</li> <li>Assign: Assign each data point to its nearest centroid.</li> <li>Update: Recalculate each centroid as the mean of all points assigned to it.</li> <li>Repeat: Repeat the Assign and Update steps until the centroids no longer move.</li> </ol>"},{"location":"04-core-ml/clustering/#what-is-cluster-variance-inertia","title":"What is Cluster Variance (Inertia)?","text":"<p>In K-Means, the quality of the clusters is often measured by Inertia, also known as Within-Cluster Sum of Squares (WCSS). It is the sum of squared distances from each point to its assigned centroid. Lower inertia means clusters are more compact and dense, which is generally better.</p>"},{"location":"04-core-ml/clustering/#probabilistic-clustering-mixture-models","title":"Probabilistic Clustering: Mixture Models","text":"<p>Mixture models offer a soft clustering approach, assuming the data is a combination (or \"mixture\") of several different probability distributions.</p>"},{"location":"04-core-ml/clustering/#what-are-mixture-models","title":"What are Mixture Models?","text":"<p>A mixture model frames the data as being generated from a mix of several underlying component distributions. Instead of assigning a point to a cluster, it calculates the probability that the point was generated by each component distribution.</p>"},{"location":"04-core-ml/clustering/#what-is-a-gaussian-mixture-model-gmm","title":"What is a Gaussian Mixture Model (GMM)?","text":"<p>A Gaussian Mixture Model (GMM) is the most common type of mixture model. It assumes that the data is a mixture of a finite number of Gaussian distributions (bell curves). This is powerful because GMM can model clusters that are not just circular, but also elliptical (oval-shaped) and have different orientations.</p>"},{"location":"04-core-ml/clustering/#what-is-the-expectation-maximization-em-algorithm","title":"What is the Expectation-Maximization (EM) Algorithm?","text":"<p>Since we don't know the properties of the underlying Gaussians (their mean, variance) or which points belong to which, we can't solve for them directly. The Expectation-Maximization (EM) algorithm is an iterative method for finding these unknown parameters. It's a two-step process:</p> <ol> <li>The E-Step (Expectation): \"Guess\" the assignments. For each data point, calculate the probability that it belongs to each cluster, given the current parameters of our Gaussian distributions. This is the \"soft\" assignment.</li> <li>The M-Step (Maximization): \"Update\" the parameters. Using the probabilities from the E-step, recalculate the parameters of each Gaussian distribution (mean, covariance, etc.) to maximize the likelihood of the data.</li> </ol> <p>The algorithm repeats these two steps until the model's parameters stabilize.</p>"},{"location":"04-core-ml/clustering/#hierarchical-clustering","title":"Hierarchical Clustering","text":"<p>Hierarchical clustering builds a hierarchy of clusters, represented as a tree-like structure called a dendrogram. It doesn't require you to pre-specify the number of clusters.</p>"},{"location":"04-core-ml/clustering/#what-is-hierarchical-clustering","title":"What is Hierarchical Clustering?","text":"<p>This method creates nested clusters by either merging or splitting them successively. There are two main approaches:</p> <ul> <li>Agglomerative (Bottom-Up): Starts with each data point as its own cluster and progressively merges the two closest clusters until only one cluster (containing all points) remains.</li> <li>Divisive (Top-Down): Starts with all data points in a single cluster and recursively splits it until each point is its own cluster.</li> </ul> <p>Agglomerative clustering is the more common of the two. The key design choice is the \"linkage criterion\"\u2014the rule for measuring the distance between two clusters (e.g., the distance between their closest points, their furthest points, or their centroids).</p>"},{"location":"04-core-ml/clustering/#determining-the-number-of-clusters","title":"Determining the Number of Clusters","text":"<p>Choosing the correct number of clusters (K) is one of the most critical steps.</p>"},{"location":"04-core-ml/clustering/#what-is-the-mountainelbow-method","title":"What is the Mountain/Elbow Method?","text":"<p>This heuristic is primarily used for K-Means. By plotting the Inertia for a range of K values, we look for the \"elbow\"\u2014the point on the graph where adding more clusters no longer provides a significant reduction in inertia. This point of diminishing returns is a good candidate for the optimal K.</p>"},{"location":"04-core-ml/clustering/#what-is-the-bayesian-information-criterion-bic","title":"What is the Bayesian Information Criterion (BIC)?","text":"<p>For probabilistic models like GMM, the Bayesian Information Criterion (BIC) is a more principled approach. BIC evaluates how well the model fits the data, but it also includes a penalty for complexity (i.e., adding more clusters/parameters). The best model is the one with the lowest BIC score, representing the optimal balance between model fit and simplicity. This helps prevent overfitting, where the model creates too many tiny clusters just to fit the noise in the data.</p>"},{"location":"04-core-ml/clustering/#summary","title":"Summary","text":"<p>Clustering is the process of discovering natural groups within data. It is valuable in many business scenarios, such as separating customers into segments for targeted marketing, organizing thousands of documents by topic, or identifying unusual credit card activity that might be fraud. By finding these groups, teams can stop treating everyone the same and start making focused plans for each segment. The goal is not just to sort data, but to create groups that lead to better, more informed decisions.</p> <p>The goal of grouping is not just to find the patterns that exist, but to find the patterns that matter</p>"},{"location":"04-core-ml/dim_reduction/","title":"Dimensionality Reduction: Seeing High\u2011Dimensional Data More Clearly","text":"<p>Dimensionality reduction simplifies high\u2011dimensional data so it becomes easier to see patterns, train models, and communicate insights. </p> <p>It does this by finding a smaller set of new features that still capture most of the useful information in the original data.</p>"},{"location":"04-core-ml/dim_reduction/#why-dimensionality-reduction-matters","title":"Why Dimensionality Reduction Matters","text":"<p>Many modern datasets have dozens or hundreds of features, which makes them hard to visualize and can slow down or destabilize models. Dimensionality reduction compresses these features into a smaller number of informative directions so that structure becomes visible and models can focus on the most important variation. Typical goals include:</p> <ul> <li>Visualizing data in 2D or 3D.</li> <li>Reducing noise and redundancy.</li> <li>Speeding up training and inference.</li> <li>Creating compact, meaningful features for downstream models.</li> </ul> <p>A helpful mental picture is a cloud of points in 3D that lies near a thin, tilted sheet rather than filling the whole cube. Even though there are three features, the data can be described well using just two coordinates along that sheet. Dimensionality reduction tries to find this kind of lower\u2011dimensional ``surface'' inside a higher\u2011dimensional space and represent each point using its coordinates on that surface.</p> <p></p>"},{"location":"04-core-ml/dim_reduction/#core-idea-important-directions-in-data","title":"Core Idea: Important Directions in Data","text":"<p>At the heart of many dimensionality reduction methods is the idea that some directions in feature space are more informative than others. Along some directions the data varies a lot (high variance), and along others it hardly changes at all.</p> <p>High\u2011variance directions often carry signal or structure.</p> <p>Very low\u2011variance directions often correspond to noise or redundant information.</p> <p>Dimensionality reduction methods look for these important directions and then represent each point using only the coordinates along a subset of them. In practice, this often means keeping a handful of ``strong'' directions and discarding many weak ones that do not change much.</p>"},{"location":"04-core-ml/dim_reduction/#eigendecomposition-special-directions-of-a-matrix","title":"Eigendecomposition: Special Directions of a Matrix","text":"<p>Eigendecomposition applies to certain square matrices, such as covariance matrices built from data, and writes a matrix in terms of eigenvalues and eigenvectors</p> <p>Intuition:</p> <ul> <li>Think of a matrix as a transformation that takes an input vector and outputs another vector.</li> <li>For most directions, this transformation changes both direction and length.</li> <li>For some special directions, called eigenvectors, the transformation only stretches or shrinks the vector without rotating it.</li> <li>The stretch factor along each eigenvector is its eigenvalue.</li> </ul> <p>In data analysis, if you form the covariance matrix of your features, its eigenvectors indicate directions in feature space where variance is highest, and the corresponding eigenvalues tell you how much variance lies along each of those directions. This is exactly the idea behind principal components: find the directions where the data spreads out the most.</p> <p></p>"},{"location":"04-core-ml/dim_reduction/#singular-value-decomposition-svd-the-practical-workhorse","title":"Singular Value Decomposition (SVD): The Practical Workhorse","text":"<p>Singular Value Decomposition (SVD) is a more general factorization that works for any real matrix, not only square ones. If \\(X\\) is your data matrix (rows are samples, columns are features), SVD factors it into three matrices in a way that reveals orthogonal directions and their strengths.</p> <p>Key intuitions:</p> <ul> <li>SVD finds a set of perpendicular directions in feature space (right singular vectors).</li> <li>These directions are ordered from \"most important\" to \"least important\" according to singular values.</li> <li>Using only the top directions and singular values, you can reconstruct the data approximately but still capture most of its structure.</li> </ul> <p>In practice, PCA is often implemented via SVD on the centered data matrix because SVD is numerically stable and efficient for rectangular data. This means that even if you never call \"eigendecomposition\" directly, you are still using the same idea when you run PCA through standard ML libraries.</p> <p></p>"},{"location":"04-core-ml/dim_reduction/#eigendecomposition-vs-svd","title":"Eigendecomposition vs SVD","text":"<p>Eigendecomposition and SVD share a common idea-both identify important directions and their strengths\u2014but they operate on different objects and have different requirements.</p> <p>Eigendecomposition</p> <ul> <li>Works on certain square matrices (for example, symmetric covariance matrices).</li> <li>Yields eigenvalues and eigenvectors of that matrix.</li> <li>Common in theory for understanding PCA via the covariance matrix.</li> </ul> <p>SVD</p> <ul> <li>Works on any \\(m \\times n\\) data matrix.</li> <li>Yields singular values and left/right singular vectors.</li> <li>Common in implementations to compute principal components directly from data.</li> </ul> <p>You can think of eigendecomposition as the theoretical lens and SVD as the practical tool you apply to real datasets. In most real ML code, you rely on SVD under the hood, but conceptually you are still looking for the main directions of variation described by eigen\u2011style reasoning.</p>"},{"location":"04-core-ml/dim_reduction/#principal-components-analysis-pca","title":"Principal Components Analysis (PCA)","text":"<p>PCA is the standard linear method for dimensionality reduction and is built on these linear algebra ideas.</p>"},{"location":"04-core-ml/dim_reduction/#intuition","title":"Intuition","text":"<p>PCA finds new axes in feature space, called principal components, that are:</p> <ul> <li>Linear combinations of the original features.</li> <li>Ordered so that the first component captures as much variance as possible.</li> <li>Mutually orthogonal, with each subsequent component capturing the largest remaining variance. </li> </ul> <p>Geometrically, PCA rotates the coordinate system so that the axes align with the directions where the data spreads out the most. After this rotation, you can drop the less important axes and keep only the first few components, which gives a compressed but informative representation of each data point.</p> <p></p>"},{"location":"04-core-ml/dim_reduction/#what-pca-produces-and-how-its-used","title":"What PCA Produces and How It\u2019s Used","text":"<p>When you run PCA, you get principal component directions, component scores for each sample, and the explained variance for each component. Together, these tell you:</p> <ul> <li>Which directions carry most of the information.</li> <li>How many components you need to keep a chosen fraction (e.g., 90\u201395%) of the total variance.</li> <li>Where each sample lies in this compressed coordinate system. </li> </ul> <p>Typical applications include:</p> <ul> <li>Creating 2D/3D plots of high\u2011dimensional data to visually inspect patterns (e.g., whether classes or clusters separate).</li> <li>Reducing dimensionality before clustering to improve stability and speed.</li> <li>Compressing features for downstream models while retaining most of the signal.</li> <li>Denoising by discarding components with very small variance that mainly capture noise. </li> </ul> <p>PCA has limitations. </p> <p>It only captures linear structure, so it can miss curved manifolds or complex non\u2011linear patterns. </p> <p>It is also sensitive to feature scale, meaning features usually need to be standardized beforehand, and the resulting components can be hard to explain to non\u2011technical stakeholders because each component mixes many original features</p>"},{"location":"04-core-ml/dim_reduction/#tdistributed-stochastic-neighbor-embedding-tsne","title":"t\u2011Distributed Stochastic Neighbor Embedding (t\u2011SNE)","text":"<p>t\u2011SNE is a non\u2011linear dimensionality reduction method designed mainly for visualization in 2D (or sometimes 3D).</p>"},{"location":"04-core-ml/dim_reduction/#intuition_1","title":"Intuition","text":"<p>t\u2011SNE focuses on preserving local neighborhoods rather than global distances:</p> <ul> <li>In the original high\u2011dimensional space, it measures how similar each pair of points is, using probabilities that emphasize close neighbors.</li> <li>It then searches for a 2D or 3D embedding where points that were close remain close, and dissimilar points are free to move apart. </li> </ul> <p>The result is usually a plot where cluster\u2011like structures visually stand out: similar items form tight groups, while dissimilar items move away. This makes t\u2011SNE a powerful tool for visually inspecting embeddings from images, text, or deep models. </p> <p></p>"},{"location":"04-core-ml/dim_reduction/#practical-use-and-caveats","title":"Practical Use and Caveats","text":"<p>t\u2011SNE is especially useful when you want to answer questions like <code>Do similar examples cluster together</code> or <code>Does my embedding model separate classes in some intuitive way?</code> It is popular for checking whether representations learned by neural networks capture meaningful structure.</p> <p>However, t\u2011SNE has important limitations:</p> <ul> <li>It is intended for visualization, not as a general feature generator for production models.</li> <li>It is sensitive to hyperparameters and random initialization, so layouts can differ between runs.</li> <li>It distorts global distances, so the relative positions of distant clusters should not be over\u2011interpreted. </li> </ul> <p>The axes themselves have no direct meaning; the main value is in the relative arrangement of nearby points and the presence of cluster\u2011like groupings.</p>"},{"location":"04-core-ml/dim_reduction/#choosing-and-applying-these-methods","title":"Choosing and Applying These Methods","text":"<p>PCA and t\u2011SNE play complementary roles in ML workflows.</p> <p>Use PCA when you want a linear, interpretable transformation; need lower\u2011dimensional features for modeling or clustering; care about global variance structure; and want something fast, deterministic, and easy to cross\u2011validate. It is often the default choice for tabular or feature\u2011engineered data.</p> <p>Use t\u2011SNE when your primary goal is visualization in 2D or 3D, especially for high\u2011dimensional embeddings where non\u2011linear structure is expected. It is best treated as an exploratory tool: it can reveal cluster\u2011like structure and anomalies that prompt deeper analysis, but the layout should not be treated as a precise geometric map.</p> <p>In practice, a common pattern is:</p> <ol> <li>Scale and clean features.</li> <li>Optionally apply PCA to reduce to a moderate number of dimensions.</li> <li>Run t\u2011SNE on the PCA output to create a 2D map that is easier to interpret and compute. </li> </ol> <p></p>"},{"location":"04-core-ml/dim_reduction/#applications-and-pitfalls","title":"Applications and Pitfalls","text":"<p>Dimensionality reduction is widely used in customer segmentation, document and image analysis, anomaly detection, and any setting where feature spaces are large. Analysts use PCA to compress correlated features before clustering or regression, and they use t\u2011SNE maps to visually inspect whether learned representations capture meaningful similarities.</p> <p>Common pitfalls include applying dimensionality reduction without checking information loss, over\u2011trusting attractive 2D plots (especially from t\u2011SNE), ignoring feature scaling before PCA, and treating t\u2011SNE axes or inter\u2011cluster distances as more meaningful than they really are. Being aware of these issues helps prevent drawing overly strong conclusions from reduced representations.</p>"},{"location":"04-core-ml/dim_reduction/#key-message","title":"Key Message","text":"<p>Dimensionality reduction is about seeing the essence of your data with fewer numbers by focusing on the most informative directions and structures. </p> <p>Start with simple, well\u2011understood methods like PCA for modeling and preprocessing, and use tools like t\u2011SNE as exploratory maps to inspect complex spaces rather than as ground truth.</p>"},{"location":"04-core-ml/lifecycle/","title":"The Machine Learning Lifecycle","text":"<p>This tutorial walks through the complete journey of building, deploying, and maintaining machine learning models. You'll learn the seven essential phases every ML project follows, understand why each phase matters, and see how they connect using house price prediction as a running example.</p> <p>Estimated time: 40 minutes</p>"},{"location":"04-core-ml/lifecycle/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: </p> <p>Building a model in a notebook is easy. Building a model that works reliably in production for months is hard.</p> <p>Most ML projects fail not because of algorithm choice, but because teams skip critical lifecycle phases. A model that achieves <code>95%</code> accuracy on your laptop means nothing if it crashes on production data, degrades after two weeks, or solves the wrong problem. Understanding the full lifecycle prevents these failures.</p> <p>Practical benefits: Following a structured lifecycle helps you catch problems early, build models that actually get deployed, create systems that maintain performance over time, and communicate progress clearly to stakeholders.</p> <p>Professional context: Companies don't hire ML practitioners to build models; they hire them to deliver business value. That requires understanding the complete lifecycle from problem definition through production monitoring. The difference between a data scientist who ships models and one who doesn't usually comes down to lifecycle discipline, not technical skill.</p> <p></p>"},{"location":"04-core-ml/lifecycle/#running-example-house-price-prediction","title":"Running Example: House Price Prediction","text":"<p>Throughout this tutorial, we'll follow a real estate company building a model to estimate house prices. Their goal is helping agents price listings accurately to sell faster. This example illustrates every lifecycle phase with concrete decisions and challenges.</p>"},{"location":"04-core-ml/lifecycle/#the-seven-lifecycle-phases","title":"The Seven Lifecycle Phases","text":""},{"location":"04-core-ml/lifecycle/#phase-1-problem-definition","title":"Phase 1: Problem Definition","text":"<p>Goal: Translate business needs into a clear ML task.</p> <p>What happens: You work with stakeholders to understand the actual problem, define success metrics, and determine if ML is the right solution. This phase sets direction for everything that follows.</p> <p>House price example:</p> <p>The real estate company initially says \"We want AI to help sell houses faster.\" That's too vague. Through discussion, you clarify:</p> <ul> <li>Business goal: Reduce time properties spend on market</li> <li>ML task: Predict accurate listing prices (regression problem)</li> <li>Success metric: Prices within 10% of final sale price</li> <li>Scope: Focus on residential properties in one city initially</li> <li>Data availability: 5 years of historical sales data exists</li> <li>Deployment: Agents will use predictions via mobile app</li> </ul> <p></p> <p>Key decisions:</p> <p>What exactly are you predicting? House prices, not \"help sell faster.\" When is the model successful? Within 10% accuracy, not \"better than current process.\" What data do you have? Historical sales with features (size, location, bedrooms, etc.). How will predictions be used? Agents input property details, receive instant price estimate.</p> <p>Common mistakes: Skipping stakeholder alignment (build wrong thing), vague success metrics (can't measure progress), not checking data availability (discover lack of data too late), ignoring deployment constraints (model too slow for mobile app).</p>"},{"location":"04-core-ml/lifecycle/#phase-2-data-collection","title":"Phase 2: Data Collection","text":"<p>Goal: Gather relevant, quality data for training.</p> <p>What happens: You identify data sources, collect historical examples, ensure data is representative, and understand limitations. Data quality here determines model quality later.</p> <p>House price example:</p> <p>The company provides three data sources:</p> <p>MLS database: Past sales with prices, dates, addresses, square footage, bedrooms, bathrooms. Contains 50,000 transactions from 2019-2024.</p> <p>Property tax records: Lot size, year built, property type, assessed values. More detailed but requires matching addresses.</p> <p>Neighborhood data: School ratings, crime statistics, walkability scores. Aggregated by zip code.</p> <p>You discover issues: <code>5%</code> of records have missing square footage, luxury homes over <code>$2M</code> are rare (only <code>200</code> examples), and recent renovations aren't captured anywhere.</p> <p></p> <p>Key decisions:</p> <p>Use all three sources and merge by address. Accept that luxury home predictions will be less accurate. Flag properties with missing data for manual review. Collect renovation data going forward but launch without it initially.</p> <p>Data collection challenges: Incomplete records (handle missing values), biased samples (luxury homes underrepresented), integration complexity (merge three databases), privacy concerns (ensure compliance with regulations).</p>"},{"location":"04-core-ml/lifecycle/#phase-3-data-preparation","title":"Phase 3: Data Preparation","text":"<p>Goal: Clean, transform, and organize data into model-ready format.</p> <p>What happens: You handle missing values, remove duplicates, fix inconsistencies, encode categorical variables, scale features, and split data for training/validation/testing.</p> <p> House price example:</p> <p>Cleaning: Remove 500 duplicate listings, fix address inconsistencies (\"St.\" vs \"Street\"), drop 12 impossible values (negative square footage, sale prices of $1).</p> <p>Handling missing data: For missing square footage, impute using median by property type and zip code. For missing school ratings, create \"unknown\" category rather than dropping rows.</p> <p>Feature engineering: Create \"price per square foot\" feature, \"age of house\" from year built, \"renovation indicator\" if assessed value jumped significantly.</p> <p>Encoding: Convert property type (single-family, condo, townhouse) to one-hot encoding. Map school ratings (A-F) to numerical scale (5-1).</p> <p>Scaling: Normalize square footage (500-5000) and lot size (1000-50000) so they contribute equally to distance-based algorithms.</p> <p>Splitting: <code>60%</code> training (30,000 houses), 20% validation (10,000), 20% test (10,000). Ensure split is random across time periods to avoid temporal bias.</p> <p>Key insight: After preparation, you have clean data with 45,000 usable examples and 23 features. This took 40% of project time but ensures model quality.</p>"},{"location":"04-core-ml/lifecycle/#phase-4-model-training","title":"Phase 4: Model Training","text":"<p>Goal: Train algorithms to learn patterns in prepared data.</p> <p>What happens: You select candidate algorithms, train them on training data, tune hyperparameters using validation data, and compare performance. This is where models actually learn.</p> <p></p> <p>House price example:</p> <p>Start simple: Begin with linear regression as baseline. It's interpretable and fast. For example, achieves 12% mean error on validation set.</p> <p>Try more complex: Train decision tree, random forest, and gradient boosting models. For example, random forest achieves 8% mean error; better than baseline.</p> <p>Hyperparameter tuning: For random forest, try different numbers of trees (50, 100, 200), maximum depth (10, 20, 30), and minimum samples per leaf. Example: best configuration: 150 trees, depth 25, min samples 5. Improves to 7.5% error.</p> <p>Feature importance: Model reveals square footage, e.g., (35% importance), location/zip code (28%), and age (15%) drive prices most. Number of bathrooms matters less than expected (3%).</p> <p>Training process: Each model trains on 30,000 houses, validates on 10,000, adjusts parameters, and repeats until performance plateaus. Random forest takes 15 minutes to train; linear regression takes 2 seconds.</p> <p>Key decisions: Choose random forest despite longer training time because 7.5% error meets the 10% target, and feature importance helps agents understand predictions.</p>"},{"location":"04-core-ml/lifecycle/#phase-5-model-evaluation","title":"Phase 5: Model Evaluation","text":"<p>Goal: Rigorously test model on unseen data to verify real-world performance.</p> <p>What happens: You test the trained model on held-out test data, analyze errors, check for biases, and ensure predictions are reliable before deployment.</p> <p></p> <p>House price example:</p> <p>Test set performance: Evaluate random forest on 10,000 never-before-seen houses. Achieves 8.2% mean error; slightly worse than validation but within target.</p> <p>Error analysis: Group predictions by price range. Model excels on \\(200K-\\)500K homes (6% error) but struggles on luxury $1M+ properties (15% error). This matches expectations from data collection phase (few luxury examples).</p> <p>Bias check: Compare errors across neighborhoods. Model slightly underpredicts in rapidly gentrifying areas (prices rising faster than historical data suggests). Overpredicts in declining areas.</p> <p>Edge cases: Test on unusual properties (houseboats, historic homes, properties with commercial zoning). Errors range 20-40%; too high for reliable predictions.</p> <p>Business validation: Show predictions to experienced agents. They confirm estimates make sense for typical properties but flag that model misses recent market trends (interest rate changes in 2024).</p> <p>Decision: Deploy model for standard residential properties (\\(100K-\\)800K). Flag luxury homes and unusual properties for manual agent review. Plan quarterly retraining to capture market trends.</p>"},{"location":"04-core-ml/lifecycle/#phase-6-model-deployment","title":"Phase 6: Model Deployment","text":"<p>Goal: Put model into production where it serves real users.</p> <p>What happens: You package the model, integrate with existing systems, build monitoring infrastructure, and ensure it runs reliably at scale.</p> <p></p> <p>House price example:</p> <p>Deployment architecture: Host model on cloud server as REST API. Mobile app sends property features, API returns price prediction in under 500ms.</p> <p>Integration: Connect to company's property listing system. When agents create new listing, app pre-populates suggested price from model. Agents can accept, adjust, or override prediction.</p> <p>Performance optimization: Original random forest with 150 trees is too slow (2 seconds per prediction). Reduce to 75 trees with minimal accuracy loss (8.3% error vs 8.2%). Now predicts in 300ms; meets requirement.</p> <p>Fallback handling: If API is down or property has missing critical features, display message: \"Price estimate unavailable. Use comparative market analysis instead.\"</p> <p>User interface: Show prediction range ($245K - \\(265K) rather than exact number (\\)255K) to convey uncertainty. Display confidence score and flag when model is less certain (unusual properties, luxury homes).</p> <p>Rollout strategy: Launch to 50 agents in one office as pilot. Collect feedback for two weeks before company-wide deployment.</p> <p>Key insight: Deployment is engineering-heavy. Model accuracy matters, but so does latency, reliability, user experience, and graceful error handling.</p>"},{"location":"04-core-ml/lifecycle/#phase-7-monitoring-maintenance","title":"Phase 7: Monitoring &amp; Maintenance","text":"<p>Goal: Track model performance over time and maintain accuracy in production.</p> <p>What happens: You monitor predictions, detect performance degradation, investigate issues, retrain when necessary, and continuously improve the system.</p> <p></p> <p>House price example:</p> <p>Monitoring dashboard: Track key metrics daily:</p> <ul> <li>Prediction volume (how many estimates per day)</li> <li>Average prediction confidence</li> <li>API latency and error rates</li> <li>Comparison between predicted prices and eventual sale prices (when available)</li> </ul> <p>Performance tracking: Three months after deployment, you notice prediction accuracy has degraded to 11% error; exceeding the 10% target.</p> <p>Root cause analysis: Housing market shifted significantly. Interest rate changes made homes less affordable. Model trained on 2019-2024 data doesn't capture this sudden change. Historical patterns (2% annual appreciation) don't match current reality (flat or declining prices).</p> <p>This is data drift: The relationship between features and prices changed. Square footage still matters, but buyers now prioritize lower price over size.</p> <p>Retraining: Collect last six months of sales data (5,000 new examples including market shift). Retrain random forest on updated dataset. New model achieves 8.5% error on recent data.</p> <p>Deployment update: Replace production model with retrained version. Test thoroughly before switching. Monitor closely for first week to catch issues.</p> <p>Continuous improvement: Based on agent feedback, add new features (days on market for comparable properties, mortgage rate at time of listing). Quarterly retraining now scheduled automatically.</p> <p>Alerting: Set up automated alerts if error rate exceeds 12%, prediction volume drops 50% (API issues), or latency exceeds 1 second.</p> <p>Key insight: Models aren't fire-and-forget. Production monitoring and regular retraining are essential for long-term success. Budget 20-30% of project time for ongoing maintenance.</p>"},{"location":"04-core-ml/lifecycle/#the-lifecycle-is-iterative","title":"The Lifecycle Is Iterative","text":"<p>The seven phases aren't strictly linear. You'll loop back frequently:</p> <p>Evaluation reveals issues \u2192 return to data collection. Luxury home predictions are poor because you lack training examples. Collect more data or narrow scope.</p> <p>Monitoring detects drift \u2192 return to training. Market changes require retraining with recent data.</p> <p>Deployment uncovers problems \u2192 return to preparation. Agents report predictions fail for properties with multiple units. Add feature to handle this case.</p> <p>Feedback suggests improvements \u2192 return to problem definition. Agents want price ranges, not point estimates. Refine requirements and retrain probabilistic model.</p> <p>Most successful ML projects iterate through phases 3-7 multiple times before achieving production stability. Expect the cycle, don't fight it.</p>"},{"location":"04-core-ml/lifecycle/#common-pitfalls","title":"Common Pitfalls","text":"<p>Skipping problem definition: Teams jump straight to modeling without clear goals. Result: technically impressive model solves wrong problem.</p> <p>Insufficient data preparation: Rush through cleaning to start modeling faster. Result: garbage in, garbage out\u2014poor model quality.</p> <p>Overfitting to validation set: Tune hyperparameters by checking validation performance repeatedly. Result: model appears accurate but fails on test data and production.</p> <p>Ignoring deployment constraints: Build complex model on powerful machine, deploy to mobile device. Result: too slow, users abandon app.</p> <p>No monitoring plan: Deploy and forget. Result: model degrades silently, users lose trust, business impact declines.</p>"},{"location":"04-core-ml/lifecycle/#best-practices","title":"Best Practices","text":"<p>Start with problem, not technique. Don't decide \"we'll use deep learning\" before understanding the problem. Let requirements guide algorithm choice.</p> <p>Invest time in data quality. Clean, representative data beats fancy algorithms on messy data every time. Make phase 3 robust.</p> <p>Keep a human in the loop. Agents can override model predictions. Humans handle edge cases models miss.</p> <p>Version everything. Track which data version, which code version, and which model version is in production. Enables debugging and rollback.</p> <p>Automate repetitive phases. Retraining happens quarterly. Automate data collection, preparation, training, and deployment to reduce errors and save time.</p> <p>Measure business impact, not just accuracy. Track whether agents using model predictions sell homes faster. That's the real success metric, not MAE or R\u00b2.</p>"},{"location":"04-core-ml/lifecycle/#quick-reference-lifecycle-phases","title":"Quick Reference: Lifecycle Phases","text":"Phase Key Question House Price Example Deliverable 1. Problem Definition What are we solving? Predict listing prices within 10% accuracy Problem statement, success metrics 2. Data Collection What data exists? 50K sales records from 3 sources Raw dataset with known limitations 3. Data Preparation How to clean and organize? Handle missing values, engineer features, split data Clean train/val/test sets 4. Model Training Which algorithm works best? Random forest with 75 trees, 8.2% error Trained model with tuned parameters 5. Model Evaluation Does it work on new data? 8.2% test error, struggles on luxury homes Performance report, error analysis 6. Model Deployment How to serve predictions? REST API, mobile app integration, 300ms latency Production system serving real users 7. Monitoring &amp; Maintenance Is it still working? Quarterly retraining, drift detection, 8.5% maintained Monitoring dashboard, update schedule"},{"location":"04-core-ml/lifecycle/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You understand the seven phases of the ML lifecycle and how they connect, see why each phase matters through the house price prediction example, recognize that lifecycle is iterative with feedback loops, and know common pitfalls and best practices for each phase.</p> <p>Critical insights:</p> <ul> <li>Problem definition sets direction: Everything else depends on clearly framed goals</li> <li>Data quality determines model quality: Invest heavily in collection and preparation</li> <li>Deployment and monitoring are as important as training: Models live in production, not notebooks</li> <li>Iteration is normal: Expect to loop back as you learn and improve</li> </ul> <p>External resources:</p> <ul> <li>Google's Rules of Machine Learning - Practical best practices for production ML</li> <li>AWS ML Lifecycle Guide - Enterprise-scale lifecycle management</li> <li>Made With ML - End-to-end tutorials covering full lifecycle</li> </ul> <p>Remember: A model in production delivering business value beats a perfect model in a notebook. Master the full lifecycle, and you'll ship models that matter instead of experiments that never deploy.</p>"},{"location":"04-core-ml/ml-categories/","title":"Machine Learning Categories","text":"<p>This tutorial introduces the three fundamental types of machine learning and helps you understand which approach fits different problems. You'll learn how supervised, unsupervised, and reinforcement learning differ in their learning methods, when to apply each, and how modern generative AI fits into this framework.</p> <p>Estimated time: 35 minutes</p>"},{"location":"04-core-ml/ml-categories/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: </p> <p>Not all learning problems are the same. Choosing the wrong ML category wastes time, resources, and produces poor results.</p> <p>Different problems require different learning approaches. Predicting house prices with known historical sales needs a different strategy than grouping customers into segments when no predefined categories exist, or teaching a robot to walk through trial and error. Understanding ML categories helps you match techniques to problems correctly.</p> <p>Practical benefits: Knowing which category fits your problem saves weeks of experimentation. You'll avoid training supervised models on unlabeled data, clustering when you need predictions, or using reinforcement learning for simple classification tasks.</p> <p>Professional context: Job interviews and project planning require explaining why you chose a particular ML approach. Teams respect practitioners who can justify category selection with clear reasoning. The most common mistake beginners make is forcing every problem into supervised learning when other categories fit better.</p> <p></p>"},{"location":"04-core-ml/ml-categories/#the-three-main-categories","title":"The Three Main Categories","text":"<p>Machine learning splits into three fundamental approaches based on what data you have and how the model learns:</p> <p>Supervised learning uses labeled examples where correct answers are provided. The model learns to map inputs to outputs by studying these pairs.</p> <p>Unsupervised learning finds hidden patterns in unlabeled data. You provide input only; no correct answers. The algorithm discovers structure on its own.</p> <p>Reinforcement learning learns through trial and error, receiving rewards for good actions and penalties for bad ones. The model discovers optimal strategies through experience.</p> <p>Each category solves fundamentally different types of problems. Let's explore when and why to use each.</p>"},{"location":"04-core-ml/ml-categories/#supervised-learning","title":"Supervised Learning","text":""},{"location":"04-core-ml/ml-categories/#the-learning-approach","title":"The Learning Approach","text":"<p>Supervised learning is like learning with a teacher who provides correct answers. You show the model examples with known outcomes, and it learns patterns that connect inputs to outputs. When new data arrives, the model applies learned patterns to make predictions.</p> <p></p> <p>Think of it as studying for an exam with answer keys. You review problems and their solutions, identify patterns in how solutions are reached, then apply those patterns to new problems. The more diverse your practice problems, the better you perform on the real test.</p> <p>The process: Feed the model thousands of labeled examples. \"This email is spam.\" \"This house sold for $250,000.\" \"This tumor is malignant.\" The model identifies relationships between features (email content, house characteristics, tumor measurements) and labels (spam/not spam, price, diagnosis). Once trained, it predicts labels for new, unlabeled examples.</p>"},{"location":"04-core-ml/ml-categories/#when-to-use-supervised-learning","title":"When to Use Supervised Learning","text":"<p>Supervised learning fits problems where you have historical data with known outcomes. Past sales with final prices. Previous loan applications with approval/default results. Medical images with expert diagnoses. Emails already labeled as spam or legitimate.</p> <p>The key requirement is labeled training data. </p> <p>If creating labels is impossible or prohibitively expensive, supervised learning won't work. But when labels exist, supervised methods typically deliver the most accurate predictions for well-defined tasks.</p> <p>Common applications:</p> <p>Predicting continuous values like house prices, stock prices, customer lifetime value, or demand forecasts uses regression. You're estimating a number that can fall anywhere on a continuous scale.</p> <p>Categorizing items into discrete classes like spam detection, fraud identification, disease diagnosis, or sentiment analysis uses classification. You're assigning inputs to predefined categories.</p> <p></p>"},{"location":"04-core-ml/ml-categories/#strengths-and-limitations","title":"Strengths and Limitations","text":"<p>Supervised learning's greatest strength is measurable accuracy. You can test predictions against known labels, calculate precise error rates, and confidently deploy models that meet performance thresholds. Clear success metrics make supervised learning attractive for business applications where outcomes matter.</p> <p>The limitation is simple: you need labeled data. Creating labels requires human expertise, time, and often significant expense. Medical diagnoses need doctors. Fraud labels need investigators. Quality labels need domain experts. When labeling costs exceed the value of predictions, supervised learning becomes impractical.</p> <p>Additionally, supervised models only learn what exists in training data. If your historical data lacks important scenarios, the model will fail when those scenarios appear in production. A fraud detector trained only on credit card fraud won't recognize wire transfer fraud. The model is only as good as its training examples.</p>"},{"location":"04-core-ml/ml-categories/#unsupervised-learning","title":"Unsupervised Learning","text":""},{"location":"04-core-ml/ml-categories/#the-learning-approach_1","title":"The Learning Approach","text":"<p>Unsupervised learning is like exploring without a map. No teacher provides correct answers. No labels indicate what patterns matter. The algorithm examines data structure, identifies similarities and differences, and discovers organization that wasn't explicitly programmed.</p> <p></p> <p>Think of organizing a messy photo collection without predefined albums. You might group beach photos together, family gatherings in another cluster, and vacation shots separately; not because someone told you these categories exist, but because the photos naturally share characteristics. Unsupervised learning discovers these natural groupings.</p> <p>The process: Provide unlabeled data-customer transactions, document text, network traffic patterns, or sensor readings. The algorithm analyzes relationships, measures similarities, and reveals structure. You might discover five distinct customer segments, three types of network behavior, or two document topics. The algorithm finds patterns; you interpret their meaning.</p>"},{"location":"04-core-ml/ml-categories/#when-to-use-unsupervised-learning","title":"When to Use Unsupervised Learning","text":"<p>Unsupervised learning shines when labels don't exist and creating them is impractical. You have millions of customers but no predefined segments. Thousands of documents with no topic labels. Network traffic with no clear \"normal\" baseline.</p> <p>It's also valuable for exploratory analysis. Before building supervised models, unsupervised methods reveal data structure, identify anomalies, and suggest useful features. Understanding natural groupings helps you design better labels for supervised learning later.</p> <p>Common applications:</p> <p>Customer segmentation groups buyers by behavior, revealing marketing opportunities. You don't label customers as \"budget shoppers\" beforehand; clustering discovers these groups exist.</p> <p>Anomaly detection identifies unusual patterns in network traffic, financial transactions, or sensor data. The model learns \"normal\" behavior, then flags deviations.</p> <p>Dimensionality reduction compresses hundreds of features into a few principal components, speeding up other algorithms or enabling visualization.</p> <p>Document clustering organizes articles, emails, or support tickets by topic without manual categorization.</p>"},{"location":"04-core-ml/ml-categories/#strengths-and-limitations_1","title":"Strengths and Limitations","text":"<p>Unsupervised learning's power lies in discovery. It finds patterns humans didn't know existed, structures data without expensive labeling, and scales to massive datasets where manual annotation is impossible.</p> <p>The challenge is evaluation. Without labels, measuring success is subjective. Did the algorithm find five customer segments because five exist, or because you told it to find five? Are those clusters meaningful for your business, or just mathematical artifacts? Interpretation requires domain expertise and business validation.</p> <p>Unsupervised methods also produce descriptive insights, not predictions. Clustering tells you groups exist but doesn't predict which group a new customer joins (though you can build classifiers afterward). It's exploratory, not predictive\u2014a different tool for different goals.</p>"},{"location":"04-core-ml/ml-categories/#reinforcement-learning","title":"Reinforcement Learning","text":""},{"location":"04-core-ml/ml-categories/#the-learning-approach_2","title":"The Learning Approach","text":"<p>Reinforcement learning is learning through consequences. An agent takes actions in an environment, receives feedback on whether those actions were good or bad, and gradually learns which strategies maximize rewards. No labeled examples exist\u2014just trial, error, and feedback.</p> <p>Think of teaching a dog new tricks. You don't show the dog labeled examples of \"sit\" vs \"stay.\" You reward correct behavior (treats for sitting) and ignore or discourage wrong behavior. Over thousands of attempts, the dog learns which actions earn rewards. Reinforcement learning follows the same principle.</p> <p></p> <p>The process: An agent explores its environment, trying different actions. Each action produces a reward (positive or negative) and a new state. The agent learns a policy; a strategy mapping situations to actions; that maximizes cumulative reward over time. Early attempts are random; over millions of trials, optimal strategies emerge.</p>"},{"location":"04-core-ml/ml-categories/#when-to-use-reinforcement-learning","title":"When to Use Reinforcement Learning","text":"<p>Reinforcement learning fits problems where actions have delayed consequences and you optimize long-term outcomes. Playing games (chess, Go, video games). Controlling robots. Optimizing trading strategies. Managing resources. Personalizing content recommendations.</p> <p>The key characteristic is sequential decision-making. Current actions affect future options. Rewards might not appear immediately; a chess move looks neutral but sets up a winning position ten moves later. Reinforcement learning discovers these long-term strategies.</p> <p>Common applications:</p> <p>Game playing produced DeepMind's AlphaGo and OpenAI's Dota 2 bots. Agents play millions of games against themselves, learning strategies that defeat human champions.</p> <p>Robotics teaches machines to walk, grasp objects, or navigate environments through simulated practice.</p> <p>Recommendation systems learn which content keeps users engaged, treating engagement as reward.</p> <p>Resource optimization manages energy grids, traffic lights, or data center cooling by learning policies that minimize costs.</p>"},{"location":"04-core-ml/ml-categories/#strengths-and-limitations_2","title":"Strengths and Limitations","text":"<p>Reinforcement learning excels at complex, sequential decisions where the optimal action depends on context and future implications. It discovers non-obvious strategies humans wouldn't program explicitly. Given enough exploration, RL finds solutions that surprise experts.</p> <p>The limitations are significant. Training requires millions of trials\u2014feasible in simulation but expensive in real hardware. Reward design is tricky; poorly specified rewards produce unintended behavior (optimizing the metric instead of the goal). Sample efficiency is low compared to supervised learning. Debugging is difficult when emergent behavior appears after thousands of episodes.</p> <p>Reinforcement learning solves specific problem classes exceptionally well, but it's overkill for most business applications. If you have labeled data, supervised learning trains faster with less data. Use RL when sequential decisions and delayed rewards make other approaches impossible.</p> <p></p>"},{"location":"04-core-ml/ml-categories/#generative-ai-a-modern-application","title":"Generative AI: A Modern Application","text":""},{"location":"04-core-ml/ml-categories/#where-it-fits","title":"Where It Fits","text":"<p>Generative AI isn't a fourth category; it's a type of model that can use any of the three learning approaches. Generative models learn to create new content (text, images, audio, video) similar to their training data. They've revolutionized creative applications and sparked massive industry interest.</p> <p>Generative AI uses supervised learning when trained on input-output pairs. Language models learn from text pairs (prompt \u2192 completion). Image generators learn from text-image pairs (caption \u2192 picture). This is supervised learning applied to generation tasks.</p> <p>Generative AI uses unsupervised learning when discovering patterns in unlabeled data. Early image generators trained on millions of unlabeled photos, learning to generate realistic faces without labels. Variational autoencoders learn data structure unsupervised, then generate new samples.</p> <p>Generative AI uses reinforcement learning for fine-tuning. ChatGPT uses reinforcement learning from human feedback (RLHF), treating helpful responses as rewards. This aligns model behavior with human preferences through RL optimization.</p>"},{"location":"04-core-ml/ml-categories/#why-it-matters-now","title":"Why It Matters Now","text":"<p>Generative AI represents the latest frontier in ML applications, but it builds on foundational categories you're learning here. Models like GPT, DALL-E, and Stable Diffusion combine supervised pre-training, unsupervised pattern discovery, and reinforcement fine-tuning. Understanding the three core categories helps you grasp how these systems actually work beneath the hype.</p> <p>The principles remain constant: supervised learning for labeled data, unsupervised for pattern discovery, reinforcement for sequential optimization. Generative AI just applies these principles to create content instead of classifying it or predicting numbers. The underlying mathematics and training processes still follow the categories we've covered.</p>"},{"location":"04-core-ml/ml-categories/#choosing-the-right-category","title":"Choosing the Right Category","text":""},{"location":"04-core-ml/ml-categories/#decision-framework","title":"Decision Framework","text":"<p>Start with your data and goal. Do you have labels? Do you need predictions or pattern discovery? Are you optimizing sequential decisions?</p> <p>If you have labeled data and need predictions, use supervised learning. This covers most business applications: forecasting sales, detecting fraud, diagnosing conditions, classifying documents, estimating prices.</p> <p>If you have unlabeled data and want to discover structure, use unsupervised learning. Segment customers, find anomalies, reduce dimensions, organize content, or explore data before labeling.</p> <p>If you're optimizing sequential decisions with delayed rewards, use reinforcement learning. Control systems, game playing, robotics, resource management, or adaptive systems that learn from interaction.</p> <p>Most real-world ML projects use supervised learning because businesses have historical data with outcomes. Unsupervised learning supports exploration and feature engineering. Reinforcement learning handles specialized sequential optimization problems. Understanding all three helps you choose correctly and combine them when appropriate.</p> <p></p>"},{"location":"04-core-ml/ml-categories/#best-practices","title":"Best Practices","text":"<p>Match the category to the problem, not the hype. Reinforcement learning sounds impressive but wastes resources if supervised learning solves your problem faster with less data.</p> <p>Start simple within categories. Supervised learning offers dozens of algorithms. Begin with linear regression or decision trees before neural networks. Unsupervised learning has k-means before complex clustering. Simple baselines establish performance targets.</p> <p>Consider combinations. Use unsupervised learning to discover customer segments, then train supervised classifiers to predict segment membership for new customers. Explore with unsupervised methods, productionize with supervised models.</p> <p>Remember the data requirement. Supervised needs labels (expensive). Unsupervised needs volume (cheap data, expensive interpretation). Reinforcement needs millions of trials (simulation or patience).</p>"},{"location":"04-core-ml/ml-categories/#quick-reference-category-comparison","title":"Quick Reference: Category Comparison","text":"Category Data Type Goal Example Evaluation Supervised Labeled (input + correct answer) Predict outcomes House price prediction Compare to true labels Unsupervised Unlabeled (input only) Find patterns Customer segmentation Domain expert validation Reinforcement Sequential rewards/penalties Optimize decisions Game playing Cumulative reward"},{"location":"04-core-ml/ml-categories/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You understand the three fundamental ML categories and their learning approaches, know when to apply supervised, unsupervised, or reinforcement learning, recognize that generative AI builds on these foundational categories, and have a decision framework for choosing the right approach.</p> <p>Critical insights:</p> <ul> <li>Categories depend on data: Labels enable supervised learning, unlabeled data requires unsupervised, sequential rewards drive reinforcement</li> <li>Most business problems use supervised learning: Historical data with outcomes is common</li> <li>Generative AI isn't a separate category: It applies the three core approaches to content generation</li> <li>Start with the simplest category that works: Don't use complex approaches when simple ones suffice</li> </ul> <p>External resources:</p> <ul> <li>Andrew Ng's Machine Learning Course - Foundational coverage of all categories</li> <li>Scikit-learn User Guide - Practical supervised and unsupervised learning</li> </ul> <p>Remember: The three categories represent fundamentally different learning approaches. Master when to use each, and you'll choose the right tool for every ML problem you encounter.</p>"},{"location":"04-core-ml/ml-intro/","title":"Introduction to Machine Learning","text":"<p>This tutorial introduces machine learning (ML) fundamentals and helps you understand what ML is, when to use it, and the core concepts that power every ML system. You'll learn how ML differs from traditional programming and develop intuition for when ML makes sense.</p> <p>Estimated time: 20 minutes</p>"},{"location":"04-core-ml/ml-intro/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: </p> <p>Traditional programming requires explicit instructions for every scenario. Machine learning learns patterns from examples instead.</p> <p></p> <p>Programming falls short when rules are too complex to code explicitly (recognizing faces in photos), when rules change frequently (detecting new fraud patterns), or when you need personalization at scale (recommending products to millions of users with different preferences).</p> <p>Practical benefits: Understanding ML fundamentals helps you identify which problems benefit from ML, avoid misapplying ML to simple rule-based problems, and communicate effectively with data scientists and engineers.</p> <p>Professional context: ML drives modern applications from search engines to voice assistants to fraud detection. Companies hiring data professionals expect fluency in ML concepts even for non-modeling roles. Knowing when ML applies (and when simpler approaches work better) separates effective practitioners from those who chase buzzwords.</p>"},{"location":"04-core-ml/ml-intro/#core-concepts","title":"Core Concepts","text":""},{"location":"04-core-ml/ml-intro/#what-is-machine-learning","title":"What is Machine Learning?","text":"<p>Machine Learning is the science of programming computers to learn patterns from data without being explicitly programmed for specific tasks.</p> <p>Traditional programming approach:</p> <p><pre><code>Rules + Data \u2192 Output\n</code></pre> Example: Calculate shipping cost</p> <ul> <li>Rule: \"If weight &gt; 5kg, charge $10 + $2 per kg over 5\"</li> <li>Data: Package weighs 7kg</li> <li>Output: $14</li> </ul> <p>Machine learning approach:</p> <p><pre><code>Data + Desired Output \u2192 Model (learned rules)\n</code></pre> Example: Predict shipping cost</p> <ul> <li>Data: 10,000 past shipments (weight, distance, cost)</li> <li>Desired output: Actual costs paid</li> <li>Model: Learns patterns to predict cost for new packages</li> </ul> <p>ML discovers patterns you didn't know existed. You provide examples, not instructions.</p>"},{"location":"04-core-ml/ml-intro/#ml-vs-ai-vs-deep-learning","title":"ML vs AI vs Deep Learning","text":"<p>These terms are often confused. Here's the relationship:</p> <p>Artificial Intelligence (AI): The broadest term. Any technique making computers behave intelligently: includes rule-based systems, ML, robotics, expert systems.</p> <p>Machine Learning (ML): A subset of AI. Systems that learn from data without explicit programming. Includes decision trees, regression, clustering, neural networks.</p> <p>Deep Learning (DL): A subset of ML. Uses neural networks with many layers (hence \"deep\"). Excels at image recognition, natural language processing, game playing.</p> <p></p> <p>This part of our training focuses on ML: You'll master foundational algorithms before exploring deep learning.</p>"},{"location":"04-core-ml/ml-intro/#key-ml-terminology","title":"Key ML Terminology","text":"<p>Features (Input Variables): The attributes used to make predictions. In house price prediction: square footage, bedrooms, location, age.</p> <p>Labels (Target Variable): What you're trying to predict. In house price prediction: the sale price.</p> <p>Model: The mathematical representation that maps features to labels. Think of it as the \"learned rules.\"</p> <p>Training: The process of feeding data to an algorithm so it learns patterns. The model adjusts internal parameters to minimize prediction errors.</p> <p>Prediction (Inference): Using a trained model to make predictions on new, unseen data.</p> <p></p> <p>Example:</p> <ul> <li>Features: [2000 sq ft, 3 bedrooms, Tirana, 5 years old]</li> <li>Label: $250,000 (training data only)</li> <li>Training: Model learns relationship between features and prices</li> <li>Prediction: Given new house [1500 sq ft, 2 bedrooms, Durr\u00ebs, 10 years old], model predicts price</li> </ul>"},{"location":"04-core-ml/ml-intro/#how-models-learn","title":"How Models Learn","text":"<p>Step 1: Initialize - Model starts with random parameters (wild guesses)</p> <p>Step 2: Make predictions - Use current parameters to predict labels for training data</p> <p>Step 3: Calculate error - Compare predictions to actual labels, measure how wrong they are</p> <p>Step 4: Adjust parameters - Update parameters to reduce error (the \"learning\" part)</p> <p>Step 5: Repeat - Loop through steps 2-4 thousands of times until error is minimized</p> <p></p> <p>Real-world analogy: Learning to shoot basketball free throws</p> <ul> <li>Initialize: First shot goes nowhere near basket (random)</li> <li>Make prediction: Take a shot with current technique</li> <li>Calculate error: Measure distance from basket</li> <li>Adjust: Modify arm angle, force, release point</li> <li>Repeat: Practice until shots consistently go in</li> </ul> <p>The model \"practices\" on training data until it gets good at predictions.</p>"},{"location":"04-core-ml/ml-intro/#when-to-use-machine-learning","title":"When to Use Machine Learning","text":"<p>ML makes sense when:</p> <p>Complexity defeats explicit rules: Face recognition requires identifying patterns across millions of pixel combinations\u2014impossible to code manually.</p> <p>Patterns exist but are unclear: Customer churn depends on dozens of factors interacting in non-obvious ways. ML finds patterns humans miss.</p> <p>Problems require personalization: Recommending movies to 100 million users means learning individual preferences\u2014rule-based systems can't scale.</p> <p>Rules change over time: Spam patterns evolve daily. ML models retrain automatically as new examples arrive.</p> <p>Data is abundant: ML needs examples to learn. More data generally means better performance.</p>"},{"location":"04-core-ml/ml-intro/#when-not-to-use-machine-learning","title":"When NOT to Use Machine Learning","text":"<p>Avoid ML when:</p> <p>Simple rules work: Calculating tax based on income brackets doesn't need ML. Use if-else statements.</p> <p>Data is scarce: Training a medical diagnosis model with 50 patient records will fail. ML needs volume.</p> <p>Interpretability is critical: Banking regulators require explainable decisions. Complex models create \"black boxes\" that can't justify predictions.</p> <p>Cost exceeds benefit: ML infrastructure (data collection, training, deployment, monitoring) has overhead. Sometimes Excel suffices.</p> <p>Rules are fixed and known: Validating email format follows clear patterns (regex). No learning needed.</p> <p>Real-time requirements are strict: If predictions must happen in microseconds and consistency is critical, rule-based systems often win.</p> <p>Use the simplest approach that solves your problem. ML is powerful but not always necessary.</p>"},{"location":"04-core-ml/ml-intro/#ml-in-practice-real-world-examples","title":"ML in Practice: Real-World Examples","text":""},{"location":"04-core-ml/ml-intro/#email-spam-detection","title":"Email Spam Detection","text":"<p>Traditional approach: Create rules (if email contains \"free money\" \u2192 spam) Problem: Spammers adapt; rules become outdated daily</p> <p>ML approach: Train model on labeled emails (spam/not spam) Advantage: Model learns evolving patterns; retrains automatically</p>"},{"location":"04-core-ml/ml-intro/#product-recommendations","title":"Product Recommendations","text":"<p>Traditional approach: \"Customers who bought X also bought Y\" (simple rules) Problem: Doesn't account for individual preferences, context, trends</p> <p>ML approach: Learn from millions of user interactions Advantage: Personalized to each user; improves over time</p>"},{"location":"04-core-ml/ml-intro/#medical-diagnosis","title":"Medical Diagnosis","text":"<p>Traditional approach: Expert systems with encoded medical knowledge Problem: Can't capture all edge cases; requires manual updates</p> <p>ML approach: Learn from thousands of patient records and outcomes Advantage: Finds subtle patterns doctors might miss; improves with more data</p>"},{"location":"04-core-ml/ml-intro/#credit-scoring","title":"Credit Scoring","text":"<p>Traditional approach: Fixed formula (income \u00d7 0.4 + assets \u00d7 0.3 - debt \u00d7 0.3) Problem: Misses non-linear relationships; treats everyone equally</p> <p>ML approach: Learn complex patterns from historical loan performance Advantage: More accurate risk assessment; adapts to economic changes</p>"},{"location":"04-core-ml/ml-intro/#common-misconceptions","title":"Common Misconceptions","text":""},{"location":"04-core-ml/ml-intro/#more-data-always-means-better-models","title":"\"More data always means better models\"","text":"<p>Reality: Quality matters more than quantity. </p> <p>Ten thousand labeled examples from representative samples outperform one million biased, mislabeled examples. More data helps only if it's relevant, accurate, and diverse.</p>"},{"location":"04-core-ml/ml-intro/#complex-models-always-beat-simple-ones","title":"\"Complex models always beat simple ones\"","text":"<p>Reality: Simple models often win in practice.</p> <p>Linear regression frequently outperforms neural networks when you have limited data, interpretability requirements, or well-understood relationships. Start simple, add complexity only if needed.</p>"},{"location":"04-core-ml/ml-intro/#ml-can-find-patterns-in-any-data","title":"\"ML can find patterns in any data\"","text":"<p>Reality: Models learn only what exists in training data.</p> <p>If your training data excludes important scenarios (rural customers, extreme weather, economic recessions), models fail when those scenarios appear in production. No algorithm fixes bad data.</p>"},{"location":"04-core-ml/ml-intro/#ml-replaces-domain-expertise","title":"\"ML replaces domain expertise\"","text":"<p>Reality: Domain knowledge guides every step.</p> <p>Experts choose relevant features, interpret results, identify data issues, and decide when predictions make sense. Algorithms are tools; humans provide context.</p>"},{"location":"04-core-ml/ml-intro/#ml-is-objective-and-bias-free","title":"\"ML is objective and bias-free\"","text":"<p>Reality: Models inherit biases from training data.</p> <p>If historical hiring data shows bias against certain groups, ML models trained on that data perpetuate the bias. Fairness requires careful data curation and model auditing.</p>"},{"location":"04-core-ml/ml-intro/#prerequisites-for-learning-ml","title":"Prerequisites for Learning ML","text":"<p>You don't need a PhD, but certain foundations make learning easier:</p> <p>Math (refresher sufficient):</p> <ul> <li>Linear algebra: Vectors, matrices, dot products</li> <li>Calculus: Derivatives, gradients (conceptual understanding)</li> <li>Probability: Distributions, conditional probability, Bayes' theorem</li> <li>Statistics: Mean, variance, hypothesis testing, correlation</li> </ul> <p>Programming (intermediate):</p> <ul> <li>Python: Comfortable with functions, loops, data structures</li> <li>NumPy: Array operations, indexing, broadcasting</li> <li>Pandas: Load data, filter, transform, aggregate</li> </ul> <p>Data skills:</p> <ul> <li>Read CSV/JSON files</li> <li>Handle missing values</li> <li>Create visualizations</li> <li>Understand databases (SQL basics)</li> </ul>"},{"location":"04-core-ml/ml-intro/#best-practices","title":"Best Practices","text":"<p>Start with the end in mind: Define success metrics before building models. \"Reduce fraud losses by 20%\" beats \"build accurate fraud detector.\"</p> <p>Embrace simplicity: Simple models are easier to debug, interpret, deploy, and maintain. Complexity should be justified by performance gains.</p> <p>Question everything: Does this data make sense? Why did the model make this prediction? What happens if this assumption is wrong? Skepticism prevents costly mistakes.</p> <p>Document decisions: Record why you chose features, models, and hyperparameters. Future you (or teammates) will need to understand choices made months ago.</p> <p>Think beyond accuracy: Consider latency, interpretability, fairness, maintenance cost, and business impact. The \"most accurate\" model isn't always the right choice.</p> <p>Test on real-world conditions: Models that work in lab settings sometimes fail in production. Test with realistic data, edge cases, and adversarial inputs.</p>"},{"location":"04-core-ml/ml-intro/#quick-reference-ml-fundamentals","title":"Quick Reference: ML Fundamentals","text":"Concept Definition Example Machine Learning Learning patterns from data without explicit programming Spam detection learns from labeled emails Features Input variables used for predictions Square footage, bedrooms, location Labels Target variable being predicted House price, spam/not spam Model Mathematical representation mapping features to labels Equation, decision tree, neural network Training Process of model learning patterns from data Adjusting parameters to minimize errors Prediction Using trained model on new data Estimate price for unseen house Traditional Programming Rules + Data \u2192 Output Calculate tax from income using formula Machine Learning Data + Output \u2192 Rules (Model) Learn what makes email spam from examples"},{"location":"04-core-ml/ml-intro/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You now understand what machine learning is and how it differs from traditional programming, know when ML makes sense and when simpler approaches work better, understand core ML terminology (features, labels, models, training, prediction), and recognize common misconceptions about ML capabilities and limitations.</p> <p>Critical insights:</p> <ul> <li>ML learns patterns, not rules: You provide examples; algorithms discover relationships</li> <li>Not every problem needs ML: Use the simplest approach that works</li> <li>Data quality matters most: Better data beats better algorithms</li> <li>Domain expertise is essential: ML is a tool that augments human knowledge</li> </ul> <p>External resources:</p> <ul> <li>Google's ML Crash Course - Interactive introduction with exercises</li> <li>Scikit-learn Documentation - Python's primary ML library tutorials</li> </ul> <p>Remember: Machine learning is a tool, not magic. Master the fundamentals, question assumptions, and prioritize data quality. With these foundations, you'll build models that work reliably in the real world.</p>"},{"location":"04-core-ml/regression/","title":"Regression: Predicting Continuous Values","text":"<p>This tutorial introduces regression, the supervised learning technique for predicting numeric values. You'll learn how different regression models work, when to use each one, and how to evaluate predictions using a real estate rent prediction example that connects every concept to practical decisions.</p> <p>Estimated time: 45 minutes</p>"},{"location":"04-core-ml/regression/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: </p> <p>Business questions demand numbers, not categories. \"How much will it cost? When will it arrive? How many will we sell?\"</p> <p>Companies need numeric predictions to make decisions. A retailer planning inventory needs to predict next month's sales in units, not \"high/medium/low.\" A bank approving loans needs to estimate default probability as a percentage, not just \"risky/safe.\" A delivery service needs estimated arrival times in minutes, not \"soon/later.\" Regression provides these continuous predictions that drive operational and financial decisions.</p> <p>Practical benefits: Regression models help you forecast revenue, estimate project timelines, predict customer lifetime value, optimize pricing, and plan resource allocation. Every prediction comes with an error estimate, telling you how confident to be. You'll understand which features matter most, enabling better data collection and feature engineering.</p> <p>Professional context: </p> <p>Regression is the workhorse of predictive analytics. </p> <p>Most business metrics are continuous numbers (revenue, time, quantity, price), making regression applicable across industries. Data scientists spend more time building regression models than any other model type. Master regression fundamentals, and you'll handle 60% of real-world ML projects. The principles you learn here apply to all supervised learning.</p> <p></p>"},{"location":"04-core-ml/regression/#running-example-predicting-apartment-rent","title":"Running Example: Predicting Apartment Rent","text":"<p>Throughout this tutorial, we'll follow a real estate platform building a rent prediction system. They have data on 10,000 apartments across Tirana\u2014each with features like <code>size_m2</code>, <code>location</code>, <code>number of bedrooms</code>, <code>building age</code>, and <code>distance to center</code>. </p> <p>Their goal is predicting monthly rent for new listings so property owners can price competitively and renters can identify fair deals.</p> <p>Success metric: Predictions within \u20ac50 of actual rent (roughly 10% error for typical \u20ac500/month apartments).</p> <p>This example anchors every concept with concrete decisions and results you can evaluate.</p>"},{"location":"04-core-ml/regression/#core-concepts","title":"Core Concepts","text":""},{"location":"04-core-ml/regression/#what-is-regression","title":"What Is Regression?","text":"<p>Regression predicts a continuous numeric value based on input features. The model learns relationships between features (apartment size, location, age) and a target (monthly rent), then uses those relationships to estimate the target for new, unseen examples.</p> <p>Key characteristics:</p> <ul> <li>Output is numeric and continuous: \u20ac850/month, not \"expensive\" or \"affordable\"</li> <li>Learns from labeled examples: Historical data with known rents trains the model</li> <li>Finds patterns in features: Discovers how size, location, and age affect rent</li> <li>Generalizes to new data: Predicts rent for apartments not in training set</li> </ul> <p>Regression vs Classification:</p> Regression Classification Predicts numbers Predicts categories \"How much?\" \u2192 \u20ac850 \"Which class?\" \u2192 Expensive/Affordable Infinite possible outputs Fixed number of classes MAE, RMSE, R\u00b2 metrics Accuracy, precision, recall metrics <p>When to use regression:</p> <ul> <li>Target variable is numeric and continuous</li> <li>Need specific value predictions (not just categories)</li> <li>Understand how much each feature contributes</li> <li>Compare predictions across a range</li> </ul>"},{"location":"04-core-ml/regression/#real-world-applications","title":"Real-World Applications","text":"<p>Finance:</p> <ul> <li>Stock price forecasting</li> <li>Credit limit determination</li> <li>Risk assessment (default probability as percentage)</li> </ul> <p>E-commerce:</p> <ul> <li>Dynamic pricing optimization</li> <li>Customer lifetime value prediction</li> <li>Demand forecasting for inventory</li> </ul> <p>Healthcare:</p> <ul> <li>Hospital length-of-stay estimation</li> <li>Disease progression timeline prediction</li> <li>Dosage optimization</li> </ul> <p>Operations:</p> <ul> <li>Delivery time estimation</li> <li>Resource allocation planning</li> <li>Maintenance cost forecasting</li> </ul> <p>Each application shares the same structure: historical examples train a model that predicts numeric outcomes for new cases.</p>"},{"location":"04-core-ml/regression/#simple-linear-regression","title":"Simple Linear Regression","text":"<p>Goal: Find the straight line that best describes the relationship between one feature and the target.</p> <p>A straight line equation:</p> \\[ y = \\beta_0 + \\beta_1 x \\] <ul> <li>y = predicted rent (what we want to know)</li> <li>x = apartment size in square feet (what we measure)</li> <li>\\(\\beta_1\\) (slope) = how much rent changes per additional square foot</li> <li>\\(\\beta_0\\) (intercept) = base rent when size = 0 (theoretical, not realistic)</li> </ul> <p></p> <p>Learning means finding the best \\(\\beta_0\\) and \\(\\beta_1\\) so the line passes as close as possible to all training points.</p>"},{"location":"04-core-ml/regression/#apartment-example-size-predicts-rent","title":"Apartment Example: Size Predicts Rent","text":"<p>You have <code>10,000</code> apartments with known size and rent. Plot them as scatter points (<code>x = m2</code>, <code>y = rent</code>). Draw a line through them. That line lets you predict rent for any size.</p> <pre><code>from sklearn.linear_model import LinearRegression\nimport numpy as np\n\n# Prepare data: one feature (size), one target (rent)\nX_train = apartments[['size_m2']]  # Must be 2D array\ny_train = apartments['rent']    # 1D array of rents\n\n# Create and train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# The learned parameters\nprint(f\"Slope: ${model.coef_[0]:.2f} per size_m2\")\nprint(f\"Intercept: ${model.intercept_:.2f}\")\n\n# Example: What's the rent for an 800 size_m2 apartment?\npredicted_rent = model.predict([[800]])\nprint(f\"Predicted rent: ${predicted_rent[0]:.0f}/month\")\n</code></pre> <p>Interpreting results:</p> <p>If \\(\\beta_1 = 6\\text{\u20ac}/\\text{m}^2\\) and \\(\\beta_0 = 150\\text{\u20ac}\\), the model equation is:</p> \\[ \\text{Rent} = 150 + 6 \\times \\text{size\\_m2} \\] <p>Rent = \u20ac150 + \u20ac6 \u00d7 Area</p> <ul> <li>60 \\(\\text{m}^2\\) apartment (typical 1+1) \u2192 \u20ac150 + \u20ac6(60) = \u20ac510/month</li> <li>90 \\(\\text{m}^2\\) apartment (typical 2+1) \u2192 \u20ac150 + \u20ac6(90) = \u20ac690/month</li> <li>Each additional square meter adds \u20ac6 to the monthly rent</li> </ul> <p>Limitation: Simple linear regression uses only one feature. Real rent depends on location, age, amenities; not just size. That's where multiple regression helps.</p>"},{"location":"04-core-ml/regression/#multiple-linear-regression","title":"Multiple Linear Regression","text":"<p>Goal: Use multiple features simultaneously to predict more accurately.</p>"},{"location":"04-core-ml/regression/#the-core-idea","title":"The Core Idea","text":"<p>Extended equation:</p> \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ... \\] <p>Each feature gets its own coefficient showing its individual contribution to the prediction.</p> <p></p>"},{"location":"04-core-ml/regression/#apartment-example-four-features","title":"Apartment Example: Four Features","text":"<p>Now use size, bedrooms, building age, and distance to subway:</p> \\[ \\text{Rent} = \\beta_0 + \\beta_1(\\text{size_m2}) + \\beta_2(\\text{bedrooms}) + \\beta_3(\\text{age}) + \\beta_4(\\text{distance}) \\] <pre><code># Multiple features\nfeatures = ['size_m2', 'bedrooms', 'age', 'distance_to_subway']\nX_train = apartments[features]\ny_train = apartments['rent']\n\n# Train model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Show each feature's effect\nfor feature, coef in zip(features, model.coef_):\n    print(f\"{feature}: ${coef:.2f}\")\n</code></pre> <p>Sample output:</p> <pre><code>size_m2: \u20ac5.50 per m2\nbedrooms: \u20ac80.00 per bedroom\nage: -\u20ac5.00 per year\ndistance_to_center: -\u20ac40.00 per km\n</code></pre> <p>Interpretation:</p> <ul> <li>Every square meter adds \u20ac5.50 (holding other features constant)</li> <li>Each bedroom adds \u20ac80 (independent of size)</li> <li>Each year older reduces rent by \u20ac5 (older = cheaper)</li> <li>Each km farther from center reduces rent by \u20ac40 (location matters)</li> </ul> <p>Why multiple features matter:</p> <ul> <li>Size alone explains <code>55%</code> of rent variation (\\(R^2 = 0.55\\))</li> <li>Adding bedrooms improves to 68%</li> <li>Adding age and location improves to 82%</li> <li>Real predictions need multiple factors</li> </ul> <p>Key insight: Coefficients show independent effects. </p> <p>The bedroom coefficient (<code>\u20ac80</code>) answers: \"If two apartments are the same size, same age, same location, but one has an extra bedroom, it rents for <code>\u20ac80</code> more.\"</p>"},{"location":"04-core-ml/regression/#polynomial-regression","title":"Polynomial Regression","text":"<p>Goal: Capture non-linear relationships when straight lines don't fit.</p>"},{"location":"04-core-ml/regression/#the-core-idea_1","title":"The Core Idea","text":"<p>Some relationships aren't linear. Rent might increase slowly for small apartments, then rapidly for large ones. Polynomial regression adds squared or cubed terms to capture curves.</p> <p>Equation:</p> \\[ y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + ... \\] <p>For apartment size:</p> \\[ \\text{Rent} = \\beta_0 + \\beta_1(\\text{size\\_m2}) + \\beta_2(\\text{size\\_m2}^2) \\] <p>The squared term bends the line into a curve.</p> <p></p>"},{"location":"04-core-ml/regression/#when-relationships-curve","title":"When Relationships Curve","text":"<pre><code>from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import make_pipeline\n\n# Create polynomial features (degree=2 adds squared terms)\nmodel = make_pipeline(\n    PolynomialFeatures(degree=2),\n    LinearRegression()\n)\n\nmodel.fit(X_train[['size_m2']], y_train)\npredictions = model.predict(X_test[['size_m2']])\n</code></pre> <p>Apartment example: </p> <p>Small apartments (40-60 m\u00b2) rent for \u20ac350-\u20ac550. Mid-size (70-100 m\u00b2) rent for \u20ac600-\u20ac900. Large luxury apartments (120-200 m\u00b2) rent for \u20ac1,200-\u20ac2,500. The increase accelerates\u2014a polynomial curve fits better than a straight line.</p> <p>Warning: Higher-degree polynomials (3rd, 4th degree) can overfit, fitting noise instead of true patterns. Start with degree 2, check if it helps, rarely go beyond degree 3.</p> <p>When to use polynomial regression:</p> <ul> <li>Scatter plot shows clear curve (not straight line)</li> <li>Residuals from linear regression show pattern (not random)</li> <li>Domain knowledge suggests non-linearity (diminishing returns, exponential growth)</li> </ul>"},{"location":"04-core-ml/regression/#tree-based-regression","title":"Tree-Based Regression","text":"<p>Goal: Capture complex non-linear patterns using decision rules.</p>"},{"location":"04-core-ml/regression/#decision-tree-regression","title":"Decision Tree Regression","text":"<p>Core idea: Split data into groups based on feature values, predict the average rent within each group.</p> <p>How it works:</p> <ol> <li>Find the feature and split point that best separates high-rent from low-rent apartments</li> <li>Example: Split at <code>\"size &lt; 80 m2\"</code></li> <li>For each resulting group, repeat the splitting process</li> <li>Continue until groups are small or pure</li> <li>Predict average rent within each final group</li> </ol> <p></p> <pre><code>from sklearn.tree import DecisionTreeRegressor\n\n# Train decision tree\nmodel = DecisionTreeRegressor(max_depth=5, min_samples_leaf=20)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n</code></pre> <p>Apartment example decision path:</p> <pre><code>Is size &lt; 80 size_m2?\n\u251c\u2500 Yes: Is distance &lt; 2 km?\n\u2502  \u251c\u2500 Yes: Predict \u20ac550 (small, central)\n\u2502  \u2514\u2500 No: Predict \u20ac350 (small, far)\n\u2514\u2500 No: Is size &lt; 120 size_m2?\n   \u251c\u2500 Yes: Predict \u20ac850 (medium)\n   \u2514\u2500 No: Predict \u20ac1,200 (large)\n</code></pre> <p>Advantages:</p> <ul> <li>Handles non-linear relationships naturally (no need for polynomial features)</li> <li>Captures interactions automatically (size matters more in central locations)</li> <li>Interpretable rules (easy to explain decisions)</li> </ul> <p>Disadvantages:</p> <ul> <li>Overfits easily if tree is too deep</li> <li>Small changes in data can drastically change tree structure</li> <li>Less smooth predictions (jumps at split boundaries)</li> </ul>"},{"location":"04-core-ml/regression/#random-forest-regression","title":"Random Forest Regression","text":"<p>Core idea: Build many decision trees on random subsets of data, average their predictions.</p> <p>How it works:</p> <ol> <li>Create 100 different training sets by randomly sampling with replacement (bootstrapping)</li> <li>Train a decision tree on each set, using random subsets of features at each split</li> <li>For new apartment, get prediction from all 100 trees</li> <li>Average those 100 predictions for final estimate</li> </ol> <p></p> <pre><code>from sklearn.ensemble import RandomForestRegressor\n\n# Train random forest (100 trees)\nmodel = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_leaf=5,\n    random_state=42\n)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n\n# Feature importance\nimportances = pd.DataFrame({\n    'feature': features,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)\nprint(importances)\n</code></pre> <p>Apartment example output:</p> <pre><code>feature          importance\nsize_m2          0.42\ndistance         0.23\nbedrooms         0.15\nage              0.11\nbathrooms        0.06\nparking          0.03\n</code></pre> <p>Square footage explains <code>42%</code> of rent variation, distance to transit <code>23%</code>, bedrooms <code>15%</code>.</p> <p>Why Random Forest beats single trees:</p> <ul> <li>Reduces overfitting: Averaging many trees smooths out individual tree mistakes</li> <li>More accurate: Ensemble of 100 trees outperforms any single tree</li> <li>Robust: Small data changes don't drastically affect predictions</li> <li>Feature importance: Shows which features matter most</li> </ul> <p>Trade-offs:</p> <ul> <li>Slower to train (100 trees vs 1 tree)</li> <li>Less interpretable (can't easily show one decision path)</li> <li>Requires more memory (stores all trees)</li> </ul> <p>When to use tree-based models:</p> <ul> <li>Non-linear relationships exist</li> <li>Feature interactions matter (size affects rent differently in different neighborhoods)</li> <li>You need feature importance scores</li> <li>Interpretability is secondary to accuracy</li> </ul>"},{"location":"04-core-ml/regression/#how-regression-learns-loss-functions","title":"How Regression Learns: Loss Functions","text":"<p>Goal: Measure how wrong predictions are so the model can improve.</p>"},{"location":"04-core-ml/regression/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<p>Most common regression loss. For each apartment, calculate <code>the square of (predicted rent - actual rent)</code>, then average across all apartments.</p> <p>Formula:</p> \\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\] <p>Why square errors?</p> <ul> <li>Large errors penalized heavily (off by <code>\u20ac500</code> is 25 times worse than off by <code>\u20ac100</code>)</li> <li>Always positive (errors don't cancel out)</li> <li>Math works nicely for finding optimal parameters</li> </ul> <p>Apartment example (Tirana):</p> <ul> <li>Apt 1: Predict \u20ac500, actual \u20ac550 \u2192 Error = \u20ac50 \u2192 Squared = 2,500</li> <li>Apt 2: Predict \u20ac700, actual \u20ac800 \u2192 Error = \u20ac100 \u2192 Squared = 10,000</li> <li>Apt 3: Predict \u20ac450, actual \u20ac430 \u2192 Error = -\u20ac20 \u2192 Squared = 400</li> </ul> \\[ \\text{MSE} = \\frac{2,500 + 10,000 + 400}{3} = 4,300 \\] <p>Training goal: Adjust coefficients to minimize MSE across all training examples.</p>"},{"location":"04-core-ml/regression/#other-loss-functions","title":"Other Loss Functions","text":"<p>Mean Absolute Error (MAE): Average of absolute errors (not squared).</p> <ul> <li>Less sensitive to outliers</li> <li>Apartment example: \\(\\text{MAE} = \\frac{50 + 100 + 20}{3} \\approx \\text{\u20ac56.7}\\)</li> </ul> <p>Huber Loss: Combines MSE and MAE benefits. Squared for small errors (smooth gradient), linear for large errors (robust to outliers).</p> <pre><code>from sklearn.metrics import mean_absolute_error\n\npredictions = model.predict(X_test)\nmae = mean_absolute_error(y_test, predictions)\nprint(f\"Average prediction error: \u20ac{mae:.0f}\")\n</code></pre>"},{"location":"04-core-ml/regression/#r2-score-coefficient-of-determination","title":"R\u00b2 Score (Coefficient of Determination)","text":"<p>What it means: Percentage of rent variation explained by your model (0 to 1).</p> <p>Formula:</p> \\[ R^2 = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum (y_i - \\bar{y})^2} \\] <p>Interpretation:</p> <ul> <li>R\u00b2 = 0.85 \u2192 Model explains 85% of why rents differ</li> <li>R\u00b2 = 0.50 \u2192 Model explains only 50% \u2192 mediocre</li> <li>R\u00b2 = 0.20 \u2192 Model explains 20% \u2192 poor, most variation unexplained</li> </ul> <pre><code>from sklearn.metrics import r2_score\n\nr2 = r2_score(y_test, predictions)\nprint(f\"Variance explained: {r2:.2%}\")\n</code></pre> <p>Apartment example: <code>$R^2 = 0.82$</code> means the four features (size, bedrooms, age, distance) explain <code>82%</code> of rent differences. The remaining <code>18%</code> comes from factors not in your data (views, renovation quality, landlord reputation).</p>"},{"location":"04-core-ml/regression/#which-metric-to-use","title":"Which Metric to Use?","text":"Metric Use When Apartment Context MAE Easy interpretation needed \"Off by \u20ac85 on average\" RMSE Penalize large errors Critical to avoid big mistakes R\u00b2 Compare models \"Model A explains 82%, Model B only 65%\" <p>Decision framework for apartment rent:</p> <ul> <li>MAE &lt; \u20ac50 \u2192 Deploy confidently</li> <li>MAE \u20ac50-\u20ac150 \u2192 Deploy with warnings</li> <li>MAE &gt; \u20ac150 \u2192 Don't deploy, collect better data or features</li> </ul>"},{"location":"04-core-ml/regression/#visualizing-predictions","title":"Visualizing Predictions","text":"<p>Predicted vs Actual plot:</p> <pre><code>import matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nplt.scatter(y_test, predictions, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], \n         [y_test.min(), y_test.max()], \n         'r--', linewidth=2)\nplt.xlabel('Actual Rent ($)')\nplt.ylabel('Predicted Rent ($)')\nplt.title('Prediction Quality')\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Perfect model: All points lie on red diagonal line. Real model: Points scatter around line. Tighter scatter = better predictions.</p> <p></p>"},{"location":"04-core-ml/regression/#common-regression-challenges","title":"Common Regression Challenges","text":""},{"location":"04-core-ml/regression/#overfitting","title":"Overfitting","text":"<p>Problem: Model memorizes training data noise instead of learning true patterns.</p> <p>Detection: Training \\(R^2 = 0.95\\), test \\(R^2 = 0.65\\) (huge gap).</p> <p>Solutions: Use regularization (stay tuned), collect more data, simplify model (fewer features, shallower trees).</p>"},{"location":"04-core-ml/regression/#outliers","title":"Outliers","text":"<p>Problem: Extreme values skew predictions.</p> <p>Apartment example: One \u20ac3,000/month penthouse pulls model's predictions up for all large apartments.</p> <p>Solutions: Remove outliers after investigation, use robust regression methods, cap extreme values, use tree-based models (naturally robust).</p>"},{"location":"04-core-ml/regression/#feature-scaling","title":"Feature Scaling","text":"<p>Problem: Features with large ranges dominate distance calculations.</p> <p>Apartment example: Size (50-500) overwhelms bedrooms (1-4) in unscaled models.</p> <p>Solution: Standardize features before training (except for tree-based models).</p> <pre><code>from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n</code></pre>"},{"location":"04-core-ml/regression/#complete-workflow-example","title":"Complete Workflow Example","text":"<p>Here's end-to-end apartment rent prediction:</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\n# 1. Load data\ndf = pd.read_csv('apartments.csv')\n\n# 2. Select features and target\nfeatures = ['size_m2', 'bedrooms', 'bathrooms', 'age', 'distance_to_subway']\nX = df[features]\ny = df['rent']\n\n# 3. Split data (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# 4. Train model\nmodel = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\nmodel.fit(X_train, y_train)\n\n# 5. Evaluate on test set\npredictions = model.predict(X_test)\nmae = mean_absolute_error(y_test, predictions)\nr2 = r2_score(y_test, predictions)\n\nprint(f\"Test MAE: ${mae:.0f}\")\nprint(f\"Test R\u00b2: {r2:.2%}\")\n\n# 6. Predict for new apartment\nnew_apartment = [[850, 2, 1, 5, 0.3]]  # 850size_m2, 2bed, 1bath, 5yrs, 0.3km\npredicted_rent = model.predict(new_apartment)\nprint(f\"\\nPredicted rent: ${predicted_rent[0]:.0f}/month\")\n\n# 7. Feature importance\nfor feature, importance in zip(features, model.feature_importances_):\n    print(f\"{feature}: {importance:.2%}\")\n</code></pre> <p></p>"},{"location":"04-core-ml/regression/#best-practices","title":"Best Practices","text":"<p>Start simple, add complexity only when needed. Begin with linear regression. If \\(R^2 &lt; 0.70\\), try polynomial or tree-based models.</p> <p>Check assumptions before modeling. Plot features vs target. If relationships look linear, linear regression works. If curved, try polynomial or trees.</p> <p>Always split data. Train on <code>80%</code>, test on <code>20%</code>. Never evaluate on training data; overfitting hides there.</p> <p>Interpret coefficients. Linear models tell you exactly how each feature affects the target. Use this to validate against domain knowledge.</p> <p>Visualize residuals. Patterns in residuals reveal problems metrics miss (e.g., underpredicting expensive apartments).</p> <p>Set realistic expectations. Perfect predictions are impossible. Aim for \"useful\" not \"perfect.\" MAE of \u20ac80 for \u20ac900 apartments (<code>9%</code> error) is good.</p> <p>Monitor in production. Track MAE monthly. If it increases from \u20ac80 to \u20ac130, retrain with recent data (market changed).</p>"},{"location":"04-core-ml/regression/#quick-reference","title":"Quick Reference","text":"Model Type When to Use Pros Cons Linear Regression Simple, linear relationships Fast, interpretable, stable Can't capture curves Polynomial Regression Non-linear relationships Fits curves, still interpretable Overfits easily Ridge/Lasso Many features, overfitting risk Prevents overfitting, automatic feature selection (Lasso) Less interpretable Decision Tree Complex interactions, non-linear Handles interactions, interpretable rules Overfits, unstable Random Forest Best accuracy needed High accuracy, robust, feature importance Slow, less interpretable Metric Meaning Good Value MAE Average error in dollars &lt; 10% of typical value RMSE Error penalizing outliers &lt; 15% of typical value R\u00b2 % variance explained &gt; 0.70"},{"location":"04-core-ml/regression/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You understand regression predicts continuous numeric values from features, know how linear regression finds coefficients minimizing prediction errors, can use multiple features to improve accuracy, recognize when to use polynomial, regularized, or tree-based models, and evaluate predictions using MAE, RMSE, and R\u00b2 with clear interpretation.</p> <p>Connections to previous tutorials:</p> <ul> <li>ML Lifecycle: Regression lives in the Model Training phase. You'll iterate between data preparation and model selection based on evaluation results.</li> <li>Data preprocessing: Feature scaling, handling missing values, and outlier treatment directly affect regression accuracy.</li> </ul> <p>What's next:</p> <ul> <li>Classification tutorial: Predicting categories instead of numbers (will this customer churn? yes/no)</li> <li>Model improvement: Cross-validation, hyperparameter tuning, feature engineering strategies</li> <li>Advanced regression: Gradient boosting, neural networks for regression, time-series forecasting</li> </ul> <p>External resources:</p> <ul> <li>Scikit-learn Regression Guide - Complete API documentation</li> <li>StatQuest Regression Playlist - Visual explanations of regression concepts</li> <li>Google ML Crash Course - Interactive regression exercises</li> </ul> <p>Remember: Regression isn't about perfect predictions. It's about understanding relationships between features and outcomes, then using those patterns to make useful estimates that improve decisions.</p>"},{"location":"04-core-ml/reinforcement/","title":"Reinforcement","text":""},{"location":"04-core-ml/reinforcement/#materials-under-construction","title":"Materials Under Construction","text":"<p>This section is still brewing. In the meantime, grab a snack, ship some code, and check back soon.</p> <p></p>"},{"location":"04-core-ml/supervised-advanced/","title":"Advanced Classification: Ensemble Methods and Support Vector Machines","text":"<p>This tutorial introduces powerful classification techniques that solve problems where simple methods struggle. You'll learn Support Vector Machines for non-linear boundaries, ensemble methods that combine multiple models, and when to choose advanced approaches over simple baselines. Every concept connects to a real spam detection system handling complex feature interactions.</p> <p>Estimated time: 60 minutes</p>"},{"location":"04-core-ml/supervised-advanced/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement:</p> <p>Simple classifiers work well on clean, linearly separable data. Real-world problems are messy, non-linear, and high-dimensional.</p> <p>Real data challenges simple methods. A spam filter with just five features (link count, CAPSLOCK ratio, word count) works reasonably well with Logistic Regression. But production systems extract hundreds of features from email content, metadata, sender reputation, and historical patterns. These features interact in complex ways that linear boundaries cannot capture. A known sender becomes suspicious if they suddenly send many links. Simple classifiers miss these interactions.</p> <p>Practical benefits: Advanced methods deliver 5-15% accuracy improvements on hard problems, which translates to thousands fewer errors on large-scale systems. They handle noisy real-world data better through regularization and ensemble averaging. You can deploy them on datasets with hundreds or thousands of features without manual feature selection. The confidence scores they provide are better calibrated for decision-making under uncertainty.</p> <p>Professional context: Winning Kaggle solutions, production fraud detection systems, medical diagnosis tools, and credit scoring models all rely on these techniques. Random Forest and XGBoost dominate industry applications because they work well with minimal tuning. Understanding when a simple Logistic Regression baseline is sufficient versus when you need ensemble methods is a critical professional skill that separates junior from senior practitioners.</p> <p></p>"},{"location":"04-core-ml/supervised-advanced/#running-example-production-spam-detection","title":"Running Example: Production Spam Detection","text":"<p>Throughout this tutorial, we'll follow the evolution of our email spam filter from the simple classification tutorial. The original system used five hand-crafted features and achieved 85% F1-score with Logistic Regression. Production demands pushed us to improve accuracy to 92%+ to reduce the manual review workload that costs the company significant time and money.</p> <p>Enhanced dataset: The team now extracts 50+ features from each email including word frequencies for the top 30 spam-indicating terms, sender domain reputation scores, email metadata like send time and reply chain depth, text patterns such as repeated punctuation and ALL CAPS word count, attachment types and sizes, and historical sender behavior patterns. This rich feature set captures subtle spam signals but creates non-linear decision boundaries that simple methods cannot learn.</p> <p>Complex patterns emerge: High link count predicts spam unless the sender domain is a recognized corporate partner. Urgent language signals spam except when it comes from known finance or legal departments. Short emails with attachments are suspicious unless they match the sender's historical pattern. These feature interactions require advanced methods that can learn conditional rules.</p> <p>Business requirements: Precision must exceed 90% because blocking legitimate business emails damages customer relationships. Recall should reach 85% to keep inboxes clean without overwhelming manual reviewers. Training time matters less than prediction speed since the model runs millions of times daily. The solution must be explainable enough to debug false positives when customers complain.</p>"},{"location":"04-core-ml/supervised-advanced/#when-simple-methods-arent-enough","title":"When Simple Methods Aren't Enough","text":"<p>Simple classifiers have clear limitations that advanced methods address. Logistic Regression assumes a linear decision boundary, so it fails when spam occupies a circular region in feature space or when class membership depends on complex feature interactions. A single decision tree overfits easily, creating unstable models where small changes in training data produce completely different trees. K-Nearest Neighbors struggles with high-dimensional data where distances become meaningless and computation becomes prohibitively slow on large datasets.</p> <p>The solution involves two main approaches. Kernel methods like Support Vector Machines transform features into higher-dimensional spaces where linear separation becomes possible without explicitly computing the transformation. Ensemble methods combine many weak models into one strong predictor, reducing both bias and variance through averaging or boosting. Both approaches trade increased computational cost and reduced interpretability for better predictive performance on complex problems.</p>"},{"location":"04-core-ml/supervised-advanced/#support-vector-machines","title":"Support Vector Machines","text":"<p>Goal: Find the decision boundary that maximally separates classes while handling non-linear patterns through kernel transformations.</p>"},{"location":"04-core-ml/supervised-advanced/#the-core-idea","title":"The Core Idea","text":"<p>Support Vector Machines search for the widest possible margin between classes. Imagine a street separating spam from legitimate emails in feature space. The street's center line is the decision boundary, and its width is the margin. SVM finds the widest street where all spam emails are on one side and legitimate emails are on the other. The emails closest to the boundary, touching the street's edges, are called support vectors because they alone determine where the boundary sits. Points far from the boundary have no influence on the decision surface.</p> <p>This maximum margin principle provides better generalization than methods that merely find any separating boundary. A wide margin means the model is confident about its decisions and less likely to misclassify new examples that fall near the boundary. The mathematical formulation seeks a hyperplane \\(\\mathbf{w} \\cdot \\mathbf{x} + b = 0\\) that maximizes the margin \\(\\frac{2}{||\\mathbf{w}||}\\) while correctly classifying training points. Only the support vectors affect this optimization, making SVM memory-efficient on large datasets.</p> <p></p>"},{"location":"04-core-ml/supervised-advanced/#the-kernel-trick","title":"The Kernel Trick","text":"<p>Linear SVM works beautifully when classes are linearly separable, but real data rarely cooperates. Consider spam emails that cluster in a circular region within feature space, surrounded by legitimate emails. No straight line separates them. The kernel trick solves this by implicitly mapping data to a higher-dimensional space where linear separation becomes possible, without ever computing the transformation explicitly.</p> <p>The <code>RBF (Radial Basis Function)</code> kernel is the most widely used transformation. It computes similarity between points using \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = e^{-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2}\\), effectively creating infinite-dimensional feature space where a hyperplane can separate any pattern. The gamma parameter controls how far the influence of a single training example reaches. Low gamma means far reach and smooth decision boundaries. High gamma means nearby reach and complex, wiggly boundaries that can overfit.</p> <p>For our spam example, linear boundaries fail because spam patterns involve combinations like \"many links AND unknown sender\" or \"high CAPSLOCK OR urgent language from new contact.\" The RBF kernel learns these OR and AND combinations naturally by creating decision regions of arbitrary shape. A polynomial kernel \\(K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i \\cdot \\mathbf{x}_j + c)^d\\) captures polynomial feature interactions up to degree d, useful when you know the relationship has that specific form.</p>"},{"location":"04-core-ml/supervised-advanced/#implementation","title":"Implementation","text":"<p>Training an SVM requires scaled features because the algorithm computes distances between points. Features with large magnitudes dominate the distance calculation, so we must standardize all features to comparable scales before training.</p> <pre><code>from sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Prepare data\nX = emails[features]  # 50+ features\ny = emails['is_spam']\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Scale features (critical for SVM!)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train SVM with RBF kernel\nmodel = SVC(\n    kernel='rbf',\n    C=1.0,                    # Regularization strength\n    gamma='scale',            # Kernel coefficient\n    class_weight='balanced',  # Handle imbalanced classes\n    random_state=42\n)\n\nmodel.fit(X_train_scaled, y_train)\n\n# Predict\npredictions = model.predict(X_test_scaled)\nprobabilities = model.decision_function(X_test_scaled)  # Distance from boundary\n</code></pre> <p>The <code>C</code> parameter controls the trade-off between margin width and training accuracy. Small <code>C</code> values (0.1) allow more misclassifications but create wider, simpler margins that generalize better. Large <code>C</code> values (100) insist on classifying training points correctly, creating narrow margins that may overfit. Start with <code>C=1</code> and adjust based on cross-validation performance.</p> <p>The <code>gamma</code> parameter for RBF kernels determines decision boundary complexity. Small <code>gamma</code> (0.001) creates smooth, simple boundaries that may underfit. Large <code>gamma</code> (10) creates complex, wiggly boundaries that can memorize training data. The <code>'scale'</code> setting uses \\(1 / (n_{features} \\times X.var())\\) as a reasonable default that adapts to your data.</p>"},{"location":"04-core-ml/supervised-advanced/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>Finding optimal <code>C</code> and <code>gamma</code> requires systematic search over a grid of values. We use cross-validation to estimate performance for each combination and select the pair that maximizes our target metric.</p> <p>Cross-validation is a technique that tests how well a model performs on unseen data by splitting the dataset into multiple subsets called folds. The model trains on some folds and tests on the remaining fold, repeating this process multiple times with different combinations so each fold serves as the test set once. The results from all iterations are averaged to provide a reliable estimate of the model's true performance and ability to generalize to new data.</p> <pre><code>from sklearn.model_selection import GridSearchCV\n\n# Define parameter grid\nparam_grid = {\n    'C': [0.1, 1, 10, 100],\n    'gamma': [0.001, 0.01, 0.1, 1, 'scale']\n}\n\n# Grid search with cross-validation\ngrid_search = GridSearchCV(\n    SVC(kernel='rbf', class_weight='balanced', random_state=42),\n    param_grid,\n    cv=5,\n    scoring='f1',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train_scaled, y_train)\n\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best F1-score: {grid_search.best_score_:.3f}\")\n\n# Use best model\nbest_model = grid_search.best_estimator_\n</code></pre> <p>This search tests 20 combinations (4 C values \u00d7 5 gamma values) using 5-fold cross-validation, running 100 total training jobs. Parallel execution with <code>n_jobs=-1</code>uses all CPU cores to speed up the search. For larger parameter spaces, use <code>RandomizedSearchCV</code> to sample a subset of combinations rather than exhaustively testing all.</p>"},{"location":"04-core-ml/supervised-advanced/#strengths-and-limitations","title":"Strengths and Limitations","text":"<p>SVM excels with non-linear patterns where kernel transformations create separable feature spaces. It works well in high-dimensional spaces and remains effective even when the number of features exceeds the number of samples. Memory efficiency comes from storing only support vectors rather than all training data. The maximum margin principle provides strong generalization when classes have clear separation.</p> <p>However, SVM trains slowly on large datasets because the optimization problem scales poorly beyond 50,000 samples. Feature scaling is mandatory, adding a preprocessing step. The many hyperparameters (kernel choice, C, gamma) require careful tuning through cross-validation. Standard SVM does not output probability estimates, only decision function values, though calibration methods can convert these. The resulting model is a black box with little interpretability compared to decision trees.</p> <p>Use SVM when you have non-linear patterns that simpler methods miss, high-dimensional feature spaces like text or images, medium-sized datasets under 50,000 samples, and expectations of clear class separation. </p> <p>Avoid SVM for very large datasets where training time becomes prohibitive, when you need probability estimates and interpretability, or when simple linear methods already work well.</p>"},{"location":"04-core-ml/supervised-advanced/#random-forest","title":"Random Forest","text":"<p>Goal: Build many diverse decision trees and combine their predictions through voting to create a robust, accurate classifier.</p>"},{"location":"04-core-ml/supervised-advanced/#the-core-idea_1","title":"The Core Idea","text":"<p>A single decision tree overfits easily and produces unstable predictions where small changes in training data create completely different trees. Random Forest solves both problems by growing hundreds of trees on random subsets of data and features, then averaging their predictions. This ensemble approach reduces overfitting because errors from individual trees cancel out when averaged. It increases stability because no single tree dominates the final prediction.</p> <p>Each tree in the forest trains on a bootstrap sample, which randomly selects samples with replacement from the training set. This means each tree sees about 63% of the data, with some samples appearing multiple times and others not at all. At each split point, the tree considers only a random subset of features rather than all features. For classification, the typical subset size is the square root of the total number of features. These two sources of randomness, sample and feature randomness, ensure trees learn different patterns and make diverse predictions.</p> <p>The final classification uses majority voting. Each tree votes for spam or legitimate, and the class receiving the most votes wins. The proportion of trees voting for the winning class serves as a confidence score. If 85 of 100 trees predict spam, the model is 85% confident. This confidence helps flag borderline cases for manual review.</p> <p></p>"},{"location":"04-core-ml/supervised-advanced/#how-it-works","title":"How It Works","text":"<p>For our spam detection with 50 features and 8,000 training emails, Random Forest might build 100 trees as follows. Each tree receives a bootstrap sample of roughly 5,000 emails (some duplicates, some missing). At each decision node, the tree considers only 7 random features (\u221a50 \u2248 7) to find the best split. The tree grows fully without pruning, potentially memorizing its particular bootstrap sample.</p> <p>Tree 1 might focus on link count and sender domain patterns because those features appeared in its random subsets. Tree 2 emphasizes text features and send time. Tree 3 captures CAPSLOCK and urgent language interactions. The diversity means each tree is an expert on different aspects of spam detection. When a new email arrives, all 100 trees vote based on their specialized knowledge, and the majority decision combines these diverse perspectives into a robust prediction.</p> <p>This diversity-through-randomness principle is why Random Forest works so well in practice. Individual trees overfit their bootstrap samples, but their errors are uncorrelated. Some trees mistakenly classify a legitimate email as spam, but other trees correctly classify it. The majority vote cancels out these random errors, leaving only the signal that all trees agree on.</p>"},{"location":"04-core-ml/supervised-advanced/#implementation_1","title":"Implementation","text":"<p>Random Forest requires no feature scaling and works directly with the original features, making implementation simpler than SVM.</p> <pre><code>from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# Train Random Forest (no scaling needed!)\nmodel = RandomForestClassifier(\n    n_estimators=100,          # Number of trees\n    max_depth=15,              # Maximum tree depth\n    min_samples_split=20,      # Minimum samples to split\n    min_samples_leaf=10,       # Minimum samples per leaf\n    max_features='sqrt',       # \u221aK features per split\n    class_weight='balanced',   # Handle class imbalance\n    random_state=42,\n    n_jobs=-1                  # Parallel training\n)\n\nmodel.fit(X_train, y_train)\n\n# Predict\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test)\n\n# Evaluate\nprint(classification_report(y_test, predictions, \n                           target_names=['Legitimate', 'Spam']))\n</code></pre> <p>The <code>n_estimators</code> parameter sets the number of trees. More trees generally improve performance but with diminishing returns beyond <code>200-300</code> trees and increased memory and prediction time. Start with <code>100</code> trees and increase if cross-validation shows continued improvement. Training parallelizes across trees with <code>n_jobs=-1</code>, providing near-linear speedup on multi-core machines.</p> <p>Limiting tree depth through max_depth prevents individual trees from overfitting their bootstrap samples. Unlimited depth allows trees to grow until all leaves are pure, which overfits dramatically. Setting max_depth between 10 and 30 usually works well, creating trees deep enough to capture patterns but shallow enough to generalize. The min_samples_split and min_samples_leaf parameters provide alternative controls by preventing splits that create tiny nodes.</p>"},{"location":"04-core-ml/supervised-advanced/#feature-importance","title":"Feature Importance","text":"<p>Random Forest provides feature importance scores that reveal which features most influence predictions. Importance is computed by measuring how much each feature decreases impurity (Gini or entropy) when used for splitting, averaged across all trees and all nodes where that feature appears.</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Extract feature importances\nimportances = model.feature_importances_\nfeature_names = X.columns\n\n# Create importance dataframe\nimportance_df = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importances\n}).sort_values('importance', ascending=False)\n\n# Display top 15 features\nprint(importance_df.head(15))\n\n# Visualize\nplt.figure(figsize=(10, 8))\ntop_features = importance_df.head(15)\nplt.barh(top_features['feature'], top_features['importance'])\nplt.xlabel('Importance Score')\nplt.title('Top 15 Features for Spam Detection')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Sample output might show <code>sender_known (0.18)</code>, <code>domain_reputation (0.14)</code>, <code>num_links (0.12)</code>, and <code>capslock_ratio (0.09)</code> as the top features. This reveals that sender reputation matters most, followed by content characteristics. Features with near-zero importance can be removed to simplify the model and speed up training without sacrificing accuracy.</p> <p>These importance scores guide feature engineering efforts. If text-based features dominate, invest in better natural language processing. If metadata features rank low despite your intuition, investigate whether they contain useful signal or are just noise. Feature importance also helps explain predictions to stakeholders by highlighting which attributes most influence the spam versus legitimate decision.</p>"},{"location":"04-core-ml/supervised-advanced/#hyperparameter-tuning_1","title":"Hyperparameter Tuning","text":"<p>While Random Forest works well with default settings, tuning can improve performance by several percentage points. Focus on the parameters that most affect model complexity and diversity.</p> <pre><code>from sklearn.model_selection import RandomizedSearchCV\n\n# Define parameter distributions\nparam_dist = {\n    'n_estimators':,[11]\n    'max_depth': [10, 15, 20, 25, None],\n    'min_samples_split': ,\n    'min_samples_leaf': ,\n    'max_features': ['sqrt', 'log2', 0.3, 0.5]\n}\n\n# Randomized search (faster than grid search)\nrandom_search = RandomizedSearchCV(\n    RandomForestClassifier(random_state=42, n_jobs=-1),\n    param_dist,\n    n_iter=30,          # Try 30 random combinations\n    cv=5,\n    scoring='f1',\n    random_state=42,\n    n_jobs=-1\n)\n\nrandom_search.fit(X_train, y_train)\n\nprint(f\"Best parameters: {random_search.best_params_}\")\nprint(f\"Best F1-score: {random_search.best_score_:.3f}\")\n\n# Use best model\nbest_rf = random_search.best_estimator_\n</code></pre> <p>Randomized search samples 30 random combinations from the parameter space, which is more efficient than grid search when you have many parameters. This completes much faster while often finding equally good solutions. Increase n_iter if you have computational budget and want more thorough exploration.</p>"},{"location":"04-core-ml/supervised-advanced/#strengths-and-limitations_1","title":"Strengths and Limitations","text":"<p>Random Forest reduces overfitting compared to single decision trees through ensemble averaging. It handles non-linear patterns naturally without kernel tricks. No feature scaling is required, simplifying preprocessing. Feature importance scores provide interpretability despite the ensemble. The algorithm handles missing values internally and works well out-of-the-box with minimal tuning. Parallel training across trees makes it fast on multi-core systems.</p> <p>Limitations include reduced interpretability compared to a single tree since you cannot visualize 100 trees easily. Prediction is slower than simple models because every tree must evaluate the input. Memory requirements grow with the number of trees and their depth. While less prone to overfitting than single trees, Random Forest can still overfit with unlimited depth and too many trees. Text and very high-dimensional data may require feature reduction before Random Forest works well.</p> <p>Use Random Forest as your default choice for tabular classification problems. It works well without extensive tuning, handles feature interactions naturally, and provides feature importance for interpretation. It is particularly good when you have hundreds of features with unknown interactions, imbalanced classes that need weighting, and limited time for hyperparameter tuning. Avoid it when you need a simple, interpretable model for stakeholders or when prediction latency is critical.</p>"},{"location":"04-core-ml/supervised-advanced/#gradient-boosting","title":"Gradient Boosting","text":"<p>Goal: Build trees sequentially where each new tree corrects errors made by previous trees, creating a strong predictor from many weak learners.</p>"},{"location":"04-core-ml/supervised-advanced/#the-core-idea_2","title":"The Core Idea","text":"<p>Gradient Boosting takes a fundamentally different approach than Random Forest. Instead of building trees independently and voting, it builds trees sequentially where each tree learns to predict the residual errors of all previous trees. Start with a simple prediction like the majority class. Build a small tree that predicts where that initial model makes errors. Add that tree's predictions to the initial model. Build another tree to predict the remaining errors. Repeat this process hundreds of times, each iteration reducing the residual errors.</p> <p>The name \"gradient\" comes from using gradient descent to minimize a loss function, similar to training neural networks. Each new tree moves predictions in the direction that most reduces the loss. The name \"boosting\" refers to boosting the performance of weak learners, typically shallow trees with only a few splits. These weak learners are easy to train and generalize well individually, but ensemble them sequentially and they create powerful predictors.</p> <p>This sequential error-correction mechanism makes Gradient Boosting particularly effective on hard problems. Early trees learn the obvious patterns that simple methods would catch. Later trees focus on edge cases and subtle patterns that only appear in the residuals. By the 100th tree, the model has refined its predictions through a hundred rounds of error correction, achieving accuracy that no single model could match.</p> <p></p>"},{"location":"04-core-ml/supervised-advanced/#boosting-vs-bagging","title":"Boosting vs Bagging","text":"<p>Random Forest uses bagging, which builds independent models and averages their predictions. Each tree in a Random Forest trains on a bootstrap sample and has no knowledge of other trees. The diversity comes from random sampling. This parallel structure allows fast training but limits how much trees can learn from each other.</p> <p>Gradient Boosting uses boosting, which builds models sequentially where each model knows about all previous models. Tree 1 learns the main patterns. Tree 2 looks at what Tree 1 got wrong and tries to fix those errors. Tree 3 looks at what Trees 1 and 2 together still get wrong. This sequential structure means later trees become experts on hard cases that early trees miss. However, training must be sequential, making it slower than Random Forest.</p> <p>For our spam example, Tree 1 might learn \"unknown sender with many links is spam\" and catch 60% of spam emails. Tree 2 examines the 40% of spam Tree 1 missed and learns \"urgent language without links is also spam.\" Tree 3 focuses on false positives, learning \"known corporate senders with many links are legitimate.\" Each tree specializes in patterns the ensemble currently handles poorly.</p>"},{"location":"04-core-ml/supervised-advanced/#implementation_2","title":"Implementation","text":"<p>Gradient Boosting requires careful hyperparameter tuning to avoid overfitting since sequential error correction can eventually memorize training data.</p> <pre><code>from sklearn.ensemble import GradientBoostingClassifier\n\n# Train Gradient Boosting\nmodel = GradientBoostingClassifier(\n    n_estimators=100,         # Number of boosting stages\n    learning_rate=0.1,        # Shrinkage parameter\n    max_depth=3,              # Keep trees shallow (weak learners)\n    min_samples_split=20,\n    min_samples_leaf=10,\n    subsample=0.8,            # Use 80% of data per tree\n    max_features='sqrt',      # Feature sampling\n    random_state=42\n)\n\nmodel.fit(X_train, y_train)\n\n# Predict\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test)\n</code></pre> <p>The learning_rate controls how much each tree contributes to the final prediction. Small learning rates like 0.01 require more trees but often generalize better. Large learning rates like 0.3 converge faster but may overfit. A common strategy uses learning_rate=0.1 with n_estimators=100 as a starting point, then adjusts the pair since their product determines total model capacity.</p> <p>Keeping trees shallow through max_depth=3 is critical for boosting. Deep trees memorize data, while shallow trees learn simple patterns that generalize. Boosting combines many shallow trees to build complexity gradually. Trees with 3 levels (max_depth=3) can encode 8 rules, enough to capture meaningful patterns without overfitting.</p> <p>The subsample parameter adds randomness by training each tree on a random 80% subset of data. This stochastic gradient boosting reduces overfitting and speeds up training. Combined with max_features for random feature selection, it brings some of Random Forest's diversity benefits to boosting.</p>"},{"location":"04-core-ml/supervised-advanced/#hyperparameter-tuning_2","title":"Hyperparameter Tuning","text":"<p>Gradient Boosting has many hyperparameters that interact in complex ways. Effective tuning proceeds in stages, fixing some parameters while searching over others.</p> <pre><code>from sklearn.model_selection import GridSearchCV\n\n# Stage 1: Find optimal learning_rate and n_estimators\nparam_grid_1 = {\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'n_estimators':[11]\n}\n\ngrid_1 = GridSearchCV(\n    GradientBoostingClassifier(max_depth=3, random_state=42),\n    param_grid_1,\n    cv=5,\n    scoring='f1',\n    n_jobs=-1\n)\ngrid_1.fit(X_train, y_train)\nbest_lr = grid_1.best_params_['learning_rate']\nbest_n = grid_1.best_params_['n_estimators']\n\n# Stage 2: Tune tree structure with best lr and n\nparam_grid_2 = {\n    'max_depth': ,\n    'min_samples_split': ,\n    'min_samples_leaf': \n}\n\ngrid_2 = GridSearchCV(\n    GradientBoostingClassifier(\n        learning_rate=best_lr,\n        n_estimators=best_n,\n        random_state=42\n    ),\n    param_grid_2,\n    cv=5,\n    scoring='f1',\n    n_jobs=-1\n)\ngrid_2.fit(X_train, y_train)\n\nprint(f\"Best parameters: {grid_2.best_params_}\")\nfinal_model = grid_2.best_estimator_\n</code></pre> <p>This two-stage approach reduces the search space by first finding the right learning rate and number of trees, then optimizing tree structure. Searching all parameters simultaneously would require testing hundreds of combinations, which is computationally expensive.</p>"},{"location":"04-core-ml/supervised-advanced/#strengths-and-limitations_2","title":"Strengths and Limitations","text":"<p>Gradient Boosting often achieves the best performance on tabular data because sequential error correction captures subtle patterns. It handles mixed feature types, missing values, and feature interactions well. Built-in feature importance helps interpret which attributes matter most. The algorithm is less prone to overfitting than Random Forest when properly tuned, especially with small learning rates and early stopping.</p> <p>However, training is slow because trees must be built sequentially rather than in parallel. The many interacting hyperparameters require careful tuning through cross-validation. It is easy to overfit without proper validation monitoring and early stopping. The model is less interpretable than Random Forest because predictions involve summing hundreds of tree predictions rather than simple voting. Gradient Boosting is more sensitive to hyperparameter choices than Random Forest, meaning default settings often underperform.</p> <p>Use Gradient Boosting when you need maximum accuracy and have time for tuning, work with tabular data that has complex patterns, can afford slower training in exchange for better predictions, and plan to deploy in production where prediction speed matters more than training speed. Avoid it when you need quick results without tuning, have very large datasets where sequential training is prohibitive, or prioritize model interpretability for stakeholder communication.</p>"},{"location":"04-core-ml/supervised-advanced/#xgboost","title":"XGBoost","text":"<p>Goal: Highly optimized implementation of gradient boosting with built-in regularization, parallel tree construction, and advanced features for production deployment.</p>"},{"location":"04-core-ml/supervised-advanced/#what-makes-xgboost-special","title":"What Makes XGBoost Special","text":"<p>XGBoost (eXtreme Gradient Boosting) revolutionized machine learning competitions and production systems by making gradient boosting faster, more accurate, and easier to use. It introduces several innovations beyond standard gradient boosting. Parallel tree construction speeds up training by 10-100\u00d7 compared to scikit-learn's implementation. Built-in L1 and L2 regularization prevents overfitting more effectively. Automatic handling of missing values eliminates preprocessing steps. Cross-validation and early stopping integrate into the training process. Custom loss functions enable optimization for business-specific metrics.</p> <p>These improvements transformed gradient boosting from an academic technique into an industry standard. Kaggle competitions are dominated by XGBoost solutions. Tech companies deploy it at scale for ranking, recommendation, and fraud detection. The library continues active development with GPU support, distributed training, and integration with modern ML platforms.</p> <p></p>"},{"location":"04-core-ml/supervised-advanced/#core-differences-from-standard-gradient-boosting","title":"Core Differences from Standard Gradient Boosting","text":"<p>Standard Gradient Boosting builds trees one at a time in strict sequence. XGBoost parallelizes tree construction by evaluating all possible splits simultaneously across CPU cores, dramatically reducing training time. It adds regularization terms to the loss function that penalize complex trees, preventing the overfitting that plagues standard implementations. The algorithm handles missing values by learning the optimal default direction for each split rather than requiring imputation.</p> <p>The software engineering is exceptional. XGBoost implements cache-aware algorithms that minimize memory access patterns. It uses sparsity-aware split finding that efficiently handles sparse features common in text and categorical data. Out-of-core computation allows training on datasets larger than memory. These optimizations make XGBoost practical for production systems processing millions of examples.</p>"},{"location":"04-core-ml/supervised-advanced/#implementation_3","title":"Implementation","text":"<p>XGBoost offers two APIs. The native API provides maximum control and performance. The scikit-learn compatible API enables drop-in replacement for existing pipelines.</p> <pre><code>import xgboost as xgb\nfrom sklearn.metrics import f1_score, classification_report\n\n# Prepare data in DMatrix format (optimized for XGBoost)\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# Set parameters\nparams = {\n    'objective': 'binary:logistic',     # Binary classification\n    'eval_metric': 'logloss',           # Loss to optimize\n    'max_depth': 6,                     # Tree depth\n    'learning_rate': 0.1,               # Eta (step size)\n    'subsample': 0.8,                   # Row sampling per tree\n    'colsample_bytree': 0.8,            # Column sampling per tree\n    'reg_alpha': 0.1,                   # L1 regularization\n    'reg_lambda': 1.0,                  # L2 regularization\n    'scale_pos_weight': 5,              # Handle imbalance (neg/pos ratio)\n    'seed': 42\n}\n\n# Train with early stopping\nevals = [(dtrain, 'train'), (dtest, 'validation')]\nmodel = xgb.train(\n    params,\n    dtrain,\n    num_boost_round=1000,               # Maximum trees\n    evals=evals,\n    early_stopping_rounds=20,           # Stop if no improvement\n    verbose_eval=50                     # Print every 50 rounds\n)\n\n# Predict\ny_pred_proba = model.predict(dtest)\ny_pred = (y_pred_proba &gt; 0.5).astype(int)\n\nprint(classification_report(y_test, y_pred, target_names=['Legitimate', 'Spam']))\n</code></pre> <p>The DMatrix format stores data in XGBoost's optimized internal representation, reducing memory and speeding up training. The params dictionary controls all aspects of training. The evals list specifies datasets to monitor during training, enabling early stopping when validation performance plateaus. Training prints progress every 50 rounds, showing how loss decreases.</p> <p>The scikit-learn API offers familiar syntax for quick experiments:</p> <pre><code>from xgboost import XGBClassifier\n\n# Train with sklearn API\nmodel = XGBClassifier(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.1,\n    reg_lambda=1.0,\n    scale_pos_weight=5,\n    random_state=42,\n    n_jobs=-1,\n    eval_metric='logloss'\n)\n\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_test, y_test)],\n    early_stopping_rounds=20,\n    verbose=False\n)\n\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test)\n</code></pre> <p>Both APIs produce equivalent models. Use the native API for maximum performance and access to all features. Use the scikit-learn API for compatibility with existing pipelines and tools like GridSearchCV.</p>"},{"location":"04-core-ml/supervised-advanced/#key-hyperparameters","title":"Key Hyperparameters","text":"<p>The <code>max_depth</code> parameter controls tree complexity. Deeper trees capture more interactions but overfit more easily. Start with 6 and decrease to 3-4 if overfitting or increase to 8-10 if underfitting. Unlimited depth almost always overfits, so always set a limit.</p> <p>The <code>learning_rate</code> (also called eta) determines how much each tree contributes. Smaller learning rates like 0.01 require more trees but generalize better. Larger rates like 0.3 converge faster but may overfit. The standard approach uses 0.1 with 100-300 trees, then decreases learning rate and increases trees if you have more computational budget.</p> <p>Sampling parameters add randomness to reduce overfitting. The <code>subsample</code> parameter trains each tree on a random fraction of rows, typically 0.6-0.9. The <code>colsample_bytree</code> parameter samples columns once per tree, while <code>colsample_bylevel</code> samples columns at each tree level. These create diversity similar to Random Forest while maintaining boosting's error-correction advantage.</p> <p>Regularization parameters penalize complex models. The <code>reg_alpha</code> parameter adds L1 regularization that encourages sparsity, pushing some feature weights to zero. The <code>reg_lambda</code> parameter adds L2 regularization that shrinks all weights toward zero. Increase these when overfitting. Typical values range from 0 (no regularization) to 10 (strong regularization).</p> <p>The <code>scale_pos_weight</code> parameter handles class imbalance by weighting positive examples more heavily. Set it to the ratio of negative to positive examples. For 5% spam in your dataset, use <code>scale_pos_weight=19</code> to weight spam 19\u00d7 more than legitimate emails. This helps the model focus on learning the minority class.</p>"},{"location":"04-core-ml/supervised-advanced/#hyperparameter-tuning-strategy","title":"Hyperparameter Tuning Strategy","text":"<p>Tuning XGBoost effectively requires an organized approach because the parameter space is large and parameters interact.</p> <pre><code>from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform, randint\n\n# Define parameter distributions\nparam_dist = {\n    'max_depth': randint(3, 10),\n    'learning_rate': uniform(0.01, 0.29),  # 0.01 to 0.3\n    'n_estimators': randint(100, 500),\n    'subsample': uniform(0.6, 0.3),        # 0.6 to 0.9\n    'colsample_bytree': uniform(0.6, 0.3),\n    'reg_alpha': uniform(0, 10),\n    'reg_lambda': uniform(0.1, 10),\n    'scale_pos_weight': uniform(1, 20)\n}\n\n# Randomized search\nsearch = RandomizedSearchCV(\n    XGBClassifier(random_state=42, n_jobs=-1),\n    param_dist,\n    n_iter=50,                # Try 50 random combinations\n    cv=5,\n    scoring='f1',\n    random_state=42,\n    n_jobs=-1,\n    verbose=1\n)\n\nsearch.fit(X_train, y_train)\n\nprint(f\"Best parameters: {search.best_params_}\")\nprint(f\"Best cross-val F1: {search.best_score_:.3f}\")\n\n# Evaluate on test set\nbest_model = search.best_estimator_\ntest_predictions = best_model.predict(X_test)\nprint(f\"Test F1: {f1_score(y_test, test_predictions):.3f}\")\n</code></pre> <p>This randomized search samples 50 combinations from continuous and discrete distributions, running 250 total training jobs with 5-fold cross-validation. Continuous parameters use uniform distributions while discrete parameters use randint. This approach finds good hyperparameters much faster than exhaustive grid search.</p> <p>After randomized search finds a promising region, you can run a finer grid search around those values. For example, if randomized search finds max_depth=5 works well, search [4, 5, 6] more carefully. This two-stage coarse-then-fine approach balances exploration and exploitation.</p>"},{"location":"04-core-ml/supervised-advanced/#feature-importance_1","title":"Feature Importance","text":"<p>XGBoost provides multiple feature importance metrics that reveal different aspects of how features contribute to predictions.</p> <pre><code>import matplotlib.pyplot as plt\n\n# Get importance by weight (number of times feature used)\nimportance_weight = model.get_score(importance_type='weight')\n\n# Get importance by gain (average gain when feature used)\nimportance_gain = model.get_score(importance_type='gain')\n\n# Get importance by cover (average coverage of samples)\nimportance_cover = model.get_score(importance_type='cover')\n\n# Visualize gain-based importance\nxgb.plot_importance(model, importance_type='gain', max_num_features=15)\nplt.title('Top 15 Features by Average Gain')\nplt.tight_layout()\nplt.show()\n\n# Convert to DataFrame for analysis\nimport pandas as pd\nimportance_df = pd.DataFrame({\n    'feature': importance_gain.keys(),\n    'gain': importance_gain.values()\n}).sort_values('gain', ascending=False)\n\nprint(importance_df.head(15))\n</code></pre> <p>Weight importance counts how many times a feature appears in split conditions across all trees. This metric identifies frequently used features but doesn't distinguish between important and trivial splits. Gain importance measures the average improvement in loss when the feature is used for splitting. This better reflects which features actually improve predictions. Cover importance counts the average number of samples affected by splits on the feature, revealing features that influence many predictions even if they don't always improve the loss much.</p>"},{"location":"04-core-ml/supervised-advanced/#strengths-and-limitations_3","title":"Strengths and Limitations","text":"<p>XGBoost often achieves the best accuracy on structured data through sophisticated regularization and optimization. Fast training and prediction enable deployment in production systems processing millions of requests. Built-in handling of missing values eliminates preprocessing. Integrated cross-validation and early stopping prevent overfitting. </p> <p>Limitations include the complexity of tuning many interacting hyperparameters. Installation can be tricky on some systems due to compiler requirements. The model is less interpretable than Random Forest despite feature importance scores. Default hyperparameters may underperform compared to properly tuned settings. XGBoost is overkill for small, simple datasets where Logistic Regression works fine.</p> <p>Use XGBoost when you need maximum accuracy on structured data, have datasets with more than 10,000 samples, need production-ready performance with fast predictions, work with imbalanced classes, and have time for hyperparameter tuning. It is the default choice for Kaggle competitions and many production ML systems. Avoid it for very small datasets under 1,000 samples where simpler methods work better, when you need maximum interpretability for stakeholders, or when default hyperparameters must work well without tuning.</p>"},{"location":"04-core-ml/supervised-advanced/#comparing-advanced-methods","title":"Comparing Advanced Methods","text":""},{"location":"04-core-ml/supervised-advanced/#performance-summary","title":"Performance Summary","text":"<p>Real-world performance on our 50-feature spam detection problem shows clear patterns. Logistic Regression as the baseline achieves 82% F1-score in 1 second training time, establishing that the problem benefits from more sophisticated methods. A single Decision Tree reaches only 78% F1 in 2 seconds, overfitting badly despite regularization. Random Forest improves to 89% F1 in 15 seconds, showing the power of ensemble methods. SVM with RBF kernel achieves 88% F1 but requires 45 seconds due to the dataset size. Standard Gradient Boosting reaches 91% F1 in 60 seconds of sequential training. XGBoost wins with 92% F1 in only 20 seconds thanks to parallelization and optimization.</p> <p>These results demonstrate that advanced methods deliver meaningful improvements. The 10 percentage point gain from Logistic Regression to XGBoost means thousands fewer errors on a production system processing millions of emails. However, the diminishing returns from Random Forest (89%) to XGBoost (92%) suggest that further improvement requires substantial additional effort through better features or ensemble stacking.</p>"},{"location":"04-core-ml/supervised-advanced/#algorithm-selection-guide","title":"Algorithm Selection Guide","text":"<p>Choose SVM when you have non-linear patterns that kernel transformations can separate, high-dimensional sparse data like text features, medium-sized datasets under 50,000 samples where training time is acceptable, and clear class separation in feature space. SVM excels in applications like text classification, bioinformatics with sequence data, and image classification with engineered features.</p> <p>Choose Random Forest as your default starting point for tabular data. It works well without extensive tuning, provides feature importance for interpretation, handles hundreds of features naturally, and trains quickly through parallelization. Use it when you need robust performance quickly, have mixed feature types, want to understand feature contributions, or lack time for hyperparameter tuning.</p> <p>Choose Gradient Boosting when you need the best possible accuracy, have time for careful hyperparameter tuning, work with structured tabular data, and can afford slower training for better predictions. It is particularly effective when Random Forest underperforms, you have carefully curated features, and production latency is more important than training time.</p> <p>Choose XGBoost for production systems requiring maximum accuracy with fast predictions, large datasets over 10,000 samples, imbalanced classes common in fraud detection and rare event prediction, and scenarios where regularization prevents overfitting. It dominates Kaggle competitions and production deployments at major tech companies.</p>"},{"location":"04-core-ml/supervised-advanced/#quick-decision-tree","title":"Quick Decision Tree","text":"<p>Start with a simple Logistic Regression baseline to establish performance without complex methods. If accuracy is sufficient and the problem is simple, deploy it. If baseline accuracy is insufficient, try Random Forest next because it requires minimal tuning and works well across diverse problems. If Random Forest provides good accuracy, deploy it unless you need better performance. If you need higher accuracy and have computational budget, tune XGBoost hyperparameters through cross-validation. If XGBoost still doesn't meet requirements, investigate better features, ensemble methods, or deep learning.</p> <p>This progression builds complexity only when simpler methods fail, avoiding the trap of using advanced techniques unnecessarily. Many production systems run on Random Forest or simple XGBoost configurations because the extra accuracy from perfect tuning doesn't justify the engineering cost.</p>"},{"location":"04-core-ml/supervised-advanced/#ensemble-stacking","title":"Ensemble Stacking","text":""},{"location":"04-core-ml/supervised-advanced/#combining-multiple-models","title":"Combining Multiple Models","text":"<p>Ensemble stacking combines predictions from diverse models to achieve better performance than any single model. The key insight is that different algorithms make different types of errors. Random Forest might excel with noisy features while XGBoost captures subtle interactions. SVM handles high-dimensional patterns that tree methods miss. Averaging their predictions often yields better results than picking the best individual model.</p> <p>Simple voting averages predictions or probabilities across models. Train a Random Forest, XGBoost, and SVM independently. For each test example, get predictions from all three. Use majority voting for hard predictions or average probabilities for soft voting. This approach works best when models are diverse and roughly equally accurate.</p> <pre><code>from sklearn.ensemble import VotingClassifier\n\n# Define base models\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nxgb_model = XGBClassifier(n_estimators=100, random_state=42)\nsvm = SVC(kernel='rbf', probability=True, random_state=42)\n\n# Create voting ensemble\nvoting_clf = VotingClassifier(\n    estimators=[\n        ('rf', rf),\n        ('xgb', xgb_model),\n        ('svm', svm)\n    ],\n    voting='soft'  # Average probabilities\n)\n\nvoting_clf.fit(X_train, y_train)\npredictions = voting_clf.predict(X_test)\n</code></pre> <p>Stacking uses a meta-learner trained on base model predictions. Train multiple base models on the training data. Use cross-validation to generate predictions on the training set without leakage. Train a meta-model like Logistic Regression on these predictions. For test examples, get predictions from base models and feed them to the meta-model.</p> <pre><code>from sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Define base models\nbase_models = [\n    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n    ('xgb', XGBClassifier(n_estimators=100, random_state=42)),\n    ('svm', SVC(kernel='rbf', probability=True, random_state=42))\n]\n\n# Create stacking ensemble\nstacking_clf = StackingClassifier(\n    estimators=base_models,\n    final_estimator=LogisticRegression(),\n    cv=5  # Cross-validation for meta-features\n)\n\nstacking_clf.fit(X_train, y_train)\npredictions = stacking_clf.predict(X_test)\n</code></pre> <p>Stacking typically gains 1-3 percentage points over the best base model. Use it when you have computational resources for training multiple models, need maximum possible accuracy, and work on competitions or critical applications where small gains matter. Avoid it when interpretability matters, production latency constraints are tight, or maintaining multiple models creates operational complexity.</p>"},{"location":"04-core-ml/supervised-advanced/#best-practices","title":"Best Practices","text":"<p>Always start with a simple baseline before trying advanced methods. Train Logistic Regression first to establish performance without complexity. This baseline tells you whether the problem truly needs sophisticated techniques. If Logistic Regression achieves 95% accuracy, XGBoost might reach 97%, but that 2% gain may not justify the added complexity.</p> <p>Use cross-validation instead of single train-test splits for reliable performance estimates. Five-fold cross-validation runs five training jobs and averages results, providing confidence intervals around your metrics. This prevents overfitting to a particular train-test split during hyperparameter tuning.</p> <p>Monitor training versus validation performance to detect overfitting. If training accuracy is 99% but validation is 85%, your model memorizes rather than generalizes. Increase regularization, reduce model complexity, or collect more data. For tree-based methods, plot training and validation learning curves to see where overfitting begins.</p> <p>Feature scaling matters for distance-based methods like SVM but not for tree-based methods. Always scale features for SVM using StandardScaler. Random Forest, Gradient Boosting, and XGBoost work directly with original features, simplifying preprocessing.</p> <p>Invest time in hyperparameter tuning for production systems where small accuracy gains have large business impact. Use RandomizedSearchCV to explore the parameter space efficiently, then GridSearchCV to refine promising regions. Track all experiments in a spreadsheet or MLflow to avoid repeating failed configurations.</p> <p>Balance accuracy against latency in production systems. XGBoost with 500 trees might be 1% more accurate than 100 trees but 5\u00d7 slower at prediction time. If your system processes millions of requests, that latency multiplies to unacceptable delays. Profile prediction time and choose the fastest model that meets accuracy requirements.</p>"},{"location":"04-core-ml/supervised-advanced/#summary-and-next-steps","title":"Summary and Next Steps","text":"<p>Key accomplishments: You have learned when simple classifiers are insufficient and advanced methods provide value. You mastered Support Vector Machines for non-linear boundaries through kernel transformations. You understood Random Forest as an ensemble of diverse trees that vote on predictions. You explored Gradient Boosting's sequential error correction and XGBoost's optimized implementation. You compared methods to choose appropriate algorithms for different scenarios. You practiced hyperparameter tuning through cross-validation.</p> <p></p> <p>Critical insights: No single algorithm dominates all problems, so match method to problem characteristics. Start simple with Logistic Regression, escalate to Random Forest, then XGBoost only when needed. Proper hyperparameter tuning improves performance by 5-15%, making it worthwhile for production systems. Ensemble methods reduce both bias and variance through diversity and averaging. XGBoost's engineering excellence makes it the industry standard for structured data.</p> <p>Connections: These advanced methods build on foundations from the simple classification tutorial, requiring the same evaluation metrics and handling imbalanced classes with the same techniques. The data preprocessing tutorial is critical because feature engineering often improves performance more than switching algorithms. These same algorithms work for regression by changing the objective function, as covered in the regression tutorial.</p> <p>What's next: Learn model deployment patterns for serving predictions in production systems at scale. Explore AutoML frameworks that automate hyperparameter tuning and model selection. Study deep learning for images, text, and sequences where neural networks outperform tree-based methods. Investigate model interpretability techniques like SHAP values for explaining individual predictions.</p> <p>External resources: The XGBoost documentation at xgboost.readthedocs.io provides comprehensive coverage of parameters and techniques. Scikit-learn's ensemble methods guide at scikit-learn.org covers Random Forest and Gradient Boosting implementation details. Kaggle competitions at kaggle.com offer practice on real problems where these techniques shine.</p> <p>Remember that advanced methods provide 5-15% improvement over simple baselines. Make sure you need that improvement before adding complexity. Random Forest and XGBoost should be your defaults for tabular data, but always compare against Logistic Regression to know whether the improvement justifies the cost.</p>"},{"location":"04-core-ml/supervised/","title":"Classification: Predicting Categories with Simple Algorithms","text":"<p>This tutorial introduces classification, the supervised learning technique for predicting categories. You'll learn four foundational algorithms, understand when to use each one, and master evaluation metrics using an email spam detection example that connects every concept to business decisions.</p> <p>Estimated time: 50 minutes</p>"},{"location":"04-core-ml/supervised/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement:</p> <p>Business decisions often require categorizing: \"Will this customer churn? Is this transaction fraudulent? Should we approve this loan?\"</p> <p>Categories drive yes/no decisions. A bank needs to classify loan applications as approved or rejected. A hospital needs to diagnose patients as healthy or sick. An email provider needs to separate spam from legitimate messages. Unlike regression that predicts numbers, classification assigns discrete labels that trigger specific actions: block the email, approve the loan, schedule the appointment.</p> <p>Practical benefits: Classification automates repetitive decisions, prioritizes limited resources, and reduces manual review costs. You can process thousands of cases instantly, focus human attention on borderline cases, and maintain consistent decision criteria. Every prediction comes with a confidence score, letting you set custom thresholds based on business costs.</p> <p>Professional context: Classification powers recommendation systems, fraud detection, medical diagnosis, customer segmentation, and quality control across industries. Most business rules (\"if condition X, then action Y\") can be learned as classification models from historical examples. Master these foundational algorithms, and you'll recognize when simple interpretable models outperform complex black boxes.</p> <p></p>"},{"location":"04-core-ml/supervised/#running-example-email-spam-detection","title":"Running Example: Email Spam Detection","text":"<p>Throughout this tutorial, we'll follow a small business email platform building a spam filter. They have <code>10,000</code> labeled emails with features extracted from content and metadata: word count, presence of urgent language, number of links, whether sender is known, attachment count, and ratio of UPPERCASE text.</p> <p>Business need: Reduce inbox clutter without blocking important business communications. False positives (blocking real emails) anger users more than false negatives (letting some spam through).</p> <p>Success metric:  - <code>Precision &gt; 90%</code> (minimize false positives; don't block real emails) - <code>Recall &gt; 80%</code> (catch most spam)</p> <p>This example anchors every algorithm in concrete, relatable decisions you can evaluate.</p>"},{"location":"04-core-ml/supervised/#core-concepts","title":"Core Concepts","text":""},{"location":"04-core-ml/supervised/#what-is-classification","title":"What Is Classification?","text":"<p>Classification predicts discrete categories based on input features. The model learns patterns that distinguish classes (spam vs legitimate) from labeled training examples, then applies those patterns to assign categories to new, unseen data.</p> <p>Key characteristics:</p> <ul> <li>Output is a category: Spam or Legitimate, not a spam probability score</li> <li>Learns from labeled examples: Historical emails with known labels train the model</li> <li>Finds decision boundaries: Discovers rules that separate classes in feature space</li> <li>Generalizes to new data: Classifies emails not in the training set</li> </ul> <p>Classification vs Regression:</p> Classification Regression Predicts categories Predicts numbers \"Which class?\" \u2192 Spam \"How much?\" \u2192 \u20ac850 Finite outputs (2-100 classes) Infinite possible outputs Accuracy, Precision, Recall MAE, RMSE, R\u00b2 <p>When to use classification:</p> <ul> <li>Target variable is categorical (spam/legitimate, approved/rejected)</li> <li>Need discrete decisions that trigger specific actions</li> <li>Understand which features separate classes</li> <li>Prioritize cases by confidence scores</li> </ul>"},{"location":"04-core-ml/supervised/#types-of-classification-problems","title":"Types of Classification Problems","text":"<p>Binary Classification: Two classes only.</p> <ul> <li>Email: spam vs legitimate</li> <li>Loan: approved vs rejected  </li> <li>Medical: positive vs negative test</li> </ul> <p>Multi-class Classification: More than two classes.</p> <ul> <li>Product category: electronics, clothing, food, books</li> <li>Customer segment: high-value, medium, low, churned</li> <li>Priority level: urgent, normal, low</li> </ul> <p>Multi-label Classification: Multiple labels per item.</p> <ul> <li>Article tags: politics + economy + international</li> <li>Movie genres: action + comedy + drama</li> <li>Skills: Python + SQL + Statistics</li> </ul> <p>This tutorial focuses on binary and multi-class; multi-label is covered in the advanced tutorial.</p>"},{"location":"04-core-ml/supervised/#real-world-applications","title":"Real-World Applications","text":"<p>Finance:</p> <ul> <li>Loan approval (approve/reject)</li> <li>Credit card fraud detection (fraudulent/legitimate)</li> <li>Customer churn prediction (will churn/won't churn)</li> </ul> <p>Healthcare:</p> <ul> <li>Disease diagnosis (positive/negative)</li> <li>Patient risk stratification (high/medium/low risk)</li> <li>Treatment recommendation (surgery/medication/observation)</li> </ul> <p>E-commerce:</p> <ul> <li>Product categorization (automatic tagging)</li> <li>Customer segmentation (targeting campaigns)</li> <li>Review sentiment (positive/negative/neutral)</li> </ul> <p>Operations:</p> <ul> <li>Quality control (defective/acceptable)</li> <li>Maintenance prediction (needs service/ok)</li> <li>Support ticket routing (billing/technical/account)</li> </ul> <p>Each application shares the same structure: historical labeled examples train a model that assigns categories to new cases.</p>"},{"location":"04-core-ml/supervised/#the-classification-workflow","title":"The Classification Workflow","text":"<p>Standard process:</p> <ol> <li>Load labeled data (features + known categories)</li> <li>Explore class distribution (balanced? imbalanced?)</li> <li>Split train/test sets (stratified to preserve class ratios)</li> <li>Train classifier on training set</li> <li>Predict categories for test set</li> <li>Evaluate with confusion matrix and metrics</li> <li>Tune and iterate</li> </ol> <p>Critical point: Always check class distribution before training. Imbalanced classes (95% legitimate, 5% spam) require special handling or your model will just predict the majority class for everything.</p>"},{"location":"04-core-ml/supervised/#simple-classification-algorithms","title":"Simple Classification Algorithms","text":""},{"location":"04-core-ml/supervised/#algorithm-1-logistic-regression","title":"Algorithm 1: Logistic Regression","text":"<p>Goal: Find the probability that an example belongs to each class using a linear combination of features.</p>"},{"location":"04-core-ml/supervised/#the-core-idea","title":"The Core Idea","text":"<p>Despite the name, logistic regression is for classification, not regression. It uses a sigmoid function to squeeze any linear combination of features into a probability between 0 and 1.</p> <p>Formula:</p> \\[ P(\\text{spam}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...)}} \\] <p>The sigmoid function \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) maps any value to (0,1).</p> <p></p> <p>Why start here: - Simple and interpretable (coefficients show feature importance) - Fast to train, works well on many problems - Outputs probabilities (useful for ranking or setting custom thresholds) - Similar to linear regression (easy to understand if you know that)</p>"},{"location":"04-core-ml/supervised/#email-spam-example","title":"Email Spam Example","text":"<p>Features: <code>num_links</code>, <code>capslock_ratio</code>, <code>word_count</code>, <code>has_urgent</code>, <code>sender_known</code></p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Prepare data\nX = emails[['num_links', 'capslock_ratio', 'word_count', 'has_urgent', 'sender_known']]\ny = emails['is_spam']  # 0 = legitimate, 1 = spam\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# Train\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test)  # Get probabilities for each class\n</code></pre> <p>Interpreting results:</p> <pre><code># Show feature coefficients\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"{feature}: {coef:.3f}\")\n</code></pre> <p>Sample output: <pre><code>num_links: 0.850\ncapslock_ratio: 1.200\nword_count: -0.002\nhas_urgent: 0.650\nsender_known: -2.100\n</code></pre></p> <p>Interpretation:</p> <ul> <li>Positive coefficient \u2192 increases spam probability</li> <li><code>num_links</code> (+0.85): More links \u2192 more likely spam</li> <li><code>capslock_ratio</code> (+1.2): More UPPERCASE \u2192 more likely spam</li> <li><code>has_urgent</code> (+0.65): Urgent language \u2192 more likely spam</li> <li>Negative coefficient \u2192 decreases spam probability</li> <li><code>sender_known</code> (-2.1): Known sender \u2192 very unlikely spam</li> <li><code>word_count</code> (-0.002): Longer emails slightly less likely spam</li> </ul> <p>Decision boundary: By default, <code>if P(spam) &gt; 0.5</code>, classify as spam. You can adjust this threshold based on business costs (e.g., require <code>P(spam) &gt; 0.7</code> to reduce false positives).</p> <p>Strengths: Simple, fast, interpretable, works well for linearly separable data, outputs calibrated probabilities</p> <p>Limitations: Assumes linear decision boundary (can't learn complex patterns like \"spam if many links AND unknown sender\")</p>"},{"location":"04-core-ml/supervised/#algorithm-2-k-nearest-neighbors-knn","title":"Algorithm 2: K-Nearest Neighbors (KNN)","text":"<p>Goal: Classify based on the majority vote of k closest training examples.</p>"},{"location":"04-core-ml/supervised/#the-core-idea_1","title":"The Core Idea","text":"<p>\"You are the average of your k closest neighbors.\" For a new email, find the k most similar emails in the training set (by feature similarity), then predict the majority class among those k neighbors.</p> <p>How it works:</p> <ol> <li>Choose k (e.g., k=5)</li> <li>For new email, calculate distance to all training emails</li> <li>Find 5 closest neighbors</li> <li>Count their classes (e.g., 3 spam, 2 legitimate)</li> <li>Predict majority class (spam)</li> </ol> <p></p> <p>Distance metric: Usually Euclidean distance. Important: Features must be scaled to the same range, or features with large values dominate distance calculations.</p>"},{"location":"04-core-ml/supervised/#email-spam-example_1","title":"Email Spam Example","text":"<p>New email: <code>10 links, 30% CAPSLOCK, 150 words, urgent=yes, known=no</code></p> <p>Find 5 training emails with most similar feature values. If 4 are spam and 1 is legitimate, predict spam.</p> <pre><code>from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale features (critical for KNN!)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train\nmodel = KNeighborsClassifier(n_neighbors=5)\nmodel.fit(X_train_scaled, y_train)\n\n# Predict\npredictions = model.predict(X_test_scaled)\n</code></pre> <p>Choosing k:</p> <ul> <li>Small k (1-3): Sensitive to noise, complex boundaries, overfits</li> <li>Large k (10-20): Smoother boundaries, less sensitive to outliers, underfits</li> <li>Rule of thumb: \\(k = \\sqrt{N}\\) where \\(N\\) is the number of training examples</li> <li>Best practice: Use cross-validation to find optimal k</li> </ul> <p>Example k values:</p> <pre><code>from sklearn.model_selection import cross_val_score\n\nfor k in :\n    model = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n    print(f\"k={k}: {scores.mean():.3f} \u00b1 {scores.std():.3f}\")\n</code></pre> <p>Strengths: No training time (just stores data), simple concept, no assumptions about data distribution, naturally handles multi-class</p> <p>Limitations: Slow predictions (must compare to all training points), sensitive to irrelevant features, requires feature scaling, memory-intensive for large datasets</p>"},{"location":"04-core-ml/supervised/#algorithm-3-naive-bayes","title":"Algorithm 3: Naive Bayes","text":"<p>Goal: Use probability theory to calculate the likelihood of each class given the features.</p>"},{"location":"04-core-ml/supervised/#the-core-idea_2","title":"The Core Idea","text":"<p>Uses Bayes' theorem to calculate the probability of each class given the observed features. Assumes features are independent (the \"naive\" assumption), which is often wrong but works surprisingly well in practice.</p> <p>Formula (Bayes' Theorem):</p> \\[ P(\\text{spam} | \\text{features}) = \\frac{P(\\text{features} | \\text{spam}) \\times P(\\text{spam})}{P(\\text{features})} \\] <p>Why \"naive\"? Assumes features are independent. For email: assumes <code>num_links</code> doesn't affect <code>capslock_ratio</code>, which isn't true (spam often has both). Despite this unrealistic assumption, Naive Bayes performs well for text classification.</p> <p></p>"},{"location":"04-core-ml/supervised/#email-spam-example_2","title":"Email Spam Example","text":"<p>Given features (many links, high CAPSLOCK, unknown sender), calculate:</p> <ul> <li><code>P(spam | features) = ?</code></li> <li><code>P(legitimate | features) = ?</code></li> </ul> <p>Predict the class with higher probability.</p> <pre><code>from sklearn.naive_bayes import GaussianNB\n\n# Train (no scaling needed!)\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\n# Predict\npredictions = model.predict(X_test)\nprobabilities = model.predict_proba(X_test)\n</code></pre> <p>Variants:</p> <ul> <li>GaussianNB: Assumes features follow normal distribution (use for continuous features)</li> <li>MultinomialNB: For count data (word frequencies in text)</li> <li>BernoulliNB: For binary features (presence/absence of words)</li> </ul> <p>When to use:</p> <ul> <li>Text classification (classic spam filtering)</li> <li>Many features, relatively small dataset</li> <li>Features are reasonably independent</li> <li>Need very fast training and prediction</li> </ul> <p>Strengths: Very fast training and prediction, works well with high-dimensional data, handles multi-class naturally, no hyperparameters to tune</p> <p>Limitations: Independence assumption often violated (but often works anyway), probability estimates can be poor, sensitive to how you transform features</p>"},{"location":"04-core-ml/supervised/#algorithm-4-decision-tree-classifier","title":"Algorithm 4: Decision Tree Classifier","text":"<p>Goal: Build a tree of yes/no questions to partition data into pure groups.</p>"},{"location":"04-core-ml/supervised/#the-core-idea_3","title":"The Core Idea","text":"<p>Creates a flowchart of if/else rules that splits data recursively until each leaf node is mostly one class. Each split chooses the feature and threshold that best separates classes.</p> <p>How it learns:</p> <p>The algorithm finds the best question at each step. \"Best\" means maximizing information gain (making child nodes purer than parent).</p> <p>Example tree structure:</p> <pre><code>Has &gt; 5 links?\n\u251c\u2500 Yes: capslock_ratio &gt; 20%?\n\u2502  \u251c\u2500 Yes: Predict SPAM (95% spam in this group)\n\u2502  \u2514\u2500 No: Predict LEGITIMATE (70% legitimate)\n\u2514\u2500 No: sender_known?\n   \u251c\u2500 Yes: Predict LEGITIMATE (98% legitimate)\n   \u2514\u2500 No: Predict SPAM (80% spam)\n</code></pre> <p></p>"},{"location":"04-core-ml/supervised/#email-spam-example_3","title":"Email Spam Example","text":"<p>Tree learns: \"If unknown sender AND many links AND high CAPSLOCK \u2192 spam\"</p> <pre><code>from sklearn.tree import DecisionTreeClassifier\n\n# Train\nmodel = DecisionTreeClassifier(max_depth=5, min_samples_split=50)\nmodel.fit(X_train, y_train)\n\n# Predict\npredictions = model.predict(X_test)\n\n# Visualize (optional)\nfrom sklearn.tree import plot_tree\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20,10))\nplot_tree(model, feature_names=X.columns, class_names=['Legitimate', 'Spam'], filled=True)\nplt.show()\n</code></pre> <p>Key hyperparameters:</p> <ul> <li><code>max_depth</code>: How many questions deep? (deeper = more complex, more overfitting)</li> <li>Too shallow: underfits (can't capture patterns)</li> <li>Too deep: overfits (memorizes training data)</li> <li>Typical values: 3-10</li> <li><code>min_samples_split</code>: Minimum examples in node to split further</li> <li>Larger value = simpler tree, less overfitting</li> <li>Typical values: 20-100</li> <li><code>min_samples_leaf</code>: Minimum examples in leaf node</li> <li>Prevents tiny, overfitted leaves</li> </ul> <p>Strengths: Highly interpretable (visualize as flowchart), handles non-linear patterns, no feature scaling needed, automatic feature selection</p> <p>Limitations: Prone to overfitting (memorizes training data), unstable (small data changes = different tree), biased toward features with many values</p>"},{"location":"04-core-ml/supervised/#evaluating-classification-models","title":"Evaluating Classification Models","text":"<p>Goal: Know if predictions are good enough for real-world deployment.</p>"},{"location":"04-core-ml/supervised/#the-confusion-matrix","title":"The Confusion Matrix","text":"<p>Purpose: Show all four outcomes\u2014correct and incorrect predictions for both classes.</p> Predicted Spam Predicted Legitimate Actual Spam TP (True Positive) FN (False Negative) Actual Legitimate FP (False Positive) TN (True Negative) <p>Email example:</p> <ul> <li>TP (True Positive): Correctly caught spam \u2192 Good!</li> <li>TN (True Negative): Correctly identified legitimate email \u2192 Good!</li> <li>FP (False Positive): Flagged real email as spam \u2192 Bad! User misses important message.</li> <li>FN (False Negative): Let spam through \u2192 Annoying but less critical than FP.</li> </ul> <p>Code:</p> <pre><code>from sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Get predictions\ny_pred = model.predict(X_test)\n\n# Create confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\n# Visualize\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n            xticklabels=['Legitimate', 'Spam'], \n            yticklabels=['Legitimate', 'Spam'])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix')\nplt.show()\n</code></pre> <p>Sample output:</p> <pre><code>[[850  30]   &lt;- 850 legitimate correctly identified, 30 misclassified as spam\n [ 45 175]]  &lt;- 175 spam caught, 45 spam missed\n</code></pre> <p></p>"},{"location":"04-core-ml/supervised/#key-metrics-derived-from-confusion-matrix","title":"Key Metrics (Derived from Confusion Matrix)","text":""},{"location":"04-core-ml/supervised/#1-accuracy","title":"1. Accuracy","text":"<p>What it means: Overall correctness\u2014what percentage of predictions are right?</p> <p>Formula:</p> \\[ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} \\] <p>Example: (850 + 175) / 1100 = 93.2%</p> <p>When to use: Classes are balanced (50/50 or close).</p> <p>Problem: Misleading with imbalanced data. If 95% of emails are legitimate and you predict \"all legitimate,\" accuracy is 95% but you catch zero spam!</p>"},{"location":"04-core-ml/supervised/#2-precision","title":"2. Precision","text":"<p>What it means: Of emails we flagged as spam, how many were actually spam?</p> <p>Formula:</p> \\[ \\text{Precision} = \\frac{TP}{TP + FP} \\] <p>Example: 175 / (175 + 30) = 85.4%</p> <p>Interpretation: When model says \"spam,\" it's right 85.4% of the time.</p> <p>When to prioritize: False positives are costly (blocking real emails).</p> <p>Email context: High precision = few false alarms. Users trust the spam filter.</p>"},{"location":"04-core-ml/supervised/#3-recall-sensitivity-true-positive-rate","title":"3. Recall (Sensitivity, True Positive Rate)","text":"<p>What it means: Of all actual spam, how many did we catch?</p> <p>Formula:</p> \\[ \\text{Recall} = \\frac{TP}{TP + FN} \\] <p>Example: 175 / (175 + 45) = 79.5%</p> <p>Interpretation: Model catches 79.5% of spam; 20.5% slips through.</p> <p>When to prioritize: False negatives are costly (missing disease, letting fraud through).</p> <p>Email context: High recall = catch most spam, clean inbox.</p>"},{"location":"04-core-ml/supervised/#4-f1-score","title":"4. F1-Score","text":"<p>What it means: Harmonic mean of precision and recall\u2014balances both.</p> <p>Formula:</p> \\[ F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\] <p>Example: 2 \u00d7 (0.854 \u00d7 0.795) / (0.854 + 0.795) = 82.3%</p> <p>When to use: You need balance between precision and recall, or classes are imbalanced.</p> <p>Why harmonic mean? Penalizes extreme imbalance. If precision is 90% but recall is 10%, F1 is only 18% (not the 50% that arithmetic mean would give).</p>"},{"location":"04-core-ml/supervised/#code-calculate-all-metrics","title":"Code: Calculate All Metrics","text":"<pre><code>from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n\ny_pred = model.predict(X_test)\n\nprint(f\"Accuracy:  {accuracy_score(y_test, y_pred):.2%}\")\nprint(f\"Precision: {precision_score(y_test, y_pred):.2%}\")\nprint(f\"Recall:    {recall_score(y_test, y_pred):.2%}\")\nprint(f\"F1-Score:  {f1_score(y_test, y_pred):.2%}\")\n\n# Detailed report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Legitimate', 'Spam']))\n</code></pre> <p>Sample output:</p> <pre><code>Accuracy:  93.18%\nPrecision: 85.37%\nRecall:    79.55%\nF1-Score:  82.35%\n\nClassification Report:\n              precision    recall  f1-score   support\n\n Legitimate       0.95      0.97      0.96       880\n        Spam       0.85      0.80      0.82       220\n\n    accuracy                           0.93      1100\n   macro avg       0.90      0.88      0.89      1100\nweighted avg       0.93      0.93      0.93      1100\n</code></pre>"},{"location":"04-core-ml/supervised/#which-metric-to-use","title":"Which Metric to Use?","text":"Situation Prioritize Example False positives costly Precision Email (don't block real emails) False negatives costly Recall Medical diagnosis (don't miss disease) Classes balanced Accuracy Coin flip prediction Need balance F1-Score General-purpose classification Classes imbalanced F1-Score or Precision + Recall Fraud detection (rare events) <p>Email spam decision:</p> <ul> <li>Prioritize precision (90%+): Users hate missing real emails</li> <li>Acceptable recall (80%+): Catching most spam is good enough</li> <li>Trade-off: Set threshold to P(spam) &gt; 0.7 instead of 0.5 to increase precision at cost of recall</li> </ul>"},{"location":"04-core-ml/supervised/#handling-imbalanced-classes","title":"Handling Imbalanced Classes","text":"<p>Problem: Real-world data is often imbalanced. If 95% of emails are legitimate and only 5% spam, a model that predicts \"all legitimate\" achieves 95% accuracy but is completely useless.</p>"},{"location":"04-core-ml/supervised/#solutions","title":"Solutions","text":""},{"location":"04-core-ml/supervised/#1-stratified-traintest-split","title":"1. Stratified Train/Test Split","text":"<p>Problem: Random split might put all spam in training set, none in test set.</p> <p>Solution: Preserve class ratios in both sets.</p> <pre><code>X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n</code></pre> <p>Now both train and test have 95% legitimate, 5% spam.</p>"},{"location":"04-core-ml/supervised/#2-class-weights","title":"2. Class Weights","text":"<p>Problem: Model optimizes overall accuracy, ignoring minority class.</p> <p>Solution: Penalize errors on minority class more heavily.</p> <pre><code># Automatically balance class weights\nmodel = LogisticRegression(class_weight='balanced')\nmodel.fit(X_train, y_train)\n</code></pre> <p>Internally calculates weights as <code>n_samples / (n_classes * class_count)</code>, making minority class errors more costly.</p>"},{"location":"04-core-ml/supervised/#3-resampling","title":"3. Resampling","text":"<p>Oversample minority class: Duplicate spam examples to balance classes.</p> <pre><code>from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n</code></pre> <p>SMOTE creates synthetic examples by interpolating between existing minority examples.</p> <p>Undersample majority class: Randomly remove legitimate examples to balance.</p> <pre><code>from imblearn.under_sampling import RandomUnderSampler\n\nundersampler = RandomUnderSampler(random_state=42)\nX_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n</code></pre> <p>Trade-offs: - Oversample: Retains all information but increases training time - Undersample: Fast but discards potentially useful data</p>"},{"location":"04-core-ml/supervised/#4-use-appropriate-metrics","title":"4. Use Appropriate Metrics","text":"<p>Never rely on accuracy for imbalanced data. Always check:</p> <ul> <li>Precision and recall for each class</li> <li>F1-score</li> <li>Confusion matrix</li> </ul>"},{"location":"04-core-ml/supervised/#comparing-algorithms","title":"Comparing Algorithms","text":""},{"location":"04-core-ml/supervised/#quick-reference-table","title":"Quick Reference Table","text":"Algorithm Speed Interpretability Handles Non-Linear Scales to Many Features Requires Scaling Logistic Regression Fast High (coefficients) No Yes Yes KNN Slow Medium (see neighbors) Yes No Yes Naive Bayes Very Fast Medium (probabilities) No Yes No Decision Tree Fast Very High (visualize tree) Yes Medium No"},{"location":"04-core-ml/supervised/#when-to-use-each","title":"When to Use Each","text":"Situation Algorithm Why Need interpretability Decision Tree or Logistic Regression Visualize tree or inspect coefficients Text/word features Naive Bayes Designed for high-dimensional sparse data Small dataset (&lt;10k samples) KNN or Naive Bayes No training needed (KNN) or works with little data Need probabilities Logistic Regression or Naive Bayes Calibrated probability outputs Non-linear patterns Decision Tree or KNN Handle complex boundaries Large dataset (&gt;100k samples) Logistic Regression or Naive Bayes Scale efficiently Real-time predictions Naive Bayes or Logistic Regression Fastest prediction time"},{"location":"04-core-ml/supervised/#email-spam-recommendation","title":"Email Spam Recommendation","text":"<p>Start with: Logistic Regression (simple baseline) Try next: Naive Bayes (classic for text/email) If need interpretability: Decision Tree (show users why email was flagged) If small dataset: KNN (simple, no training)</p> <p>Typical performance order for spam:</p> <ol> <li>Naive Bayes (80-85% F1) \u2014 designed for text</li> <li>Logistic Regression (75-80% F1) \u2014 solid baseline</li> <li>Decision Tree (70-75% F1) \u2014 interpretable but overfits</li> <li>KNN (65-70% F1) \u2014 struggles with high dimensions</li> </ol>"},{"location":"04-core-ml/supervised/#best-practices","title":"Best Practices","text":"<p>Always stratify splits when classes are imbalanced to preserve ratios in train and test sets.</p> <p>Scale features for Logistic Regression and KNN (they're sensitive to feature magnitude). Not needed for Naive Bayes or Decision Tree.</p> <p>Cross-validate to get reliable performance estimates instead of single train/test split.</p> <pre><code>from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\nprint(f\"F1-Score: {scores.mean():.3f} \u00b1 {scores.std():.3f}\")\n</code></pre> <p>Check confusion matrix before trusting accuracy, especially with imbalanced data.</p> <p>Start simple (Logistic Regression) before trying complex models. Simple models often work well and are easier to debug and deploy.</p> <p>Interpret models to understand what they learned. Check coefficients (Logistic Regression) or visualize trees (Decision Tree) to catch data leakage or spurious correlations.</p> <p>Test on realistic data that matches production distribution. If production has 10% spam but test set has 50%, results are misleading.</p> <p>Set thresholds based on business costs. Default threshold is 0.5, but if false positives cost 10\u00d7 more than false negatives, increase threshold to 0.7 or 0.8.</p>"},{"location":"04-core-ml/supervised/#common-pitfalls","title":"Common Pitfalls","text":"<p>Class imbalance ignored: High accuracy but minority class never predicted. Always check precision and recall for each class separately.</p> <p>No feature scaling: KNN and Logistic Regression fail when features have different scales (e.g., word count 0-1000 vs capslock ratio 0-1).</p> <p>Overfitting: Decision trees too deep memorize training data. Limit <code>max_depth</code> to 5-10 and set <code>min_samples_split</code> to 20-50.</p> <p>Wrong metric: Optimizing accuracy when precision matters (or vice versa). Choose metric that matches business costs.</p> <p>Data leakage: Including features that contain target information. Example: including \"spam_score\" feature when predicting spam label.</p> <p>Not stratifying: Random split creates imbalanced train/test sets, making evaluation unreliable.</p> <p>Assuming calibrated probabilities: predict_proba outputs aren't always true probabilities. Logistic Regression is well-calibrated; Decision Tree and Naive Bayes are not.</p>"},{"location":"04-core-ml/supervised/#complete-example-workflow","title":"Complete Example Workflow","text":"<pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 1. Load data\nemails = pd.read_csv('emails.csv')\nX = emails[['num_links', 'capslock_ratio', 'word_count', 'has_urgent', 'sender_known']]\ny = emails['is_spam']\n\n# 2. Check class distribution\nprint(y.value_counts(normalize=True))\n# Output: 0 (legitimate): 85%, 1 (spam): 15%\n\n# 3. Stratified split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, stratify=y, random_state=42\n)\n\n# 4. Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# 5. Train with class weights\nmodel = LogisticRegression(class_weight='balanced', random_state=42)\nmodel.fit(X_train_scaled, y_train)\n\n# 6. Cross-validate on training set\ncv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='f1')\nprint(f\"Cross-val F1: {cv_scores.mean():.3f} \u00b1 {cv_scores.std():.3f}\")\n\n# 7. Predict on test set\ny_pred = model.predict(X_test_scaled)\n\n# 8. Evaluate\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Legitimate', 'Spam']))\n\n# 9. Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Legitimate', 'Spam'],\n            yticklabels=['Legitimate', 'Spam'])\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix')\nplt.show()\n\n# 10. Interpret coefficients\nprint(\"\\nFeature Importance:\")\nfor feature, coef in zip(X.columns, model.coef_):\n    print(f\"{feature}: {coef:.3f}\")\n\n# 11. Adjust threshold if needed\ny_proba = model.predict_proba(X_test_scaled)[:, 1]  # Spam probabilities\ny_pred_custom = (y_proba &gt; 0.7).astype(int)  # Custom threshold\nprint(\"\\nWith threshold=0.7:\")\nprint(classification_report(y_test, y_pred_custom, target_names=['Legitimate', 'Spam']))\n</code></pre>"},{"location":"04-core-ml/supervised/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned the difference between classification and regression, mastered four foundational algorithms (Logistic Regression, KNN, Naive Bayes, Decision Tree), understood confusion matrix and metrics (Precision, Recall, F1), handled imbalanced classes with stratification and class weights, and compared algorithms to choose appropriate ones for problems.</p> <p>Critical insights:</p> <ul> <li>Start simple: Logistic Regression often works as well as complex models</li> <li>Metric matters: Choose precision, recall, or F1 based on business costs, not just accuracy</li> <li>Check the matrix: Confusion matrix reveals what accuracy hides</li> <li>Balance classes: Stratify splits and use class weights for imbalanced data</li> <li>Interpret models: Understand what features drive predictions to catch errors</li> </ul> <p>Connections:</p> <ul> <li>Data Preprocessing tutorial: Feature scaling essential for Logistic Regression and KNN</li> <li>Regression tutorial: Similar scikit-learn API but different output (categories vs numbers)</li> <li>Visualization tutorial: Plot confusion matrices, ROC curves, decision boundaries</li> </ul> <p>What's next:</p> <ul> <li>Advanced Classification tutorial: Ensemble methods (Random Forest, Gradient Boosting), SVM, Neural Networks</li> <li>Model tuning: Hyperparameter optimization with GridSearchCV, cross-validation strategies</li> <li>Feature engineering for classification: Creating better predictive features, handling text data</li> </ul> <p>External resources:</p> <ul> <li>scikit-learn Classification Documentation</li> <li>Google's ML Crash Course: Classification</li> <li>Kaggle Learn: Intro to Machine Learning</li> </ul> <p>Remember: Start simple (Logistic Regression), check the confusion matrix, and choose metrics that match your business costs. Interpretable models often outperform complex ones when stakeholders need to trust and understand predictions.</p>"},{"location":"04-core-ml/unsupervised/","title":"Unsupervised Learning: Discovering Structure Without Labels","text":"<p>This tutorial introduces unsupervised learning, the ML approach for finding patterns in data when no target labels exist. It gives the mental model, goals, and workflow you\u2019ll use in the next tutorials on Clustering and Dimensionality Reduction.</p> <p>Estimated time: 20\u201325 minutes</p>"},{"location":"04-core-ml/unsupervised/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement:</p> <p>Many real projects start with \"What\u2019s in this data?\" before they start with \"Can we predict Y?\"</p> <p>When labels are missing, expensive, subjective, or changing, unsupervised learning helps teams explore data, organize it, and make it understandable enough to decide what to do next.</p> <p>Practical benefits:</p> <ul> <li>Discover understandable structure (groups, summaries, patterns) from messy datasets.</li> <li>Reduce manual analysis time by organizing large collections (customers, products, documents, events).</li> <li>Create signals and representations that later improve supervised models (features, segments, similarity scores).</li> </ul> <p>Professional context:</p> <p>Unsupervised learning is common in segmentation, exploratory data analysis, visualization, anomaly discovery, and representation learning across finance, healthcare, e-commerce, operations, and product analytics.</p> <p></p>"},{"location":"04-core-ml/unsupervised/#running-example-conceptual","title":"Running Example (Conceptual)","text":"<p>To keep this series consistent, imagine an e-commerce company with thousands of customers described by behavioral features (how recently they purchased, how often, how much they spend, returns patterns, support usage, discount usage).</p> <p>Business need: \u201cWe don\u2019t have predefined segments, but we need to understand different customer behaviors to tailor marketing and retention.\u201d</p> <p>What <code>success</code> looks like (without labels):</p> <ul> <li>Clear patterns that stakeholders can describe and act on (e.g., distinct behavior groups or meaningful low-dimensional views).</li> <li>Results that remain similar when re-run on new data or slightly different samples.</li> <li>Insights that lead to measurable improvements in decisions (campaign targeting, retention outreach, support prioritization).</li> </ul>"},{"location":"04-core-ml/unsupervised/#core-concepts","title":"Core Concepts","text":""},{"location":"04-core-ml/unsupervised/#what-is-unsupervised-learning","title":"What Is Unsupervised Learning?","text":"<p>Unsupervised learning finds structure in the input data \\(X\\) without a labeled target \\(y\\). Instead of predicting a known outcome, it produces representations that make the data easier to understand or operate on.</p> <p>Typical outputs include:</p> <ul> <li>Group assignments (segments)</li> <li>Coordinates in a simplified space (for visualization or compression)</li> <li>Similarity relationships (nearest neighbors)</li> <li>Unusualness scores (for review/monitoring)</li> </ul>"},{"location":"04-core-ml/unsupervised/#main-families-of-tasks","title":"Main Families of Tasks","text":"<p>This tutorial introduces the families at a high level; the next tutorials go deep on the first two.</p> <ul> <li>Clustering: Group items that are <code>similar</code> to each other.</li> <li> <p>Use when the goal is segmentation or organizing a large set into a manageable set of groups.</p> </li> <li> <p>Dimensionality reduction / embeddings: Represent each item using fewer dimensions while keeping important structure.</p> </li> <li> <p>Use when the goal is visualization, compression, denoising, or faster downstream learning.</p> </li> <li> <p>Anomaly discovery (preview): Identify items that look rare or inconsistent with the majority.</p> </li> <li>Use when the goal is review prioritization, quality control, or detecting failures.</li> </ul>"},{"location":"04-core-ml/unsupervised/#unsupervised-vs-supervised-quick-contrast","title":"Unsupervised vs Supervised (quick contrast)","text":"Aspect Supervised learning Unsupervised learning Input Features + labels Features only Primary goal Predict known outcomes Discover structure Output Predictions Groups/representations/scores \u201cGood\u201d means Matches labels well Useful, stable, interpretable"},{"location":"04-core-ml/unsupervised/#the-key-design-choice-what-does-similar-mean","title":"The Key Design Choice: What Does \"Similar\" Mean?","text":"<p>Unsupervised learning often depends on an implicit or explicit definition of similarity.</p> <p>Similarity can be shaped by:</p> <ul> <li>Which features are included (and which are excluded)</li> <li>Feature scaling and transformations (units and ranges matter)</li> <li>How missing values and outliers are handled</li> <li>Whether time, geography, or identity-like fields leak \u201ctrivial grouping\u201d</li> </ul> <p>Critical idea: In unsupervised projects, your feature choices and preprocessing can matter as much as the algorithm\u2014sometimes more.</p>"},{"location":"04-core-ml/unsupervised/#the-unsupervised-workflow-high-level","title":"The Unsupervised Workflow (High Level)","text":"<p>This is a practical pipeline that works across clustering and dimensionality reduction.</p> <p></p> <p>1. Define the objective</p> <ul> <li>Are you trying to segment, visualize, compress, or detect unusual cases?</li> <li>What decision will change if the result is useful?</li> </ul> <p>2. Assemble the right dataset</p> <ul> <li>Choose entities (customers, sessions, products) and time windows.</li> <li>Ensure the features represent behavior you actually want to group/visualize.</li> </ul> <p>3. Prepare features</p> <ul> <li>Handle missing values, inconsistent formats, and extreme outliers.</li> <li>Normalize/scale where appropriate so \u201cdistance\u201d is meaningful.</li> <li>Remove or rethink features that create accidental grouping (IDs, timestamps that encode cohorts, etc.).</li> </ul> <p>4. Fit an unsupervised method</p> <ul> <li>Produce groupings or representations.</li> <li>Keep track of settings and preprocessing so results are reproducible.</li> </ul> <p>5. Evaluate without labels</p> <ul> <li>Check stability (across seeds, samples, time windows).</li> <li>Check interpretability (can people describe the structure?).</li> <li>Check usefulness (does it enable a better decision or workflow?).</li> </ul> <p>6. Interpret and communicate</p> <ul> <li>Convert patterns into plain-language descriptions and recommended actions.</li> <li>Identify \"borderline\" cases and what additional data would clarify them.</li> </ul> <p>7. Iterate</p> <ul> <li>Adjust features, preprocessing, or objective.</li> <li>Re-run and compare results in a controlled way.</li> </ul>"},{"location":"04-core-ml/unsupervised/#evaluation-without-labels-what-to-look-for","title":"Evaluation Without Labels (What to Look For)","text":"<p>Because there is no ground truth label, evaluation is a combination of technical checks and practical validation.</p> <p>Focus on:</p> <ul> <li>Stability: Do results remain similar under small changes (sampling, time period, minor feature noise)?</li> <li>Separation / clarity: Are patterns distinct enough to be actionable, or are they arbitrary slices of a continuum?</li> <li>Interpretability: Can domain experts explain what differentiates groups/regions?</li> <li>Actionability: Can teams attach different actions to different groups/regions?</li> <li>Downstream impact: Do decisions improve when using the unsupervised output (through experiments or operational KPIs)?</li> </ul>"},{"location":"04-core-ml/unsupervised/#best-practices","title":"Best Practices","text":"<ul> <li>Start from the decision, not the method: define what will change if the structure is meaningful.</li> <li>Keep a strong baseline mindset: compare new outputs against simple alternatives (even \u201cno segmentation\u201d).</li> <li>Document preprocessing carefully: unsupervised results can be sensitive; reproducibility is essential.</li> <li>Use human validation early: domain review can catch spurious patterns quickly.</li> <li>Re-check over time: behavior drift can make last month\u2019s structure misleading.</li> </ul>"},{"location":"04-core-ml/unsupervised/#common-pitfalls","title":"Common Pitfalls","text":"<ul> <li>Treating discovered groups as \u201ctruth\u201d: groups are model outputs, not natural laws.</li> <li>Over-interpreting patterns: unsupervised learning describes structure; it does not prove causality.</li> <li>Letting one feature dominate similarity: poor scaling or skew can create misleading structure.</li> <li>Accidental leakage: including identity-like or outcome-like features can make \u201csegments\u201d that are tautologies.</li> <li>Believing a nice visualization equals correctness: visuals are aids, not proof.</li> </ul>"},{"location":"04-core-ml/unsupervised/#whats-next-in-this-series","title":"What's Next in This Series","text":"<ul> <li>Tutorial 2: Clustering</li> <li> <p>How clustering works, how to choose settings, how to interpret and validate segments, and how to connect segments to actions.</p> </li> <li> <p>Tutorial 3: Dimensionality Reduction</p> </li> <li>How to compress and visualize high-dimensional data responsibly, when to use which approach, and how to avoid misleading interpretations.</li> </ul> <p>Remember: unsupervised learning has no \"right answer.\" Your results depend on your features and how you define similarity. Start simple, check stability, and only add complexity if it leads to clearer insights or better decisions.</p>"},{"location":"05-dl-fundamentals/","title":"Overview","text":"<p>Deep Learning Fundamentals is your step-by-step path into feedforward neural networks and the core ideas that make deep models actually train, generalize, and stay stable. </p> <p></p>"},{"location":"05-dl-fundamentals/#classification-using-neural-networks","title":"Classification Using Neural Networks","text":"<p>From linear models to multilayer perceptrons</p> <p>Move from classic ML to neural networks by using fully connected layers for classification. Connect activation functions, loss functions, and backpropagation to the project's implementation.</p> <p>You'll learn: Perceptrons and MLPs, activations (ReLU, sigmoid, softmax), cross-entropy loss, forward/backward pass intuition, basic multiclass classification with neural nets</p>"},{"location":"05-dl-fundamentals/#optimization","title":"Optimization","text":"<p>Make training actually converge</p> <p>Go beyond \"it trains\" by understanding how optimizers and learning rate choices shape your loss curve, mirroring the Optimization project tasks. Learn to debug training runs instead of guessing.</p> <p>You'll learn: Gradient descent variants (SGD, momentum, RMSProp, Adam), learning rate schedules, loss landscapes, exploding/vanishing gradients, practical tips for stable training</p>"},{"location":"05-dl-fundamentals/#error-analysis","title":"Error Analysis","text":"<p>Debug models with data, not vibes</p> <p>Use systematic error analysis to understand why your model fails, directly supporting the Error Analysis project. Learn to look at examples, slices, and metrics before changing architectures.</p> <p>You'll learn: Confusion matrices revisited, per-class and per-slice analysis, common failure patterns, building checklists, prioritizing fixes that actually move metrics</p>"},{"location":"05-dl-fundamentals/#regularization","title":"Regularization","text":"<p>Control overfitting in deep networks</p> <p>Connect regularization theory to the Regularization project, seeing how each technique changes training and validation curves in practice. Learn to keep models flexible but not memorizing.</p> <p>You'll learn: L1/L2 weight penalties, dropout, early stopping, data-related regularization, capacity vs. overfitting, reading learning curves to choose regularization strength</p> <p>Treat each project as a lab: run small experiments, track metrics, and use error analysis and regularization to turn \"it works\" models into reliable baselines before you move on to CNNs.</p>"},{"location":"05-dl-fundamentals/classification-nn/","title":"Neural Networks Fundamentals","text":"<p>Neural networks are widely used because they perform well on applications where writing rules by hand is unrealistic and where systems must learn directly from large numbers of examples. </p> <p>They show up in many everyday products and systems, such as:</p> <ul> <li>Computer vision: recognizing faces in photos and understanding visual content beyond simple rule-based image checks. </li> <li>Speech and audio: voice-assistant style systems that turn audio into text and actions. </li> <li>Personalization: recommendation systems that adapt to different users at scale instead of using one-size-fits-all rules. </li> <li>Fraud and risk: detecting fraud patterns that change frequently and benefit from models that can be retrained as new examples arrive. </li> <li>Healthcare support: learning patterns from medical records and outcomes to support diagnosis-type tasks when rules and edge cases are too complex to encode manually. </li> <li>Search and ranking: improving how results are ordered by learning from many interactions rather than relying only on fixed heuristics. </li> </ul> <p></p>"},{"location":"05-dl-fundamentals/classification-nn/#why-neural-networks","title":"Why Neural Networks?","text":"<p>Neural networks are useful when the relationship between inputs and outputs is too complex for hand-written rules or simple feature engineering.  They are especially common in areas like vision, text, and speech, where patterns are layered (simple patterns combine into more complex ones).  </p> <p>By the end, you should be able to:</p> <ul> <li>Explain what a neural network is in plain terms.</li> <li>Describe the training loop (predict \u2192 measure error \u2192 improve).</li> <li>Choose sensible defaults for a first model.</li> <li>Recognize early signs of overfitting and unstable training.</li> </ul>"},{"location":"05-dl-fundamentals/classification-nn/#what-is-a-neural-network","title":"What Is a Neural Network?","text":"<p>A neural network is a model made of layers that transform input data into predictions. Each layer learns a small transformation, and stacked layers can represent more complex patterns than a single layer.</p> <p>High-level components:</p> <ul> <li>Inputs (features): The values you feed into the model (numbers, pixels, counts, embeddings).  </li> <li>Parameters (weights and biases): What the model learns during training.  </li> <li>Layers: Organized groups of parameters that transform data step by step.  </li> <li>Output: The model\u2019s prediction (a number, a class, or a probability distribution).  </li> </ul> <p></p>"},{"location":"05-dl-fundamentals/classification-nn/#key-ideas-without-heavy-math","title":"Key Ideas (without heavy math)","text":"<p>Neural networks are built by stacking simple building blocks so the model can learn patterns at multiple levels.  Deep learning is essentially neural networks with many layers, which is why layers and representations matter so much. </p>"},{"location":"05-dl-fundamentals/classification-nn/#neuron-the-basic-unit","title":"Neuron: the basic unit","text":"<p>A neuron is a small unit that takes several input values, combines them into a single signal, and produces an output.  That output becomes an input to other neurons, so a network can build complex behavior from many simple parts. </p> <p>You can think of a neuron as doing three jobs:</p> <ul> <li>Combine information: It looks at multiple inputs and forms one internal \u201cscore\u201d that reflects what it\u2019s seeing. </li> <li>Control what passes forward: It uses an activation function to decide how strongly to pass that signal to the next layer. </li> <li>Support specialization: Different neurons can become sensitive to different patterns, so the network can cover many cases at once. </li> </ul>"},{"location":"05-dl-fundamentals/classification-nn/#activation-why-it-sometimes-gets-a-brain-analogy","title":"Activation: why it sometimes gets a \u201cbrain\u201d analogy","text":"<p>Activation functions are sometimes compared to a neuron \u201cfiring\u201d: when the combined signal is strong enough, more information flows forward.  This is just intuition; neural networks are mathematical models, but the idea is useful because it explains the role of activation as a gate. </p> <p>Practically, activation functions:</p> <ul> <li>Help the model represent more complex relationships than straight-line patterns. </li> <li>Let the network suppress weak signals and amplify important ones. </li> <li>Improve what the network can learn when you stack many layers. </li> </ul>"},{"location":"05-dl-fundamentals/classification-nn/#layers-neurons-working-together","title":"Layers: neurons working together","text":"<p>A layer is a group of neurons that all take the same input and produce a set of outputs.  Each layer can be understood as producing a new representation of the data\u2014often a more useful one for the final prediction. </p> <p>Common layer roles:</p> <ul> <li>Input layer: Receives raw features (numbers, pixels, etc.). </li> <li>Hidden layers: Transform inputs into intermediate representations. </li> <li>Output layer: Produces the final prediction (a class, probability, or value). </li> </ul>"},{"location":"05-dl-fundamentals/classification-nn/#layers-learn-representations","title":"Layers learn representations","text":"<p>Neural networks don\u2019t just \u201cfit a curve\u201d once; they learn intermediate representations that later layers can reuse.  This is a big reason deep networks work well on complex data: each layer can focus on a different level of detail. </p> <p>A helpful way to picture it (conceptually):</p> <ul> <li>Earlier layers learn simple patterns that are easier to detect. </li> <li>Middle layers combine simple patterns into richer patterns. </li> <li>Later layers combine richer patterns into task-level signals that make prediction easier. </li> </ul>"},{"location":"05-dl-fundamentals/classification-nn/#non-linearity-matters","title":"Non-linearity matters","text":"<p>If every layer were only a linear transformation, stacking layers would not add much expressive power, many linear steps still behave like one linear step.  Activation functions introduce non-linearity, which lets neural networks model complex relationships that simple models can\u2019t capture well. </p> <p>Common activations (conceptual view):</p> <ul> <li>ReLU: A practical default for hidden layers because it\u2019s simple and often trains well. </li> <li>Sigmoid: Common for binary outputs (probabilities between 0 and 1). </li> <li>Softmax: Common for multi-class classification outputs (probabilities across classes). </li> </ul>"},{"location":"05-dl-fundamentals/classification-nn/#putting-it-together","title":"Putting it together","text":"<p>A single neuron is limited on its own.  A layer combines many neurons to capture multiple patterns in parallel.  Multiple layers create a pipeline where each stage transforms the data into something the next stage can use more effectively. </p> <p></p>"},{"location":"05-dl-fundamentals/classification-nn/#how-training-works-the-training-loop","title":"How Training Works (the training loop)","text":"<p>Training is an iterative improvement process: the model makes predictions, compares them to correct answers, then adjusts its internal settings so the next predictions are slightly better.  A useful way to keep this intuitive is to follow one running example all the way through.</p>"},{"location":"05-dl-fundamentals/classification-nn/#running-example-classifying-email-as-spam-vs-not-spam","title":"Running example: classifying email as spam vs not spam","text":"<p>Imagine a dataset of emails where each email has a label:</p> <ul> <li>1 = spam</li> <li>0 = not spam</li> </ul> <p>Each email is converted into numeric features (for example: frequency of certain words, number of links, sender reputation score, etc.).  The neural network\u2019s job is to take those numbers and output a prediction like \u201c0.92 spam\u201d or \u201c0.08 spam\u201d.</p>"},{"location":"05-dl-fundamentals/classification-nn/#training-loop-overview-what-happens-repeatedly","title":"Training loop overview (what happens repeatedly)","text":"<ol> <li> <p>Forward pass (make a prediction)    The model reads one batch of emails and outputs predictions for each one (probabilities or class scores).     Early in training, these predictions can be poor because weights start out untrained. </p> </li> <li> <p>Loss (measure how wrong the prediction is)    The loss function turns \u201chow wrong\u201d the predictions are into a single number.     If the model predicts \u201c0.05 spam\u201d for a spam email, the loss will be high; if it predicts \u201c0.95 spam\u201d, the loss will be low.</p> </li> <li> <p>Backpropagation (figure out what to change)    Backpropagation computes how much each weight in the network contributed to the error.     Conceptually: it answers \u201cwhich parts of the network pushed the prediction in the wrong direction, and by how much?\u201d </p> </li> <li> <p>Optimizer step (update the weights)    The optimizer updates weights slightly so the loss would be lower next time on similar examples.     The key idea is \u201csmall controlled changes,\u201d not big jumps\u2014training is a sequence of tiny improvements.</p> </li> <li> <p>Repeat across many batches and epochs    A batch is a small chunk of the training data processed at once, which makes training efficient and stable.     An epoch is one full pass through the training set; after multiple epochs, the model typically becomes more accurate because it has seen many varied examples. </p> </li> </ol>"},{"location":"05-dl-fundamentals/classification-nn/#what-good-training-looks-like","title":"What \u201cgood training\u201d looks like","text":"<p>Using the spam example, progress often looks like this:</p> <ul> <li>Early epochs: the model misses obvious spam and produces inconsistent confidence scores.</li> <li>Mid training: the model learns stronger signals (for example, suspicious link patterns) and accuracy rises.</li> <li>Later epochs: improvements slow down; at this stage, validation performance matters most to ensure the model is not just memorizing training emails. </li> </ul> <p>Practical habit: Track both training and validation curves; training loss going down is not enough if validation stops improving. </p>"},{"location":"05-dl-fundamentals/classification-nn/#what-you-are-optimizing-loss-metrics","title":"What You Are Optimizing (loss + metrics)","text":"<p>A loss function is the score the model is trained to reduce during learning; it is the objective the training algorithm follows step by step.   </p> <p>Metrics are the numbers you use to judge whether the model is improving in a way that matches the real goal (and whether it will be useful outside the training set). </p> <p>A simple way to remember the difference:</p> <ul> <li>Loss = what the model learns from. </li> <li>Metrics = what you evaluate and communicate. </li> </ul> <p>Typical pairing (high level):</p> <ul> <li>Classification: Use a classification loss, and track accuracy plus other metrics that reveal how the model is making mistakes (especially when classes are imbalanced). </li> <li>Regression: Use a regression loss, and track error measures that fit the problem (absolute error vs squared error depends on whether large mistakes should be punished more heavily). </li> </ul> <p>Practical note: Always monitor metrics on a validation set; good training loss alone doesn\u2019t guarantee good real-world performance. </p>"},{"location":"05-dl-fundamentals/classification-nn/#a-practical-workflow-for-first-neural-networks","title":"A practical workflow for first neural networks","text":"<p>This is a simple, repeatable process you can apply to most beginner neural network tasks, and it works well as a checklist during projects.  To keep the ideas concrete, the running example below is an email spam vs not spam classifier. </p>"},{"location":"05-dl-fundamentals/classification-nn/#step-1-start-with-a-baseline","title":"Step 1: Start with a baseline","text":"<p>Before using a neural network, train a simple baseline model on the same spam dataset (for example logistic regression).  This baseline gives you a \u201cminimum acceptable performance\u201d and helps you spot data problems early (label noise, leakage, strange feature values).   </p> <ul> <li>Use the same train/validation split you plan to use later.   </li> <li>Record baseline results (accuracy and at least one more informative metric if spam is rare).   </li> </ul>"},{"location":"05-dl-fundamentals/classification-nn/#step-2-build-a-small-neural-network","title":"Step 2: Build a small neural network","text":"<p>Start small so you can debug quickly and understand what changes help.  For spam classification, a simple feedforward network that takes numeric features and outputs a probability is enough to learn the workflow. </p> <ul> <li>Begin with 1\u20132 hidden layers and a modest number of units.   </li> <li>Use a standard hidden-layer activation (ReLU is a common default).   </li> <li>Use a standard optimizer and keep most settings default at first so you have fewer moving parts.   </li> </ul>"},{"location":"05-dl-fundamentals/classification-nn/#step-3-train-and-observe-curves","title":"Step 3: Train and observe curves","text":"<p>Train the model and watch what happens on both training and validation data, not just final accuracy.  In the spam example, it\u2019s common for training loss to keep improving while validation stops improving, which signals overfitting. </p> <ul> <li>Track training loss and validation loss each epoch.   </li> <li>Save the best model based on validation performance (not the last epoch).   </li> <li>Look at a few misclassified emails to understand failure modes (certain spam types, newsletters, short messages).   </li> </ul>"},{"location":"05-dl-fundamentals/classification-nn/#step-4-improve-systematically","title":"Step 4: Improve systematically","text":"<p>When results are not good, improve the system step by step instead of changing many things at once.  For spam detection, this keeps you focused on whether you\u2019re fixing a data issue, a training issue, or a model-capacity issue. </p> <ul> <li>Change one thing at a time (learning rate, network size, regularization strength).   </li> <li>Keep short experiment notes: what changed, why it changed, and what metric moved.   </li> <li>Prefer fixes that improve validation results consistently, not just a single lucky run.   </li> </ul>"},{"location":"05-dl-fundamentals/classification-nn/#when-neural-networks-are-a-good-choice","title":"When neural networks are a good choice","text":"<p>Neural networks are often a good fit when:</p> <ul> <li>There is enough data to learn from (not just dozens of examples).</li> <li>The pattern is complex and non-linear.</li> <li>Feature engineering would be difficult or fragile.</li> </ul> <p>Neural networks may be a poor choice when:</p> <ul> <li>Data is very limited.</li> <li>Interpretability is a strict requirement.</li> <li>A simpler model already meets the goal with less complexity. </li> </ul> <p></p>"},{"location":"05-dl-fundamentals/classification-nn/#common-mistakes-and-how-to-avoid-them","title":"Common mistakes (and how to avoid them)","text":"<p>Mistake: Skipping the baseline - Fix: Always run a simple model first so improvements are measurable. </p> <p>Mistake: Training without a validation set - Fix: Use a validation split for decisions; keep a final test set untouched.</p> <p>Mistake: Overfitting and not noticing - Signs: Training improves while validation stalls or worsens. - Fix: Add regularization, reduce model size, or improve data quality.</p> <p>Mistake: Changing many things at once - Fix: One change per experiment; keep notes (what you changed, why, and results). </p> <p></p>"},{"location":"05-dl-fundamentals/classification-nn/#quick-glossary","title":"Quick glossary","text":"<ul> <li>Epoch: One full pass through the training dataset.</li> <li>Batch: A small subset of data processed before an update.</li> <li>Learning rate: Step size of parameter updates.</li> <li>Generalization: Performance on new data, not just training data.</li> <li>Overfitting: Training performance improves while real-world performance does not.</li> </ul>"},{"location":"05-dl-fundamentals/classification-nn/#next-steps","title":"Next steps","text":"<p>After this page, the practical goal is to implement a basic feedforward network for classification and learn how to evaluate it honestly. Then move to:</p> <ul> <li>TensorFlow 2 &amp; Keras (build and train models efficiently)</li> <li>Optimization (make training stable and faster)</li> <li>Regularization (reduce overfitting and improve generalization)</li> <li>Error analysis (debug failures with evidence) </li> </ul> <p>Use simple baselines, validate carefully, and iterate in small steps. Strong fundamentals plus honest evaluation beats complexity.</p>"},{"location":"05-dl-fundamentals/classification-nn/#learning-resources","title":"Learning Resources","text":"<p>If you want clearer intuition for what\u2019s happening inside neural networks (without starting from heavy formulas), these 3Blue1Brown resources are a strong next step:</p> <ul> <li>Neural Networks (topic page / series hub) </li> <li> <p>But what is a neural network? (Deep Learning, Chapter 1)</p> </li> <li> <p>Gradient descent, how neural networks learn (Deep Learning, Chapter 2)</p> </li> </ul>"},{"location":"05-dl-fundamentals/error-analysis/","title":"Error Analysis (Professional Guide)","text":"<p>Error analysis is how you turn \u201cthe model is wrong\u201d into actionable categories of mistakes, so your next iteration targets the real bottleneck (data, threshold, features, labeling, or model capacity).  </p> <p>Running example: binary classification where positive = 1 (\u201cfraud\u201d, \u201cdisease\u201d, \u201cspam\u201d) and negative = 0.</p> <p></p>"},{"location":"05-dl-fundamentals/error-analysis/#confusion-matrix-and-type-iii","title":"Confusion matrix (and Type I/II)","text":"<p>A confusion matrix counts how often your predicted class matches the true class, broken down into True Positive (TP), False Positive (FP), False Negative (FN), and True Negative (TN).  It\u2019s the most direct way to see what kind of wrong your classifier is making, not just \u201chow often\u201d it is wrong.  </p> <p></p> <p>Type I vs Type II error (binary): </p> <ul> <li>Type I error = FP (false positive): predicted positive, but actually negative (a false alarm).    </li> <li>Type II error = FN (false negative): predicted negative, but actually positive (a miss).    </li> </ul> <p>In practice, you usually choose an operating point (often via the decision threshold) that trades off FP vs FN based on business cost, safety, and capacity constraints. </p> Use case Positive class = 1 Type I (FP) means\u2026 Type II (FN) means\u2026 Typically worse Medical screening (early triage) \u201cHas disease\u201d Healthy person flagged; extra tests/anxiety Sick person missed; delayed treatment FN (Type II) Spam filtering \u201cSpam\u201d Legit email sent to spam; user frustration Spam reaches inbox; annoyance/risk Depends; often FP is very costly for trust Fraud detection alerts \u201cFraud\u201d Legit transaction flagged; support load/friction Fraud passes; direct financial loss FN often costly, but FP limited by review capacity Credit/loan approval \u201cWill default\u201d Good customer rejected; lost revenue Bad loan approved; losses Context-dependent; risk appetite drives choice Intrusion detection / safety monitoring \u201cAttack/incident\u201d Benign activity triggers alarm; alert fatigue Real attack missed; major damage FN (Type II) <p>Rule of thumb: If the \u201cmiss\u201d is dangerous or expensive, prioritize reducing Type II (FN); if false alarms overload teams or harm users, prioritize reducing Type I (FP). </p>"},{"location":"05-dl-fundamentals/error-analysis/#performance-measures-what-they-mean","title":"Performance measures (what they mean)","text":"<p>All these measures are computed from TP, FP, FN, TN (so start from the confusion matrix).  </p> <p>Let positive = 1.</p> <ul> <li>Sensitivity / Recall / TPR: fraction of actual positives you correctly catch \\( \\text{Recall} = \\frac{TP}{TP + FN} \\) </li> <li>Specificity / TNR: fraction of actual negatives you correctly reject \\( \\text{Specificity} = \\frac{TN}{TN + FP} \\) </li> <li>Precision / PPV: when you predict positive, how often you\u2019re right \\( \\text{Precision} = \\frac{TP}{TP + FP} \\) </li> <li>F1 score: harmonic mean of precision and recall \\( F1 = \\frac{2 \\cdot \\text{Precision}\\cdot \\text{Recall}}{\\text{Precision}+\\text{Recall}} \\) </li> </ul> <p>Why F1 exists: it punishes models that \u201ccheat\u201d by maximizing only precision or only recall, and it\u2019s commonly used when classes are imbalanced and you want one balanced number.  </p>"},{"location":"05-dl-fundamentals/error-analysis/#which-metric-matters-when-industry-patterns","title":"Which metric matters when? (industry patterns)","text":"<p>Metric choice is a product decision disguised as math: it encodes which error costs more (false alarms vs misses) and how you want to operate the threshold.  </p> Scenario What you optimize Why this is common Disease screening (initial test) High recall (sensitivity) Missing a real positive (FN) is costly; you can confirm positives later with a second test. Spam / content moderation triage High precision (at a chosen recall) Too many false positives (FP) harms user trust; you often add human review for borderline cases. Fraud detection Recall at fixed FP rate, or precision\u2013recall tradeoff Teams often pick an operating point that matches investigation capacity (alerts/day). Safety-critical anomaly detection Very low FN tolerance The \u201cmiss\u201d cost dominates, so you accept more false alarms and add downstream filtering. KPI reporting to non-technical stakeholders A small set: precision, recall, F1 These are interpretable in terms of \u201cfalse alarms vs misses,\u201d unlike a single opaque score. <p>Practical rule: don\u2019t ask \u201cwhat\u2019s the best metric?\u201d. Instead ask \u201cwhat\u2019s the cost of FP vs FN, and can we change the threshold to hit an operating target?\u201d  </p> <p></p>"},{"location":"05-dl-fundamentals/error-analysis/#bias-variance-irreducible-error-bayes-error-why-models-plateau","title":"Bias, variance, irreducible error, Bayes error (why models plateau)","text":"<p>Bias: error from overly simplistic assumptions (the model can\u2019t represent the real pattern, so it underfits). - Scenario: You fit a linear model to a clearly nonlinear relationship; training and validation performance are both poor, and adding more training data doesn\u2019t help much.</p> <p>Variance: error from being too sensitive to the particular training set (the model \u201clearns noise,\u201d so it overfits). - Scenario: A very flexible model achieves excellent training performance but noticeably worse validation/test performance; small changes in the training split lead to big changes in results.</p> <p>Irreducible error: error no model can eliminate because the input features don\u2019t fully determine the target (noise, ambiguity, overlapping classes, imperfect labels). - Scenario: Two radiologists disagree on an X-ray label, or the same customer behavior could correspond to either \u201cwill churn\u201d or \u201cwon\u2019t churn\u201d depending on hidden factors you didn\u2019t measure.</p> <p>Bayes error: the theoretical lowest possible error for the task given the true data distribution and the available features (an absolute floor). - Scenario: In fraud detection, some fraudulent and legitimate transactions are genuinely indistinguishable using your current features, so a non-zero error rate is unavoidable unless you collect better signals.</p> <p>Practical habit: when performance plateaus, decide whether you\u2019re blocked by bias (need a more expressive model/features), variance (need regularization/more data), or data limits (irreducible/Bayes-ish floor), and change one knob at a time to verify the cause. </p> <p>How to approximate Bayes error (practical proxies):</p> <ul> <li>Human/expert performance on the same labeling rules (if humans are near-optimal).</li> <li>Label disagreement rate / audit of label noise.</li> <li>\u201cStrong model plateau\u201d: if several well-tuned, high-capacity models converge to similar test performance, that level may reflect a near-floor for your current dataset and labels.</li> </ul> <p></p>"},{"location":"05-dl-fundamentals/error-analysis/#how-to-compute-biasvariance-build-a-confusion-matrix","title":"How to compute bias/variance + build a confusion matrix","text":""},{"location":"05-dl-fundamentals/error-analysis/#confusion-matrix-python","title":"Confusion matrix (Python)","text":"<p>You can compute a confusion matrix directly from arrays of true and predicted labels (common in professional workflows for reporting and monitoring).</p> <pre><code>from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\n\n# 1 = positive class (e.g., fraud), 0 = negative class\ny_true = [1, 0, 1, 1, 0, 0, 1, 0, 0, 1]\ny_pred = [1, 0, 0, 1, 0, 1, 1, 0, 0, 0]\n\n# Confusion matrix layout (binary, labels=[0,1]):\n#  [[TN, FP],\n#  [FN, TP]]\ncm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n\nprint(cm)\n\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm,\n    display_labels=[\"negative (0)\", \"positive (1)\"]\n)\n\ndisp.plot(values_format=\"d\", cmap=\"Blues\", colorbar=False)\nplt.title(\"Confusion Matrix\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p>Confusion matrices also extend to multi-class classification as an \\(N \\times N\\) table (each cell counts how often class i was predicted as class j). </p>"},{"location":"05-dl-fundamentals/error-analysis/#biasvariance-metric-decision-guide","title":"Bias/variance + metric decision guide","text":"<p>Teams don\u2019t \u201coptimize a metric\u201d in the abstract\u2014they choose what to optimize based on the cost of false positives vs false negatives and where they want to operate the decision threshold. In parallel, they diagnose whether progress is blocked by bias (underfitting) or variance (overfitting) and apply targeted fixes instead of redesigning everything.  </p>"},{"location":"05-dl-fundamentals/error-analysis/#4-common-industry-cases-what-to-optimize-what-to-do","title":"4 common industry cases (what to optimize, what to do)","text":"Use case (typical goal) What \u201cbad errors\u201d look like What to optimize What to do (practical levers) Intuitive example Medical screening / safety triage (don\u2019t miss positives) Too many FN (misses): dangerous cases pass through Recall / Sensitivity Lower threshold to catch more positives  ; use class weights / cost-sensitive training; add a second-stage confirmatory test (high recall first stage, high precision second stage) Smoke alarm: better to alarm too often than miss a real fire Content moderation / spam \u201cauto-block\u201d (avoid false alarms) Too many FP: legitimate items blocked, user trust harmed Precision Raise threshold so only very confident positives are actioned ; add \u201cabstain \u2192 human review\u201d for borderline scores; improve negative examples (hard negatives) Airport security: false accusations are costly, so you require higher certainty before action Fraud detection alerts (limited review capacity) Alert queue overloaded (FP) or losses increase (FN) Precision\u2013recall tradeoff, often precision at a chosen recall or \u201calerts/day\u201d operating point Choose threshold to match team capacity  ; rank by score and investigate top-k; tune features to separate hard borderline cases A call center can only handle 200 investigations/day\u2014set the system to produce ~200 high-quality alerts Marketing / churn outreach (limited budget) Campaign wastes money on non-churners (FP) or misses churners (FN) Often precision in the top segment (who you contact), sometimes recall if retention is critical Increase threshold or target top-k scores; calibrate probabilities; use uplift/causal targeting if applicable (only contact people you can change) You can call 1,000 customers\u2014pick the 1,000 most likely to leave (and most likely to be saved)"},{"location":"05-dl-fundamentals/error-analysis/#bias-vs-variance-quick-diagnosis-fix","title":"Bias vs variance: quick diagnosis \u2192 fix","text":"<p>A simple (and very usable) pattern is to compare training vs validation performance:</p> <ul> <li> <p>High bias (underfitting): training is poor and validation is also poor.   What to do: increase model capacity (richer features, nonlinear model), train longer, reduce overly-strong regularization, revisit label/feature definition.</p> </li> <li> <p>High variance (overfitting): training is very strong but validation/test is much worse.   What to do: add regularization (L2/weight decay, dropout), simplify the model, early stop, collect more data, and stabilize features (reduce leakage, consistent preprocessing).</p> </li> </ul> <p>Practical habit: treat this like an experiment\u2014change one knob per run and keep short notes so you can trust what caused the improvement. </p>"},{"location":"05-dl-fundamentals/error-analysis/#biasvariance-practical-estimation-idea-decision-workflow","title":"Bias/variance (practical estimation idea \u2192 decision workflow)","text":"<p>In practice, teams often estimate variance by retraining the same pipeline across folds or bootstrap samples and measuring how much metrics and predictions move (high spread \u2192 high variance).</p> <ol> <li>Fix a test set (or use nested CV if data is limited).</li> <li>Retrain the same pipeline across K folds / bootstrap resamples.</li> <li>Track variability: metrics (e.g., precision/recall/F1) and prediction stability for the same examples.</li> <li>Decide:</li> <li>If results vary a lot across folds \u2192 variance problem: regularize/simplify, add data, improve feature stability.</li> <li>If results are consistently bad across folds \u2192 bias problem: add signal (features/data), use a more expressive model, reduce constraints.</li> </ol> <p>A clean mental model: Bias means \u201cthe model can\u2019t learn the pattern,\u201d variance means \u201cit learns a different pattern each time.\u201d</p>"},{"location":"05-dl-fundamentals/error-analysis/#quick-glossary","title":"Quick glossary","text":"<ul> <li>TP/FP/FN/TN: core outcomes behind all classification metrics.   </li> <li>Type I error: false positive (FP).  </li> <li>Type II error: false negative (FN).  </li> <li>Precision vs Recall: \u201cWhen I predict positive, am I right?\u201d vs \u201cDid I catch the positives?\u201d </li> <li>Threshold: decision cutoff that trades FP vs FN through the confusion matrix. </li> </ul> <p>Measure the mistakes, name the cost, and set the threshold. Good models aren\u2019t just accurate, they\u2019re accountable.</p> <p></p>"},{"location":"05-dl-fundamentals/optimization/","title":"Optimization (Neural Network Training Fundamentals)","text":"<p>Optimization is the part of training where you systematically improve a neural network\u2019s parameters (weights and biases) so that its loss gets smaller on the training data. In this chapter, we\u2019ll treat optimization as an iterative process first, then introduce the main tools (SGD, momentum, RMSProp, Adam, learning-rate decay, batch normalization) with both intuition and minimal Keras snippets.</p> <p></p>"},{"location":"05-dl-fundamentals/optimization/#optimization-as-a-process","title":"Optimization as a process","text":"<p>Think of training a Fashion-MNIST classifier as repeated, small edits to the model so it becomes less wrong over time.</p> <p>Running example (Fashion-MNIST):</p> <ul> <li>Input: 28\u00d728 grayscale clothing images.</li> <li>Output: one of 10 classes (T-shirt/top, trouser, \u2026, ankle boot).</li> <li>Model: e.g., a small MLP that outputs class probabilities.</li> </ul> <p>The optimization loop (what happens repeatedly):</p> <ol> <li>Forward pass: compute predictions for a batch of images.</li> <li>Loss: convert \u201chow wrong\u201d the predictions are into a single number.</li> <li>Backward pass: compute gradients (how each parameter affects the loss).</li> <li>Update step: change parameters using an optimizer.</li> <li>Repeat: across many mini-batches (steps) and epochs (full passes through the dataset).</li> </ol> <p></p> <p>Parameters vs hyperparameters</p> <ul> <li> <p>Parameters: Learned values inside the model that get updated during training (via backpropagation + the optimizer). Examples: the weights and biases in Dense layers, and (in general) any trainable tensors that determine how inputs are transformed into outputs.</p> </li> <li> <p>Hyperparameters: Settings you choose outside the model\u2019s learned weights that control the learning process or model capacity. Examples include the learning rate, batch size, optimizer choice (SGD/RMSProp/Adam), momentum/betas, learning-rate schedule, number of layers/units, regularization strength (L2/weight decay), dropout rate, and early-stopping patience.</p> </li> </ul> <p>A quick rule of thumb: if it\u2019s learned automatically from data during training, it\u2019s a parameter; if you set it before (or while) training to guide learning, it\u2019s a hyperparameter.</p> <p>Practical habit: When training is bad, first ask \u201cis the optimization process unstable or misconfigured?\u201d before redesigning the model.</p>"},{"location":"05-dl-fundamentals/optimization/#where-basic-gradient-descent-fails","title":"Where basic gradient descent fails","text":"<p>\u201cBasic gradient descent\u201d usually means using a single global learning rate and applying gradient updates without any stabilization tricks. It can fail even when the model could learn, because the path to a good solution is difficult.</p> <p></p> <p>Common failure modes (and what you observe):</p> <ul> <li>Learning rate too high: loss may explode, bounce wildly, or never settle.</li> <li>Learning rate too low: loss decreases painfully slowly; training feels \u201cstuck.\u201d</li> <li>Ravines / ill-conditioning: the loss changes steeply in one direction and slowly in another, causing zig-zagging rather than smooth progress.</li> <li>Saddle points: places where the gradient can be near zero but the point is not a good minimum (flat in some directions, curved in others).</li> <li>Noisy gradients (real data): updates based on small batches are \u201cdirectionally correct on average\u201d but noisy step-to-step.</li> </ul> <p>Why this happens (intuition)</p> <ul> <li>Gradients give a local direction, not a global plan.</li> <li>A single learning rate is a blunt tool: some parameters need bigger steps, others need smaller steps.</li> <li>Noise can be helpful (it can shake you out of flat regions), but it can also make training unstable without smoothing.</li> </ul>"},{"location":"05-dl-fundamentals/optimization/#the-optimization-toolkit-intuition-first","title":"The optimization toolkit (intuition-first)","text":"<p>This section introduces the most common fixes as ideas (what problem they solve), independent of any specific framework.</p>"},{"location":"05-dl-fundamentals/optimization/#normalize-inputs-make-gradients-behave","title":"Normalize inputs (make gradients behave)","text":"<p>If inputs have wildly different scales, the loss surface becomes harder to navigate and updates can become inefficient or unstable. Normalizing inputs makes training smoother because parameter updates don\u2019t have to fight inconsistent feature scales.</p> <p>Two common choices:</p> <ul> <li>Rescale pixels: map [0, 255] \u2192 [0, 1].</li> <li>Standardize features: subtract mean and divide by standard deviation (common for tabular features and some intermediate representations).</li> </ul>"},{"location":"05-dl-fundamentals/optimization/#batch-stochastic-and-mini-batch-gradient-descent","title":"Batch, stochastic, and mini-batch gradient descent","text":"<ul> <li>Batch gradient descent: uses the full dataset to compute one update (stable but expensive).</li> <li>Stochastic gradient descent (SGD): uses one example per update (fast but very noisy).</li> <li>Mini-batch gradient descent: uses a small batch (the standard default in deep learning because it\u2019s efficient and reasonably stable).</li> </ul> <p>All three methods are the same idea: update model weights by following the loss gradient, repeatedly, until the model improves. The key difference is how many training examples you use to estimate the gradient for each update step\u2014more examples gives a cleaner (less noisy) direction but costs more compute per step.</p> <p>In practice, mini-batches are the default because they balance speed (vectorized computation) and stability (less noise than single-example updates).</p> Method Data per update Updates per epoch (approx.) Gradient noise Speed per update Typical use Batch GD All \\(N\\) examples 1 Low (smooth) Slowest Small datasets, debugging, very stable curves SGD 1 example \\(N\\) High (jittery) Fastest Rare as-is; conceptually important Mini-batch GD \\(B\\) examples (e.g., 32\u2013256) \\(N/B\\) Medium Fast (GPU-friendly) Standard default for deep learning"},{"location":"05-dl-fundamentals/optimization/#moving-averages-a-core-building-block","title":"Moving averages (a core building block)","text":"<p>A moving average is a simple way to \u201cremember the recent past\u201d of a quantity that jumps around from step to step (like gradients or losses). Instead of reacting fully to the newest value\u2014which might be noisy\u2014you blend it with the previous average, producing a smoother signal that leads to more stable updates.</p> <p>Exponential moving average (EMA) uses a decay factor \\(\\beta\\) (often 0.9, 0.99, etc.): higher \\(\\beta\\) means longer memory and stronger smoothing, while lower \\(\\beta\\) means the average adapts faster but is noisier. In optimization, EMAs show up directly in momentum (EMA of gradients), RMSProp (EMA of squared gradients), and Adam (both).</p> <p></p> <p>Minimal implementation (conceptual): <pre><code>def ema_update(moving_avg, value, beta=0.9):\n    # beta close to 1.0 = smoother but slower to react\n    return beta * moving_avg + (1 - beta) * value\n</code></pre></p>"},{"location":"05-dl-fundamentals/optimization/#momentum-smooth-direction","title":"Momentum (smooth direction)","text":"<p>Momentum adds \u201cinertia\u201d to gradient descent by maintaining a velocity\u2014a running direction that accumulates past gradients\u2014so each update is influenced by recent history, not just the current (noisy) mini-batch. This tends to reduce zig-zagging in narrow valleys (where gradients flip direction side-to-side) and helps the optimizer make faster progress along directions that stay consistently downhill.</p> <p>Intuitively: plain gradient descent is like taking a new step based only on what you see right now; momentum is like rolling a ball that keeps moving in the same direction unless there\u2019s strong evidence to turn. The key hyperparameter is <code>momentum</code> (often ~0.9): higher values smooth more (but can overshoot), lower values react faster (but keep more jitter).</p> <p></p> <p>Minimal Keras snippet (SGD + momentum): Keras exposes momentum directly on the SGD optimizer via the <code>momentum</code> argument. keras</p> <pre><code>from tensorflow import keras\n\nmodel.compile(\n    optimizer=keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n</code></pre>"},{"location":"05-dl-fundamentals/optimization/#rmsprop-adapt-step-sizes-per-parameter","title":"RMSProp (adapt step sizes per parameter)","text":"<p>RMSProp is an adaptive optimizer: instead of using one global step size for every weight, it adjusts the effective step size per parameter based on recent gradient behavior. The core trick is to keep an exponential moving average of the squared gradients; weights that repeatedly see large gradients get their updates dampened, while weights with small gradients get relatively larger steps.  </p> <p>Intuitively, this helps when the loss surface has uneven curvature (some directions are steep, others are flat): RMSProp automatically \u201csteps carefully\u201d in steep directions and \u201csteps more boldly\u201d in flat ones, which often makes training more stable than plain SGD with a single learning rate.</p> <p></p> <p>Minimal Keras snippet: RMSProp is available as <code>keras.optimizers.RMSprop(...)</code> with common knobs like <code>learning_rate</code>, <code>rho</code> (decay for the moving average), <code>momentum</code>, and <code>epsilon</code>. tensorflow</p> <pre><code>from tensorflow import keras\n\nmodel.compile(\n    optimizer=keras.optimizers.RMSprop(\n        learning_rate=1e-3,\n        rho=0.9,\n        momentum=0.0,\n        epsilon=1e-7\n    ),  # RMSprop parameters \n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n</code></pre>"},{"location":"05-dl-fundamentals/optimization/#adam-momentum-adaptive-scaling","title":"Adam (momentum + adaptive scaling)","text":"<p>Adam is a practical \u201cbest of both worlds\u201d optimizer: it keeps a momentum-like running direction (so updates don\u2019t swing wildly from mini-batch noise) and it adapts step sizes per parameter (so weights that see consistently large gradients don\u2019t take overly aggressive steps).  This combination makes training feel more \u201cself-tuning\u201d than plain SGD because the optimizer both smooths the direction and rescales the magnitude of updates automatically. tensorflow</p> <p>In everyday terms: momentum helps you move steadily in the right direction, while adaptive scaling helps you avoid taking the same-size step on parameters that live in very different gradient regimes.  Adam still has a learning rate, and it still matters\u2014but Adam often works well with reasonable defaults, which is why it\u2019s commonly used as a first optimizer choice. tensorflow</p> <p></p> <p>Minimal Keras snippet: <code>keras.optimizers.Adam(...)</code> exposes <code>learning_rate</code>, <code>beta_1</code>, and <code>beta_2</code> (the EMA decay factors that control the \u201cmemory\u201d of the optimizer). tensorflow</p> <pre><code>from tensorflow import keras\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(\n        learning_rate=1e-3,\n        beta_1=0.9,\n        beta_2=0.999\n    ),  # Adam parameters \n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n</code></pre>"},{"location":"05-dl-fundamentals/optimization/#learning-rate-decay-schedule-smaller-steps-later","title":"Learning rate decay (schedule smaller steps later)","text":"<p>Learning rate decay means you intentionally change the learning rate during training, usually starting larger to make rapid progress and then reducing it so training can \u201csettle\u201d into a good solution instead of bouncing around near the end. A simple way to think about it: early training is about finding the right region of the loss landscape, and late training is about careful fine-tuning.</p> <p>In Keras, a common approach is to use a scheduler callback that updates the learning rate each epoch via a function <code>schedule(epoch, lr)</code> and applies the returned value. keras</p> <pre><code>from tensorflow import keras\n\n# Example: inverse time decay (simple, predictable)\ndef lr_schedule(epoch, lr):\n    initial_lr = 1e-3\n    decay_rate = 1.0\n    return initial_lr / (1.0 + decay_rate * epoch)\n\nlr_cb = keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1)  # schedule(epoch, lr) \n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\nhistory = model.fit(\n    x_train, y_train_oh,\n    validation_data=(x_val, y_val_oh),\n    epochs=20,\n    batch_size=128,\n    callbacks=[lr_cb]\n)\n</code></pre>"},{"location":"05-dl-fundamentals/optimization/#batch-normalization-stabilize-activations","title":"Batch normalization (stabilize activations)","text":"<p>Batch normalization (BN) normalizes a layer\u2019s activations during training using statistics computed from the current mini-batch, which often makes optimization easier by keeping activation scales more stable as training progresses. keras It also keeps moving averages of the batch mean and variance (stored as <code>moving_mean</code> and <code>moving_var</code>) so that at inference time it can use these learned running estimates instead of batch statistics. keras</p> <p>For this stage of the handbook (before CNNs), you\u2019ll mostly use BN in fully connected networks: a common pattern is <code>Dense \u2192 BatchNormalization \u2192 ReLU</code>. The Keras <code>Dense</code> docs also note that if a <code>Dense</code> layer is followed by <code>BatchNormalization</code>, it\u2019s recommended to set <code>use_bias=False</code> since BN has its own offset term. keras</p> <pre><code>from tensorflow import keras\n\ninputs = keras.Input(shape=(784,))  # e.g., flattened Fashion-MNIST\n\nx = keras.layers.Dense(256, use_bias=False)(inputs)  # recommended when followed by BN \nx = keras.layers.BatchNormalization()(x)             # BN layer (tracks moving_mean/moving_var) \nx = keras.layers.ReLU()(x)\n\nx = keras.layers.Dense(128, use_bias=False)(x)       # recommended when followed by BN \nx = keras.layers.BatchNormalization()(x)             # BN layer \nx = keras.layers.ReLU()(x)\n\noutputs = keras.layers.Dense(10, activation=\"softmax\")(x)\nmodel = keras.Model(inputs, outputs)\n</code></pre>"},{"location":"05-dl-fundamentals/optimization/#summary","title":"Summary","text":"Technique Core idea Helps with Tradeoffs / gotchas When to use Input normalization (rescale / standardize) Put inputs on a consistent scale. Smoother training, less learning-rate sensitivity. Standardization: compute stats on train only (e.g., via adapt). Almost always. Batch gradient descent Gradient from all NNN examples per update. Stable updates. Slow/expensive; ~1 update per epoch. Small datasets, debugging. SGD (stochastic) Gradient from 1 example per update. Fast feedback; noise can help exploration. Very noisy; unstable without tuning. Mostly conceptual; rare \u201cas-is\u201d. Mini-batch GD Gradient from B examples per update. Best speed/stability tradeoff. Batch size impacts noise + memory. Default for deep learning. Moving average (EMA) Smooth noisy signals with an exponential average. Stability; foundation of momentum/RMSProp/Adam. High decay = smoother but slower response. Whenever gradients are noisy. Momentum Track \u201cvelocity\u201d (EMA of gradients) to smooth direction. Less zig-zag; faster progress in consistent directions. Can overshoot with high learning rate/momentum. When SGD is jittery/slow. RMSProp EMA of squared gradients \u2192 per-parameter scaling. Handles uneven curvature; stabilizes training. Still needs learning-rate tuning; rho/epsilon can matter. When SGD struggles; solid general option. Adam Momentum-like direction + RMSProp-like scaling. Strong default; works well with minimal tuning. Learning rate still matters; sometimes worse generalization than SGD. First baseline optimizer. Learning rate decay Reduce learning rate over epochs via a schedule. Fast start + stable fine-tuning. Bad schedule can stall learning. When learning slows; for better final results. Batch normalization (BN) Normalize activations during training; keep moving stats for inference. More stable optimization; often allows higher learning rates. Train vs inference behavior differs; placement matters. Deeper MLPs; unstable/sensitive training."},{"location":"05-dl-fundamentals/optimization/#practical-workflow","title":"Practical workflow","text":"<p>Use this as a repeatable checklist when optimization is the bottleneck.</p> <ol> <li>Normalize inputs first (rescale images or use a normalization layer).</li> <li>Start with a reliable optimizer (Adam is a common default), and keep other knobs fixed.</li> <li>Pick a reasonable batch size (e.g., 64\u2013256) and train for a small number of epochs to validate the setup.</li> <li>Watch training + validation curves, not just final accuracy.</li> <li>If training is unstable:</li> <li>Lower learning rate.</li> <li>Add batch normalization (especially in deeper networks).</li> <li>Try momentum (SGD+momentum) or RMSProp if needed.</li> <li>Add learning rate decay once the model is learning but improvements slow down.</li> </ol>"},{"location":"05-dl-fundamentals/optimization/#common-mistakes-and-fixes","title":"Common mistakes (and fixes)","text":"<ul> <li>Mistake: No input normalization. Fix: rescale/standardize inputs early in the pipeline.</li> <li>Mistake: Learning rate too high. Fix: reduce LR by 10\u00d7 and retry before changing architecture.</li> <li>Mistake: Confusing optimizer problems with model capacity. Fix: stabilize optimization first, then adjust model size.</li> <li>Mistake: Changing many knobs at once. Fix: one change per run; keep short notes.</li> </ul>"},{"location":"05-dl-fundamentals/optimization/#quick-glossary","title":"Quick glossary","text":"<ul> <li>Hyperparameter: a setting you choose (learning rate, batch size, optimizer, momentum).</li> <li>Saddle point: a flat-ish point that is not a true minimum; gradients can be small but you\u2019re not \u201cdone.\u201d</li> <li>SGD: stochastic gradient descent (often implemented as mini-batch updates in practice).</li> <li>Mini-batch gradient descent: update using a batch (typical default).</li> <li>Moving average: a smoothed estimate of a noisy signal across steps.</li> <li>Momentum: uses a moving average of gradients to smooth direction.</li> <li>RMSProp: uses a moving average of squared gradients to adapt step sizes.</li> <li>Adam: combines momentum-like direction + RMSProp-like scaling.</li> <li>Learning rate decay: decreases learning rate over epochs/steps to fine-tune.</li> <li>Batch normalization: normalizes activations during training and uses moving averages for inference. </li> </ul> <p>Optimize like a scientist: keep the data clean, change one knob at a time, and trust the process more than your guesses.</p>"},{"location":"05-dl-fundamentals/regularization/","title":"Regularization Techniques for Deep Learning","text":"<p>Regularization is the set of methods that improves generalization: you intentionally limit how \u201cbrittle\u201d or overly complex your learned solution can become so it performs well on new data.  </p> <p>In real projects, it\u2019s how you convert \u201cmy model overfits\u201d into concrete levers you can tune (penalties, noise injection, stopping rules, normalization choices) instead of guessing architecture changes.</p> <p>Running symptom: training loss keeps decreasing while validation loss stops improving (or gets worse) \u2192 you likely need regularization.</p> <p></p>"},{"location":"05-dl-fundamentals/regularization/#why-it-matters-and-what-it-fixes","title":"Why it matters (and what it fixes)","text":"<p>Deep networks can fit extremely complex functions, so they may learn real signal and accidental patterns from the training set (noise, leakage-like artifacts, rare correlations).  </p> <p>Regularization helps in three common industry situations</p> <ol> <li>Limited labeled data</li> <li>Non-stationary data (drift)</li> <li>Label noise (imperfect ground truth).</li> </ol> <p>A practical goal is not \u201cmaximize train accuracy,\u201d but \u201cmaximize validation/test performance at a stable operating point\u201d (reproducible across seeds/splits, robust to minor data changes).  </p> <p>That\u2019s why teams treat regularization as part of the model\u2019s reliability toolkit, not as an optional trick.</p>"},{"location":"05-dl-fundamentals/regularization/#biasvariance-tradeoff-how-to-decide","title":"Bias\u2013variance tradeoff (how to decide)","text":"<p>Bias is error from overly simplistic assumptions (underfitting): both training and validation performance are poor. </p> <p>Variance is error from being too sensitive to the training set (overfitting): training performance is strong but validation/test is much worse. </p> <p>Most regularization methods increase bias slightly while reducing variance more, so validation/test performance improves when variance is your bottleneck. </p> <p>The fastest workflow is: diagnose bias vs variance from learning curves, then change one knob at a time (so you can trust causality). </p> <p>Rule of thumb: if you\u2019re overfitting, add regularization; if you\u2019re underfitting, reduce regularization or add capacity/signal. </p> <p></p>"},{"location":"05-dl-fundamentals/regularization/#core-techniques-intuition-why-it-works-keras","title":"Core techniques (intuition + \"why it works\" + Keras)","text":"<p>Below, each technique includes (1) intuition, (2) why it works, (3) how to implement in Keras, and (4) professional usage guidance.</p>"},{"location":"05-dl-fundamentals/regularization/#1-l1-l2-regularization-weight-penalties","title":"1) L1 / L2 regularization (weight penalties)","text":"<p>What it is You add a penalty term to the loss so the optimizer prefers smaller (or sparser) weights instead of \u201cspiky\u201d solutions that memorize training quirks. In Keras you attach penalties with <code>kernel_regularizer</code>, <code>bias_regularizer</code>, or <code>activity_regularizer</code> on layers. keras</p> <p>Intuition </p> <ul> <li>L2: \u201cdon\u2019t let any weight become too large,\u201d which tends to make the learned function smoother and less sensitive to small input changes.  </li> <li>L1: \u201cprefer zero weights when possible,\u201d which encourages sparsity (some connections become exactly 0).</li> </ul> <p>Why it works (mechanism) You\u2019re optimizing a modified objective like \\(J(\\theta) = \\text{data\\_loss}(\\theta) + \\lambda \\cdot \\Omega(\\theta)\\), where \\(\\Omega(\\theta)\\) penalizes complexity (e.g., \\(\\|W\\|_2^2\\) for L2 or \\(\\|W\\|_1\\) for L1). This reduces variance because many \u201cmemorizing\u201d solutions require large or fragile parameter configurations.</p> <p>Keras snippet (Dense + L2) <pre><code>from tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\n\nmodel = keras.Sequential([\n    layers.Dense(\n        256, activation=\"relu\",\n        kernel_regularizer=regularizers.L2(1e-4)\n    ),\n    layers.Dense(1, activation=\"sigmoid\")\n])\n</code></pre></p> <p>Keras snippet (L1+L2, plus showing options) </p> <p>Keras supports <code>L1</code>, <code>L2</code>, and <code>L1L2</code>, and you can regularize kernel, bias, and activity. keras <pre><code>from tensorflow.keras import layers, regularizers\n\nlayer = layers.Dense(\n    64,\n    kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n    bias_regularizer=regularizers.L2(1e-4),\n    activity_regularizer=regularizers.L2(1e-5),\n)\n</code></pre></p> <p>Industry best practices</p> <ul> <li>Use L2 as a default baseline regularizer for MLPs and many CNN-style blocks; tune \\(\\lambda\\) on validation (it\u2019s a real hyperparameter).  </li> <li>Prefer small, consistent penalties over huge penalties on a few layers (large penalties often cause underfitting and slow learning).  </li> <li>If you\u2019re using BatchNorm everywhere, be cautious with aggressive L2 on layers whose scale is later normalized; tune carefully rather than assuming \u201cmore is better.\u201d</li> </ul>"},{"location":"05-dl-fundamentals/regularization/#2-dropout","title":"2) Dropout","text":"<p>What it is </p> <p>Dropout randomly sets input units to 0 with frequency <code>rate</code> at each training step, which helps prevent overfitting. keras Keras scales the remaining (not-dropped) activations by \\(1/(1-\\text{rate})\\) so the expected sum/scale stays consistent. </p> <p>Intuition Dropout forces redundancy: the network cannot rely on any single neuron being present, so it must learn distributed, robust features. A useful mental model is \u201canti-co-adaptation\u201d: prevent units from only working in brittle combinations.</p> <p></p> <p>Why it works (mechanism) </p> <p>Dropout injects structured noise into the forward pass during training, which discourages memorization and reduces variance. It often behaves like a cheap form of ensembling: training many \u201cthinned\u201d subnetworks and using the full network at inference.</p> <p>Important behavior (professional gotcha) </p> <p>Dropout only applies when the layer is called with <code>training=True</code>, so no values are dropped during inference.  Setting <code>trainable=False</code> does not disable dropout behavior because Dropout has no weights to freeze; the <code>training</code> flag is what matters. </p> <p>Keras snippet <pre><code>from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(256, activation=\"relu\"),\n    layers.Dropout(0.3),\n    layers.Dense(1, activation=\"sigmoid\")\n])\n</code></pre></p> <p>Industry best practices</p> <ul> <li>Treat dropout rate as a tuning knob: too low won\u2019t help; too high can cause underfitting or slow convergence.  </li> <li>Use dropout more in large dense blocks (where overfitting is common) and be more conservative in early convolutional feature extractors unless you have a reason.  </li> <li>If your pipeline already uses strong regularization (L2 + BN + data augmentation), add dropout only if the generalization gap persists.</li> </ul>"},{"location":"05-dl-fundamentals/regularization/#3-early-stopping","title":"3) Early stopping","text":"<p>What it is Early stopping halts training when a monitored validation quantity stops improving, controlled by parameters like <code>monitor</code>, <code>min_delta</code>, and <code>patience</code>.  <code>restore_best_weights</code> restores model weights from the epoch with the best value of the monitored quantity. </p> <p>Intuition Many networks first learn general patterns, then later start fitting idiosyncrasies of the training set; early stopping ends training before the \u201cmemorization phase\u201d dominates. It\u2019s the most direct \u201cdon\u2019t over-train\u201d mechanism because it\u2019s driven by the metric you care about.</p> <p></p> <p>Why it works (mechanism) Training longer effectively increases the model\u2019s ability to fit noise; stopping earlier is a form of capacity control through the optimization path. It also saves compute, which matters in real training loops and hyperparameter searches.</p> <p>Keras snippet (recommended defaults) <pre><code>from tensorflow import keras\n\nearly_stop = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=5,\n    min_delta=0.0,\n    restore_best_weights=True\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=200,\n    callbacks=[early_stop]\n)\n</code></pre></p> <p>Industry best practices</p> <ul> <li>Always keep a true test set untouched; early stopping repeatedly consults validation, so validation is no longer a clean \u201cfinal exam.\u201d  </li> <li>Use <code>patience</code> to avoid stopping on random metric noise, and prefer monitoring <code>val_loss</code> unless a business metric is truly the objective.   </li> <li>Log the best epoch and best validation metric; in professional reporting, \u201cwhich epoch did we ship?\u201d must be traceable.</li> </ul>"},{"location":"05-dl-fundamentals/regularization/#4-batch-normalization-bn","title":"4) Batch Normalization (BN)","text":"<p>What it is BatchNorm normalizes activations and then applies learned scale (<code>gamma</code>) and offset (<code>beta</code>). </p> <p>During training it uses batch statistics, while during inference it uses moving averages accumulated during training. </p> <p>Intuition BN stabilizes the scale of internal signals so optimization is less fragile (you often can train faster and with less sensitivity to initialization). It can also regularize because batch-to-batch statistic noise perturbs activations during training.</p> <p>Why it works (mechanism) BN has different behavior in training vs inference, which is critical to understand when evaluating models or exporting to production.  Keras documents the moving-stat updates as <code>moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)</code> and similarly for variance. [ppl-ai-file-upload.s3.amazonaws]</p> <p></p> <p>Keras snippet (common placement) <pre><code>from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Dense(256, use_bias=False),\n    layers.BatchNormalization(),\n    layers.Activation(\"relu\"),\n    layers.Dense(1, activation=\"sigmoid\")\n])\n</code></pre></p> <p>Professional gotcha (fine-tuning) In Keras, setting <code>trainable = False</code> on a <code>BatchNormalization</code> layer makes the layer run in inference mode (it will use moving mean/variance rather than batch stats). [ppl-ai-file-upload.s3.amazonaws]</p> <p>This matters a lot during transfer learning and when you freeze parts of a network. [ppl-ai-file-upload.s3.amazonaws]</p> <p>Industry best practices</p> <ul> <li>BN is usually helpful for optimization stability, but it can be tricky with very small batch sizes (batch statistics get noisy), so monitor carefully.  </li> <li>If you already have BN in many blocks, you may need less dropout; don\u2019t stack regularizers blindly\u2014measure the generalization gap.</li> </ul>"},{"location":"05-dl-fundamentals/regularization/#a-practical-regularize-like-a-pro-workflow","title":"A practical \u201cregularize like a pro\u201d workflow","text":"<p>Regularization works best as a workflow, not a checklist of tricks: diagnose \u2192 pick a minimal intervention \u2192 validate \u2192 iterate. </p> <p>Use learning curves and train/val gap as your main debugging signal, then confirm improvements on a held-out test set only at the end. </p> <p>Minimal baseline recipe (common in teams) </p> <ul> <li>Start with: sensible model capacity + good data split + early stopping.   </li> <li>Add: light L2 on large trainable layers (especially if you see a persistent generalization gap).</li> <li>Add: BatchNorm if training is unstable or you want faster convergence; then reassess whether dropout is still needed.</li> </ul> <p>One combined Keras template (Sequential) This is not \u201cthe best model,\u201d it\u2019s a clean starting point you can tune systematically. <pre><code>from tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\n\nmodel = keras.Sequential([\n    layers.Dense(256, use_bias=False,\n                 kernel_regularizer=regularizers.L2(1e-4)),\n    layers.BatchNormalization(),\n    layers.Activation(\"relu\"),\n    layers.Dropout(0.2),\n\n    layers.Dense(128, use_bias=False,\n                 kernel_regularizer=regularizers.L2(1e-4)),\n    layers.BatchNormalization(),\n    layers.Activation(\"relu\"),\n    layers.Dropout(0.2),\n\n    layers.Dense(1, activation=\"sigmoid\")\n])\n\nmodel.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\nearly_stop = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=5,\n    restore_best_weights=True\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    epochs=100,\n    callbacks=[early_stop]\n)\n</code></pre></p> <p>Common pitfalls (quick checks)</p> <ul> <li>If train and val are both bad, don\u2019t add more regularization\u2014fix bias first (capacity, features, training setup). </li> <li>If your validation metric jumps around a lot, use patience in early stopping and consider whether your validation set is too small or non-representative.   </li> <li>If you freeze layers for fine-tuning, remember BN\u2019s special behavior when <code>trainable=False</code>. </li> <li>If you suspect dropout is affecting evaluation, verify you\u2019re not calling layers with <code>training=True</code> during inference; dropout only activates in training mode. </li> </ul>"},{"location":"05-dl-fundamentals/regularization/#quick-glossary","title":"Quick glossary","text":"<ul> <li>Regularizer (Keras): penalties attached to a layer (kernel/bias/activity) that contribute extra terms to the training loss. </li> <li>Dropout rate: fraction of units set to 0 during training; Keras scales survivors by \\(1/(1-\\text{rate})\\). </li> <li>Early stopping patience: number of epochs with no improvement before stopping; <code>restore_best_weights</code> restores the best epoch\u2019s weights.   </li> <li>BatchNorm training vs inference: training uses batch stats; inference uses moving averages collected during training. </li> </ul> <p>Measure the gap, choose one lever, validate the change, and ship. Repeat until generalization becomes habit</p> <p></p>"},{"location":"05-dl-fundamentals/tf-keras/","title":"\ud83d\udea7 Under Construction \ud83d\udea7","text":"<p>This page is being built faster than a neural net on GPU! Check back soon - good things take time (and debugging). \ud83d\ude04</p>"},{"location":"06-dl-architectures/","title":"Overview","text":"<p>Build computer vision systems that actually see. Build computer vision systems that work. Create models used in apps like self-driving cars and medical imaging.</p> <p></p>"},{"location":"06-dl-architectures/#convolutions-and-pooling","title":"Convolutions and Pooling","text":"<p>Teach models to \"see\" edges and shapes</p> <p>Convolutions slide over pixels to find patterns automatically. Your project uses these exact operations.</p> <p>You'll learn: How filters detect edges/textures, stride/padding control, pooling to focus on important areas</p>"},{"location":"06-dl-architectures/#convolutional-neural-networks","title":"Convolutional Neural Networks","text":"<p>Complete image classifiers from scratch</p> <p>Stack conv \u2192 pool \u2192 dense layers into working CNNs. See your classification project come alive.</p> <p>You'll learn: Full CNN pipeline, feature extraction step-by-step, why CNNs beat regular networks for images</p>"},{"location":"06-dl-architectures/#data-augmentation","title":"Data Augmentation","text":"<p>Turn 100 photos into 10,000 training examples</p> <p>Flip, rotate, zoom - make models ready for the real world. Instant accuracy boost for your CNN project.</p> <p>You'll learn: Smart transforms that preserve labels, augmentation pipelines, balancing strength</p>"},{"location":"06-dl-architectures/#deep-convolutional-architectures","title":"Deep Convolutional Architectures","text":"<p>Scale to 100M+ parameter vision models</p> <p>ResNet connections + VGG blocks = production CNNs. Build the deep networks from your project.</p> <p>You'll learn: Residual blocks that train deep, depth vs width, modern ImageNet winners</p>"},{"location":"06-dl-architectures/#transfer-learning","title":"Transfer Learning","text":"<p>Borrow years of training in 5 minutes</p> <p>Start with ImageNet-pretrained models. Fine-tune for your data - the smart way to build vision apps.</p> <p>You'll learn: Freeze/extract/fine-tune strategies, avoiding common transfer pitfalls</p>"},{"location":"06-dl-architectures/#object-detection","title":"Object Detection","text":"<p>Draw boxes around every object</p> <p>Find where AND what is in images. Your project's anchors + NMS = production object detection.</p> <p>You'll learn: Bounding boxes, IoU matching, non-max suppression, mAP evaluation</p> <p>Real vision systems = CNN engineering. Right data + architecture + validation = models that work on your phone camera tomorrow.</p>"},{"location":"06-dl-architectures/cnns/","title":"Convolutional Neural Networks","text":"<p>Convolutional Neural Networks (CNNs) are neural networks built for images. They use convolution layers to spot small patterns (like edges or corners) and often use pooling layers to shrink the image representation so the model is faster and needs fewer weights.</p> <p>The big idea: find simple patterns first, then combine them into more meaningful features as you go deeper in the network.</p> <p></p>"},{"location":"06-dl-architectures/cnns/#what-they-solve-why-cnns-work","title":"What they solve (why CNNs work)","text":"<p>In images, nearby pixels usually belong together (a line, a corner, a texture), and the same kind of pattern can show up anywhere in the picture. </p> <p>A convolution layer takes one small \u201cpattern detector\u201d (a filter) and slides it across the whole image, so it can spot that pattern no matter where it appears, without needing separate weights for every location.</p> <p>Rule of thumb: Convolution finds useful patterns; downsampling (pooling or using a bigger stride) makes feature maps smaller, which speeds things up and makes the model less sensitive to small shifts.</p>"},{"location":"06-dl-architectures/cnns/#layers-convolution-and-pooling","title":"Layers: convolution and pooling","text":""},{"location":"06-dl-architectures/cnns/#intuition-first-what-these-layers-do","title":"Intuition first (what these layers do)","text":"<p>A CNN is usually built from repeating \u201cblocks\u201d: convolution \u2192 (optional) pooling/downsampling, and then a small classifier at the end (often Dense layers).  </p> <p>Convolution layers learn to detect small patterns (like edges) and turn them into feature maps, while pooling (or strided convolution) makes those maps smaller so the next layers are faster and focus on the most important signals.</p> <p>Typical flow: <code>Conv2D (+ activation)</code> \u2192 <code>Conv2D</code> \u2192 <code>MaxPooling2D</code> \u2192 repeat \u2192 <code>Flatten</code>/<code>GlobalAveragePooling2D</code> \u2192 <code>Dense</code></p>"},{"location":"06-dl-architectures/cnns/#convolutional-layer-conv2d-what-it-is","title":"Convolutional layer (Conv2D) \u2014 what it is","text":"<p>In Keras, <code>Conv2D</code> creates a convolution kernel and applies it across the input\u2019s height and width to produce output feature maps.  Conceptually: a small filter slides over the image; at each location it computes a score, and that score becomes one pixel in an output feature map. </p> <p>Key knobs you choose</p> <ul> <li><code>filters</code>: how many feature maps you want (more filters = more pattern types). </li> <li><code>kernel_size</code>: the window size (commonly <code>(3, 3)</code>). </li> <li><code>strides</code>: step size of the slide (bigger stride = smaller output). </li> <li><code>padding</code>: <code>\"valid\"</code> (no padding) or <code>\"same\"</code> (keep size when <code>strides=1</code>). </li> </ul> <p>Shapes (channels_last)</p> <ul> <li>Input: <code>(batch, height, width, channels)</code></li> <li>Output: <code>(batch, new_height, new_width, filters)</code> </li> </ul> <p></p>"},{"location":"06-dl-architectures/cnns/#pooling-layer-maxpooling2d-what-it-is","title":"Pooling layer (MaxPooling2D) \u2014 what it is","text":"<p><code>MaxPooling2D</code> downsamples by taking the maximum value in each window, independently for each channel, sliding the window using <code>strides</code>.   </p> <p>Pooling keeps the number of channels the same and mainly reduces height/width (so compute drops). </p> <p>Key knobs you choose</p> <ul> <li><code>pool_size</code>: window size (commonly <code>(2, 2)</code>). </li> <li><code>strides</code>: step size (often equals <code>pool_size</code>). </li> <li><code>padding</code>: <code>\"valid\"</code> or <code>\"same\"</code>. </li> </ul> <p>Shape idea (channels_last)</p> <ul> <li>Input: <code>(batch, height, width, channels)</code></li> <li>Output: <code>(batch, pooled_height, pooled_width, channels)</code> </li> </ul>"},{"location":"06-dl-architectures/cnns/#how-to-build-these-blocks-in-keras","title":"How to build these blocks in Keras","text":"<p>1) One convolution layer <pre><code>from tensorflow.keras import layers\n\nx = layers.Conv2D(\n    filters=32,\n    kernel_size=(3, 3),\n    strides=(1, 1),\n    padding=\"same\",\n    activation=\"relu\",\n)(x)\n</code></pre></p> <p>2) Add max pooling (common downsampling) <pre><code>x = layers.MaxPooling2D(\n    pool_size=(2, 2),\n    strides=(2, 2),\n    padding=\"valid\",\n)(x)\n</code></pre></p> <p>3) A small CNN you can extend <pre><code>from tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Input(shape=(128, 128, 3)),   # channels_last\n\n    layers.Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"),\n    layers.MaxPooling2D((2, 2)),\n\n    layers.Flatten(),\n    layers.Dense(128, activation=\"relu\"),\n    layers.Dense(10)  # logits for 10 classes\n])\n</code></pre></p> <p>Rule of thumb: <code>Conv2D</code> grows \u201cwhat you know\u201d (features/channels), and pooling/stride shrinks \u201cwhere you know it\u201d (height/width).</p>"},{"location":"06-dl-architectures/cnns/#forward-propagation-conv-pool","title":"Forward propagation (conv + pool)","text":"<p>Forward propagation is just the \u201cdata flow\u201d through your CNN: you start with an input image (or a batch of images), apply convolutions to create feature maps, apply activations (e.g., ReLU), and sometimes downsample with pooling to make the next layers cheaper.</p>"},{"location":"06-dl-architectures/cnns/#convolution-forward-pass-what-happens","title":"Convolution forward pass (what happens)","text":"<p>At a high level, a convolution layer scans the input with each filter and produces one output feature map per filter (so <code>filters=64</code> means 64 output channels). The important practical detail during the forward pass is shape tracking: stride and padding decide how fast spatial size shrinks, while the number of filters decides how many channels you produce.</p> <p>Shape checklist (channels_last)</p> <ul> <li>Input: <code>(batch, H, W, Cin)</code></li> <li>Output after conv: <code>(batch, H2, W2, Cout)</code> where <code>Cout = filters</code></li> <li>With <code>padding=\"same\"</code> and <code>strides=1</code>, Keras keeps spatial size (so <code>H2 = H</code>, <code>W2 = W</code>). </li> <li>With <code>padding=\"valid\"</code>, spatial size usually gets smaller because you don\u2019t pad borders. </li> </ul>"},{"location":"06-dl-architectures/cnns/#pooling-forward-pass-max-pooling","title":"Pooling forward pass (max pooling)","text":"<p>Pooling does not create new channels; it only reduces height/width by summarizing small windows (for max pooling: keep the maximum). Keras defines <code>MaxPooling2D</code> as downsampling by taking the maximum value over a window for each channel and shifting the window by <code>strides</code>. </p> <p>Output size (quick reference) Keras provides the output-size formulas for max pooling:</p> <ul> <li>For <code>padding=\"valid\"</code>: <code>out = floor((in - pool_size) / strides) + 1</code> </li> <li>For <code>padding=\"same\"</code>: <code>out = floor((in - 1) / strides) + 1</code> </li> </ul>"},{"location":"06-dl-architectures/cnns/#what-learners-should-do-in-practice","title":"What learners should do in practice","text":"<ul> <li>Write down the tensor shape after every layer (or call <code>model.summary()</code> early and often).</li> <li>Decide where you want to downsample (after every 1\u20132 conv layers is common), and confirm the spatial sizes match your plan.</li> <li>If your model becomes too slow or too big, the first knobs to adjust are: earlier downsampling (pooling or larger stride) and fewer filters.</li> </ul>"},{"location":"06-dl-architectures/cnns/#tiny-keras-snippet-watch-shapes-during-the-forward-pass","title":"Tiny Keras snippet: watch shapes during the forward pass","text":"<pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Input(shape=(128, 128, 3)),\n\n    layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n    layers.MaxPooling2D(2),\n\n    layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n    layers.MaxPooling2D(2),\n])\n\nmodel.summary()\n</code></pre>"},{"location":"06-dl-architectures/cnns/#optional-debugging-trick-inspect-intermediate-outputs","title":"Optional debugging trick: inspect intermediate outputs","text":"<pre><code>debug_model = tf.keras.Model(model.input, [layer.output for layer in model.layers])\n\nx = tf.random.uniform((1, 128, 128, 3))\nouts = debug_model(x)\n\nfor i, o in enumerate(outs):\n    print(i, o.shape)\n</code></pre>"},{"location":"06-dl-architectures/cnns/#back-propagation-conv-pool","title":"Back propagation (conv + pool)","text":"<p>Backpropagation is how the network learns. After a forward pass computes a loss (how wrong the prediction was), backprop sends a \u201cblame signal\u201d (the gradient) backward to answer two questions:</p> <ul> <li>Which weights should change?</li> <li>In which direction, and by how much?</li> </ul> <p>Think of it as: forward pass builds the answer, backward pass tells every layer how to improve it.</p>"},{"location":"06-dl-architectures/cnns/#1-convolution-backprop-what-gets-updated","title":"1) Convolution backprop (what gets updated)","text":"<p>A convolution layer has three main things gradients flow through:</p> <p>A) Bias gradients</p> <ul> <li>Each output channel has (usually) one bias value.</li> <li>The bias gradient is basically: \u201cadd up the gradient values for that channel across all spatial positions (and across the batch).\u201d</li> </ul> <p>B) Filter (kernel) gradients</p> <ul> <li>The same filter is used at many positions during the forward pass.</li> <li>So during backprop, the filter\u2019s gradient is the sum of contributions from every position where it was applied.</li> <li>Intuition: if a filter helped reduce the loss in many places, its update will be larger.</li> </ul> <p>C) Input gradients</p> <ul> <li>Each output value came from a small input patch.</li> <li>During backprop, the gradient from an output value is sent back to the pixels in that patch, scaled by the filter weights.</li> <li>Result: pixels that influenced many outputs (because many windows covered them) collect more gradient.</li> </ul> <p>Learner mental model: convolution backprop is still \u201csliding-window math,\u201d just running in reverse to compute updates.</p>"},{"location":"06-dl-architectures/cnns/#2-max-pooling-backprop-only-the-winner-gets-the-gradient","title":"2) Max pooling backprop (only the winner gets the gradient)","text":"<p>Max pooling keeps the largest value in each window. So in the backward pass:</p> <ul> <li>The gradient goes only to the element that was the maximum in that window.</li> <li>Every other element in that window gets 0 gradient.</li> </ul> <p>Practical detail: implementations keep track of the \u201cwinning index\u201d (argmax) from the forward pass so they know where to send the gradient in the backward pass.</p> <p>Why this matters: max pooling gradients are sparse (most positions get zero), which can make learning depend heavily on the strongest activations.</p>"},{"location":"06-dl-architectures/cnns/#3-average-pooling-backprop-everyone-shares","title":"3) Average pooling backprop (everyone shares)","text":"<p>Average pooling outputs the mean of a window. So in the backward pass:</p> <ul> <li>The incoming gradient is split evenly across all elements in the window.</li> <li>If the window has 4 elements (2\u00d72), each gets about one quarter of the gradient (per channel).</li> </ul> <p>Compared to max pooling, average pooling gradients are dense (many positions receive some gradient).</p> <p>Rule of thumb: Convolution spreads learning across many locations (shared filters); max pooling routes learning through \u201cwinners only.\u201d</p>"},{"location":"06-dl-architectures/cnns/#build-a-cnn-in-tensorflowkeras-end-to-end","title":"Build a CNN in TensorFlow/Keras (end-to-end)","text":"<p>A clean beginner-friendly CNN is: a small convolutional \u201cfeature extractor\u201d (Conv2D + MaxPooling2D stacks) followed by a classifier head (Flatten + Dense). TensorFlow\u2019s CNN tutorial uses a Sequential model for CIFAR-10 with three <code>Conv2D</code> layers, two <code>MaxPooling2D</code> layers, then <code>Flatten</code> and <code>Dense</code> layers. tensorflow</p> <p>Model code (template you can reuse) <pre><code>import tensorflow as tf\nfrom tensorflow.keras import layers, models\n\nmodel = models.Sequential([\n    layers.Input(shape=(32, 32, 3)),  # channels_last\n\n    layers.Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"),\n    layers.MaxPooling2D(pool_size=(2, 2)),\n\n    layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"),\n    layers.MaxPooling2D(pool_size=(2, 2)),\n\n    layers.Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\"),\n\n    layers.Flatten(),\n    layers.Dense(128, activation=\"relu\"),\n    layers.Dense(10)  # logits for 10 classes\n])\n\nmodel.compile(\n    optimizer=\"adam\",\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"]\n)\n\nmodel.summary()\n</code></pre></p> <p>Training loop (minimal) <pre><code># Example: x_train shape (N, 32, 32, 3), y_train shape (N,)\nhistory = model.fit(\n    x_train, y_train,\n    validation_data=(x_val, y_val),\n    epochs=10,\n    batch_size=64\n)\n</code></pre></p>"},{"location":"06-dl-architectures/cnns/#practical-best-practices-things-that-prevent-bugs","title":"Practical best practices (things that prevent bugs)","text":"<ul> <li> <p>Track shapes on purpose. Decide one data format and stick to it (<code>channels_last</code> is the common default), because <code>Conv2D</code> expects different tensor layouts depending on <code>data_format</code>, and this is a very common source of shape mistakes.    Practical habit: run <code>model.summary()</code> whenever you add/remove a layer, and make sure the height/width are shrinking exactly when you planned.</p> </li> <li> <p>Use padding intentionally (especially early layers). If you want to keep the same height/width while you extract early features, use <code>padding=\"same\"</code> with <code>strides=1</code> (Keras notes this preserves spatial size in that case).    Then downsample deliberately (either with pooling or with a larger stride) so you control the speed/accuracy trade-off instead of shrinking \u201cby accident.\u201d</p> </li> <li> <p>Know how pooling changes size. With pooling, <code>\"valid\"</code> typically shrinks more because there is no padding, while <code>\"same\"</code> keeps output sizes larger; Keras provides explicit output-shape formulas for both modes.    Practical habit: before training, write the planned sizes (e.g., 128\u219264\u219232\u219216) next to your pooling/stride choices and verify they match the model summary.</p> </li> </ul>"},{"location":"06-dl-architectures/cnns/#quick-glossary","title":"Quick glossary","text":"<ul> <li> <p>Conv2D: A layer that learns a set of small filters (kernels) and slides them across the input image (height \u00d7 width) to produce output feature maps.    In Keras, it returns <code>activation(conv2d(inputs, kernel) + bias)</code> (activation and bias are optional). </p> </li> <li> <p>Kernel / filter: The small grid of learnable numbers used by <code>Conv2D</code>. One filter produces one output channel (one feature map). </p> </li> <li> <p>Feature map: The output produced by applying one filter across the image; it shows where that filter \u201cfires strongly\u201d in different locations.</p> </li> <li> <p>Stride: How many pixels the filter (or pooling window) moves each step. Bigger stride usually means a smaller output (more downsampling). </p> </li> <li> <p>Padding: What you do at the borders before sliding the window. <code>padding=\"valid\"</code> means no padding, while <code>padding=\"same\"</code> pads evenly; for <code>Conv2D</code>, Keras notes that with <code>padding=\"same\"</code> and <code>strides=1</code>, the output has the same height/width as the input. </p> </li> <li> <p>MaxPooling2D: A layer that downsamples by taking the maximum value inside each window (per channel), moving the window by <code>strides</code>. </p> </li> </ul> <p>Everything should be made as simple as possible, but not simpler.</p> <p>\u2014 Albert Einstein </p>"},{"location":"06-dl-architectures/convolutions-pooling/","title":"Convolution and Pooling (Professional Guide)","text":"<p>Convolution and pooling are the core building blocks that make CNNs effective for images: they let a model detect local patterns (edges, corners, textures) and build them into higher-level features while keeping the number of parameters manageable.  </p> <p>They solve a practical deep-learning problem: fully-connected layers on raw images scale poorly (too many weights) and ignore spatial structure, while convolution exploits locality and weight sharing to learn \u201cthe same detector everywhere.\u201d</p> <p></p>"},{"location":"06-dl-architectures/convolutions-pooling/#what-problem-they-solve-why-cnns-work","title":"What problem they solve (why CNNs work)","text":"<p>Images have strong local structure: nearby pixels are related, and useful patterns repeat across the image (an edge is an edge whether it appears top-left or bottom-right).  </p> <p>Convolution layers implement this idea by sliding a small learnable filter across the image to produce a feature map, and Keras describes <code>Conv2D</code> as creating a convolution kernel convolved over height/width to produce outputs. </p> <p>Pooling layers then downsample feature maps (reduce height/width) to lower compute and make representations less sensitive to small shifts, and Keras describes max pooling as downsampling by taking the maximum over a window per channel. </p>"},{"location":"06-dl-architectures/convolutions-pooling/#convolution-the-essentials","title":"Convolution: the essentials","text":""},{"location":"06-dl-architectures/convolutions-pooling/#what-is-a-convolution-in-cnn-terms","title":"What is a convolution (in CNN terms)?","text":"<p>A 2D convolution layer slides a kernel (filter) over an input\u2019s spatial dimensions (height and width) and produces an output tensor (feature maps). </p> <p>At each position, the kernel and the local image patch are combined (in practice: element-wise multiply and sum), and the result becomes one cell in the output feature map.</p>"},{"location":"06-dl-architectures/convolutions-pooling/#what-is-a-kernel-filter","title":"What is a kernel / filter?","text":"<p>A kernel (filter) is the small learnable weight tensor used by a convolution layer, and Keras explicitly says <code>Conv2D</code> \u201ccreates a convolution kernel\u201d that is convolved with the input.  Each filter produces one output channel (one feature map), and multiple filters produce multiple feature maps (output channels). </p> <p></p>"},{"location":"06-dl-architectures/convolutions-pooling/#what-are-channels","title":"What are channels?","text":"<p>Channels are the depth dimension of an image-like tensor: e.g., RGB images typically have 3 channels. In Keras <code>channels_last</code> format, the input shape is <code>(batch_size, height, width, channels)</code>, and in <code>channels_first</code> it is <code>(batch_size, channels, height, width)</code>. </p> <p>Rule of thumb: most modern code uses <code>channels_last</code>, and you should keep it consistent throughout the model to avoid shape confusion. </p> <p></p>"},{"location":"06-dl-architectures/convolutions-pooling/#padding-stride-and-output-size-the-shape-controls","title":"Padding, stride, and output size (the \u201cshape controls\u201d)","text":""},{"location":"06-dl-architectures/convolutions-pooling/#what-is-padding","title":"What is padding?","text":"<p>Padding means adding extra border values around the input so the kernel can be applied at the edges. In Keras, <code>padding=\"valid\"</code> means no padding, while <code>padding=\"same\"</code> pads evenly left/right and up/down; when <code>padding=\"same\"</code> and <code>strides=1</code>, the output has the same spatial size as the input. </p>"},{"location":"06-dl-architectures/convolutions-pooling/#what-is-valid-padding-vs-same-padding","title":"What is \"valid\" padding vs \"same\" padding?","text":"<ul> <li>Valid padding: no padding; output spatial size usually shrinks after convolution.   </li> <li>Same padding: zero-padding is added so that (with stride 1) spatial size is preserved. [page:1]</li> </ul>"},{"location":"06-dl-architectures/convolutions-pooling/#what-is-a-stride","title":"What is a stride?","text":"<p>Stride is the step size of the sliding window. In Keras, <code>Conv2D</code> has a <code>strides</code> argument (default <code>(1,1)</code>), and pooling layers also shift the window by <code>strides</code> along each dimension. </p> <p>Intuition: stride controls downsampling. Larger stride means fewer kernel positions evaluated, so the output becomes smaller (but faster/cheaper). Pooling layers similarly use stride to move the pooling window across the feature map. </p>"},{"location":"06-dl-architectures/convolutions-pooling/#output-shape-intuition-quick-guide","title":"Output shape intuition (quick guide)","text":"<p>Keras documents that a <code>Conv2D</code> layer outputs a 4D tensor whose last dimension (for channels_last) is <code>filters</code>: <code>(batch_size, new_height, new_width, filters)</code>. </p> <p>Pooling outputs keep the same number of channels and change only spatial size, e.g. <code>(batch_size, pooled_height, pooled_width, channels)</code> for channels_last. </p> <p>Rule of thumb: Convolution changes \u201cchannels\u201d (via number of filters) and often changes spatial size; pooling typically keeps channels and reduces spatial size. </p>"},{"location":"06-dl-architectures/convolutions-pooling/#how-to-do-it-by-hand-worked-mini-examples","title":"How to do it by hand (worked mini-examples)","text":""},{"location":"06-dl-architectures/convolutions-pooling/#how-to-perform-a-convolution-over-an-image-single-channel","title":"How to perform a convolution over an image (single-channel)","text":"<p>We\u2019ll use a tiny grayscale image (5\u00d75) and a 3\u00d73 kernel, with stride 1 and <code>valid</code> padding (no border). </p> <p>The output will be 3\u00d73 because the 3\u00d73 kernel can be placed in 3 positions horizontally and 3 vertically without padding.</p> <p>Example image \\(X\\) (5\u00d75): <pre><code>X =\n[ 1,  2,  0,  1,  3]\n[ 4,  1,  1,  0,  2]\n[ 0,  2,  3,  1,  1]\n[ 1,  0,  2,  2,  0]\n[ 3,  1,  0,  1,  2]\n</code></pre></p> <p>Example kernel \\(K\\) (3\u00d73): <pre><code>K =\n[ 1,  0, -1]\n[ 1,  0, -1]\n[ 1,  0, -1]\n</code></pre></p> <p>Compute output[0,0] (top-left placement):</p> <ol> <li>Take the top-left 3\u00d73 patch of <code>X</code>.</li> <li>Multiply element-wise by <code>K</code>.</li> <li>Sum all 9 products \u2192 that sum is the output cell.</li> </ol> <p>Then slide the kernel one step to the right (stride 1) to compute output[0,1], repeat across the row, then move down and repeat for the next output row. This is exactly the \u201ckernel convolved with the input over height/width\u201d behavior that <code>Conv2D</code> implements. </p> <p></p>"},{"location":"06-dl-architectures/convolutions-pooling/#how-convolution-works-with-channels-rgb-intuition","title":"How convolution works with channels (RGB intuition)","text":"<p>If the input has channels (e.g., RGB), a single filter spans all input channels, so it has shape <code>(kernel_h, kernel_w, in_channels)</code>, and produces one feature map (one output channel).  Using <code>filters=N</code> means you learn N different filters and output N feature maps (output channels). </p>"},{"location":"06-dl-architectures/convolutions-pooling/#how-to-perform-max-pooling-over-an-image-feature-map","title":"How to perform max pooling over an image (feature map)","text":"<p>Max pooling downsamples by taking the maximum value in each window for each channel. </p> <p>Use a 2\u00d72 pool with stride 2 (common default behavior).</p> <p>Example feature map (4\u00d74): <pre><code>A =\n[ 1,  3,  2,  0]\n[ 4,  6,  1,  2]\n[ 0,  2,  5,  3]\n[ 1,  2,  2,  4]\n</code></pre></p> <p>Split into 2\u00d72 windows (stride 2):</p> <ul> <li>Window top-left: [[1,3],[4,6]] \u2192 max = 6  </li> <li>Window top-right: [[2,0],[1,2]] \u2192 max = 2  </li> <li>Window bottom-left: [[0,2],[1,2]] \u2192 max = 2  </li> <li>Window bottom-right: [[5,3],[2,4]] \u2192 max = 5  </li> </ul> <p>So the pooled output is: <pre><code>MaxPool(A) =\n[ 6, 2]\n[ 2, 5]\n</code></pre></p> <p>This matches Keras\u2019 definition: downsample by taking the maximum over a window and shifting the window by strides. </p>"},{"location":"06-dl-architectures/convolutions-pooling/#how-to-perform-average-pooling-over-an-image-feature-map","title":"How to perform average pooling over an image (feature map)","text":"<p>Average pooling downsamples by taking the average value in each window for each channel. </p> <p>It\u2019s the same sliding-window process as max pooling, but with mean instead of max. </p> <p></p>"},{"location":"06-dl-architectures/convolutions-pooling/#keras-snippets-industry-best-practices","title":"Keras snippets + industry best practices","text":""},{"location":"06-dl-architectures/convolutions-pooling/#convolution-in-keras-conv2d","title":"Convolution in Keras (Conv2D)","text":"<p>Keras <code>Conv2D</code> takes arguments like <code>filters</code>, <code>kernel_size</code>, <code>strides</code>, and <code>padding</code> (<code>\"valid\"</code> or <code>\"same\"</code>). [page:1] Keras notes that <code>\"same\"</code> padding preserves spatial size when <code>strides=1</code> (but not necessarily when stride &gt; 1). [page:1]</p> <pre><code>from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.Conv2D(\n        filters=32,\n        kernel_size=(3, 3),\n        strides=(1, 1),\n        padding=\"same\",     # \"valid\" or \"same\"\n        activation=\"relu\",\n        input_shape=(128, 128, 3)  # channels_last\n    ),\n    layers.Conv2D(64, (3, 3), padding=\"valid\", activation=\"relu\")\n])\n</code></pre>"},{"location":"06-dl-architectures/convolutions-pooling/#max-pooling-in-keras","title":"Max pooling in Keras","text":"<p>Keras <code>MaxPooling2D</code> downsamples by taking the maximum in each window per channel, shifted by <code>strides</code>, with <code>padding=\"valid\"</code> or <code>\"same\"</code>. [page:0] Keras also provides output-shape formulas for pooling under <code>\"valid\"</code> and <code>\"same\"</code> padding. [page:0]</p> <pre><code>from tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")\n])\n</code></pre>"},{"location":"06-dl-architectures/convolutions-pooling/#average-pooling-in-keras","title":"Average pooling in Keras","text":"<p>Keras <code>AveragePooling2D</code> downsamples by taking the average value over a window per channel, shifted by <code>strides</code>. </p> <pre><code>from tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"valid\")\n])\n</code></pre>"},{"location":"06-dl-architectures/convolutions-pooling/#industry-orientations-practical","title":"Industry orientations (practical)","text":"<ul> <li>Track shapes at every stage (write them down): Keras clearly defines Conv2D input/output tensor shapes for <code>channels_last</code> vs <code>channels_first</code>, and pooling preserves channel count while changing spatial dimensions.  </li> <li>Use padding intentionally: <code>\"valid\"</code> shrinks spatial dimensions, while <code>\"same\"</code> is often used to preserve spatial resolution (especially early in a network) when stride is 1. </li> <li>Downsample deliberately: pooling reduces spatial size, but stride in <code>Conv2D</code> is also a downsampling mechanism, so choose one based on your architecture goals and compute budget. </li> </ul>"},{"location":"06-dl-architectures/convolutions-pooling/#quick-glossary","title":"Quick glossary","text":"<ul> <li>Convolution (Conv2D): layer that creates a convolution kernel and convolves it over height/width to produce feature maps.  </li> <li>Kernel/filter: the learnable weights of a conv layer; each filter produces one output channel (feature map). </li> <li>Stride: step size for sliding the conv/pooling window.   </li> <li>Padding: border handling; <code>\"valid\"</code> means no padding, <code>\"same\"</code> pads evenly and (with stride 1) preserves spatial size in conv.  </li> <li>Max pooling: downsampling by taking the max in each window per channel. </li> <li>Average pooling: downsampling by taking the average in each window per channel.</li> </ul> <p>REMEMBER: Convolutions spot small patterns, and pooling keeps what matters while making things simpler.\u201d</p> <p>Convolution uses a small filter across an image to produce useful maps, and pooling downsamples by summarizing small regions</p> <p></p>"},{"location":"06-dl-architectures/data-augmentation/","title":"Data Augmentation","text":"<p>Data augmentation means creating new training examples from your existing ones by applying small, realistic changes (for images: flips, rotations, crops, brightness changes).  </p> <p>You are not changing the task; you are teaching the model that \u201cthis is still the same class even if it looks a bit different.\u201d</p> <p></p>"},{"location":"06-dl-architectures/data-augmentation/#when-to-use-it-and-when-not-to","title":"When to use it (and when not to)","text":"<p>Use data augmentation when your model learns the training set well but does worse on new images (a common sign of overfitting).  </p> <p>It is also useful when your dataset is small, or when real-world photos vary a lot in lighting, position, zoom, and background.</p> <p>Avoid (or limit) augmentation when a transform can change the label. Example: flipping a \u201cleft arrow\u201d to the right, or rotating digits like 6/9, can silently create wrong labels.</p>"},{"location":"06-dl-architectures/data-augmentation/#benefits-what-you-gain","title":"Benefits (what you gain)","text":"<ul> <li>Better generalization: the model sees more variety during training, so it is less surprised at test time.</li> <li>Less dependence on \u201caccidents\u201d in the dataset (exact camera angle, exact brightness, exact framing).</li> <li>A cheaper way to get more useful training signal than collecting and labeling lots of new images.</li> </ul> <p>Rule of thumb: Augment the kind of variation you expect in real life, and keep labels correct.</p>"},{"location":"06-dl-architectures/data-augmentation/#common-augmentation-methods","title":"Common augmentation methods","text":"<p>You can think of augmentation as \u201csafe changes\u201d that keep the label the same but make the training images more varied. A good strategy is to pick 2\u20134 augmentations that match real-world variation for your dataset, then tune how strong they are.</p> <p></p>"},{"location":"06-dl-architectures/data-augmentation/#1-geometry-position-and-shape","title":"1) Geometry (position and shape)","text":"<p>Use these when objects can appear in different places, sizes, or orientations.</p> <ul> <li>Flips (horizontal/vertical): useful when left vs right does not change the class.</li> <li>Small rotations: helps when the camera is slightly tilted.</li> <li>Translations (shifts): teaches the model that the object can be a bit off-center.</li> <li>Zoom / random crop: teaches robustness to framing (object closer/farther, partially cropped).</li> </ul>"},{"location":"06-dl-architectures/data-augmentation/#2-color-and-lighting","title":"2) Color and lighting","text":"<p>Use these when the same object might be photographed under different light conditions.</p> <ul> <li>Brightness: darker / brighter photos (indoors vs outdoors). </li> <li>Contrast: washed-out vs sharp images. </li> <li>Saturation / hue (RGB images): different color temperature, camera settings, or filters. </li> </ul> <p>Tip: For tasks where color is the clue (e.g., ripeness by color), keep these changes small.</p>"},{"location":"06-dl-architectures/data-augmentation/#3-noise-and-compression-camera-social-media-effects","title":"3) Noise and compression (camera + social media effects)","text":"<p>Use these when images come from phones, messaging apps, or the web.</p> <ul> <li>JPEG quality changes: simulates compression artifacts. </li> <li>Small noise / blur (if available in your pipeline): simulates sensor noise or motion blur.</li> </ul>"},{"location":"06-dl-architectures/data-augmentation/#4-cut-and-mix-methods-advanced-very-task-dependent","title":"4) \u201cCut and mix\u201d methods (advanced, very task-dependent)","text":"<p>These are strong regularizers, but you should test carefully because they can change what the model focuses on.</p> <ul> <li>Cutout / random erasing: hide small patches so the model can\u2019t rely on one tiny detail.</li> <li>Mixup: blend two images and also blend their labels.</li> <li>CutMix: cut a patch from one image and paste it into another, then mix labels by area.</li> </ul> <p>Rule of thumb: Start with geometry + a little lighting, get a baseline, then add stronger methods only if you need them.</p>"},{"location":"06-dl-architectures/data-augmentation/#practical-data-augmentation-in-tensorflow-two-clean-ways","title":"Practical: Data augmentation in TensorFlow (two clean ways)","text":"<p>TensorFlow provides image augmentation layers like <code>RandomFlip</code>, <code>RandomRotation</code>, <code>RandomZoom</code>, and <code>RandomContrast</code>, and notes these layers apply random transforms and are only active during training.   </p> <p>TensorFlow also explains two common placements: inside the model (often best on GPU for images) or in the <code>tf.data</code> pipeline (CPU, asynchronous).</p>"},{"location":"06-dl-architectures/data-augmentation/#option-a-augmentation-inside-the-model-simple-portable","title":"Option A: Augmentation inside the model (simple, portable)","text":"<p>This is the easiest setup: you put augmentation layers as part of the model, right after the input (and usually after <code>Rescaling</code>).  </p> <p>TensorFlow points out two big reasons to do this: you can export a truly end-to-end model (more portable), and it helps reduce training/serving skew because the same preprocessing logic is packaged with the model. </p>"},{"location":"06-dl-architectures/data-augmentation/#when-to-choose-this-option-use-cases","title":"When to choose this option (use cases)","text":"<p>Choose \u201cinside the model\u201d when:</p> <ul> <li>You want a single SavedModel that \u201cjust works\u201d on raw images (no separate preprocessing script to remember). </li> <li>You train on GPU and want image preprocessing/augmentation to benefit from running on device (TensorFlow notes this is often the best option for image preprocessing and augmentation layers when training on a GPU). </li> <li>You are teaching/learning and want fewer moving parts (no custom <code>tf.data.map</code> function at first).</li> </ul> <p>Avoid this option (or reconsider) when:</p> <ul> <li>You are training on a TPU: TensorFlow recommends placing preprocessing layers in the <code>tf.data</code> pipeline on TPU (with a couple of exceptions like <code>Normalization</code>/<code>Rescaling</code>). </li> <li>Your augmentation needs very custom logic that is easier to write with <code>tf.image</code> functions.</li> </ul>"},{"location":"06-dl-architectures/data-augmentation/#key-behavior-learners-should-remember","title":"Key behavior learners should remember","text":"<p>Image augmentation layers are random and TensorFlow notes they are only active during training (similar to how <code>Dropout</code> behaves).  So: training sees varied images; validation/test sees the original images (no random changes).</p>"},{"location":"06-dl-architectures/data-augmentation/#minimal-keras-example-good-default","title":"Minimal Keras example (good default)","text":"<pre><code>import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\ndata_augmentation = keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.1),\n    layers.RandomContrast(0.1),\n])  # Active only during training.[1]\n\nmodel = keras.Sequential([\n    layers.Input(shape=(224, 224, 3)),\n    layers.Rescaling(1.0 / 255),  \n    data_augmentation,\n\n    # ... your CNN backbone ...\n    layers.Conv2D(32, 3, activation=\"relu\"),\n    layers.MaxPooling2D(),\n\n    layers.Flatten(),\n    layers.Dense(10)\n])\n</code></pre>"},{"location":"06-dl-architectures/data-augmentation/#option-b-augmentation-in-a-tfdata-pipeline-fast-input-pipelines","title":"Option B: Augmentation in a <code>tf.data</code> pipeline (fast input pipelines)","text":"<p>This option applies augmentation before the data reaches the model: you transform the dataset with <code>Dataset.map(...)</code> and keep training fast with <code>prefetch(...)</code>. </p> <p>TensorFlow shows this pattern so that input work (decode, resize, augment) can run efficiently in parallel with model training. </p>"},{"location":"06-dl-architectures/data-augmentation/#when-to-choose-this-option-use-cases_1","title":"When to choose this option (use cases)","text":"<p>Choose augmentation in <code>tf.data</code> when:</p> <ul> <li>You want a high-throughput input pipeline (CPU prepares batches while the GPU/accelerator trains). </li> <li>You train on TPU: TensorFlow recommends placing preprocessing layers in the <code>tf.data</code> pipeline on TPU (with a couple of exceptions like <code>Normalization</code>/<code>Rescaling</code>). </li> <li>Your training data comes from files (JPEG/PNG), and your pipeline already does decode/resize/shuffle/batch\u2014augmentation fits naturally there. </li> </ul> <p>Avoid (or simplify) this option when:</p> <ul> <li>You want the easiest \u201csingle exported model\u201d story; putting augmentation inside the model can be more portable. </li> <li>You\u2019re new and your pipeline is already complicated; start inside the model, then move to <code>tf.data</code> once everything works.</li> </ul>"},{"location":"06-dl-architectures/data-augmentation/#key-idea-learners-should-remember","title":"Key idea learners should remember","text":"<p>Augmentation should be applied to the training dataset only; validation/test should measure performance on clean (non-augmented) inputs.  </p> <p>With preprocessing layers, you can control training behavior explicitly (e.g., calling them with <code>training=True</code> inside your <code>map</code> function). </p>"},{"location":"06-dl-architectures/data-augmentation/#recommended-pipeline-pattern","title":"Recommended pipeline pattern","text":"<p>A clean, common order is: <code>shuffle -&gt; map(augment) -&gt; batch -&gt; prefetch</code></p> <pre><code>import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nAUTOTUNE = tf.data.AUTOTUNE\n\ndata_augmentation = keras.Sequential([\n    layers.RandomFlip(\"horizontal\"),\n    layers.RandomRotation(0.1),\n    layers.RandomZoom(0.1),\n])  # These layers apply random transforms meant for training. \n\ndef augment(x, y):\n    x = tf.cast(x, tf.float32)\n    x = data_augmentation(x, training=True)  # force augmentation on training batches\n    return x, y\n\n# Training pipeline\ntrain_ds = train_ds.shuffle(1000)\ntrain_ds = train_ds.map(augment, num_parallel_calls=AUTOTUNE)\ntrain_ds = train_ds.batch(64)\ntrain_ds = train_ds.prefetch(AUTOTUNE)  # common performance pattern. \n\n# Validation pipeline (no augmentation)\nval_ds = val_ds.batch(64).prefetch(AUTOTUNE)\n</code></pre>"},{"location":"06-dl-architectures/data-augmentation/#practical-basic-ops-with-tfimage-manual-but-very-flexible","title":"Practical: Basic ops with <code>tf.image</code> (manual but very flexible)","text":"<p>TensorFlow\u2019s <code>tf.image</code> module includes functions like <code>random_flip_left_right</code>, <code>random_brightness</code>, <code>random_contrast</code>, <code>random_crop</code>, and <code>random_jpeg_quality</code>. </p> <p>It also provides stateless random versions (e.g., <code>stateless_random_flip_left_right</code>, <code>stateless_random_brightness</code>) when you want deterministic results given a seed. </p> <pre><code>import tensorflow as tf\n\ndef augment_with_tf_image(image, label):\n    image = tf.image.convert_image_dtype(image, tf.float32)  # keep values in a stable dtype.\n\n    image = tf.image.random_flip_left_right(image)  # horizontal flip. \n    image = tf.image.random_brightness(image, max_delta=0.1)  # lighting change.\n    image = tf.image.random_contrast(image, lower=0.9, upper=1.1)  # contrast change. \n\n    # Example crop: output size must match your model input.\n    image = tf.image.random_crop(image, size=(224, 224, 3))  # random crop. \n\n    return image, label\n</code></pre> <p>Tips that help you avoid mistakes:</p> <ul> <li>Always confirm your final image shape matches the model input shape (especially after crops).</li> <li>Keep augmentations \u201csmall\u201d at first; if training becomes unstable, reduce rotation/zoom/brightness ranges.</li> <li>Apply augmentation only to the training set (not validation/test), otherwise you measure the wrong thing.</li> </ul> <p>REMEMBER: Augmentation is not about making images look \u201ccool\u201d, it\u2019s about teaching the model the right kind of variation.</p>"},{"location":"06-dl-architectures/deep-cnns/","title":"\ud83d\udea7 Under Construction \ud83d\udea7","text":"<p>This page is being built faster than a neural net on GPU! Check back soon - good things take time (and debugging). \ud83d\ude04</p>"},{"location":"06-dl-architectures/object-detection/","title":"\ud83d\udea7 Under Construction \ud83d\udea7","text":"<p>This page is being built faster than a neural net on GPU! Check back soon - good things take time (and debugging). \ud83d\ude04</p>"},{"location":"06-dl-architectures/transfer-learning/","title":"\ud83d\udea7 Under Construction \ud83d\udea7","text":"<p>This page is being built faster than a neural net on GPU! Check back soon - good things take time (and debugging). \ud83d\ude04</p>"},{"location":"0X-portfolio/","title":"Portfolio Projects and ML Challenges","text":"<p>This section brings together a collection of Machine Learning practice challenges and mini projects designed to help you grow, while having fun.</p> <p>Each challenge focuses on building intuition, applying conceptual thinking, and hands-on project development. </p>"},{"location":"0X-portfolio/#challenges","title":"Challenges","text":""},{"location":"0X-portfolio/#math-foundations-challenge","title":"Math Foundations Challenge","text":"<p>A friendly, team-based review competition covering the core math concepts behind ML, such as Linear Algebra, Calculus, and Probability. It focuses on reasoning, intuition, and how these ideas connect to real ML workflows.</p> <p>More challenges and mini-projects will be added here over time, including topics such as: - Data exploration and visualization - Model understanding and interpretation - Applied ML problem-solving - Short research-inspired exercises  </p> <p>Stay tuned and keep exploring!</p>"},{"location":"0X-portfolio/math-challenge/","title":"ML Math Challenge - Competition Overview","text":""},{"location":"0X-portfolio/math-challenge/#objective","title":"Objective","text":"<p>This friendly competition is designed to help you review and connect key math concepts that form the foundation of Machine Learning (<code>Linear Algebra</code>, <code>Calculus</code>, and <code>Probability</code>.)</p> <p>You\u2019ll work in teams to solve short, intuitive challenges. No coding (maybe only vibe coding) or long calculations; just reasoning, teamwork, and quick thinking.</p>"},{"location":"0X-portfolio/math-challenge/#teams","title":"Teams","text":"<ul> <li>You\u2019ll work in teams. Discuss ideas together before answering.</li> <li>One team member will share the final answer for each question or task.</li> <li>Each team member must present at least one task</li> </ul>"},{"location":"0X-portfolio/math-challenge/#competition-flow","title":"Competition Flow","text":"<p>There are four rounds, each focusing on a different skill area. Each round includes a few short questions or mini tasks. Points are earned for correct answers, reasoning, and teamwork.</p>"},{"location":"0X-portfolio/math-challenge/#round-1-true-or-false-showdown","title":"Round 1 \u2013 True or False Showdown","text":"<ul> <li>This round will include quick statements from all topics.</li> <li>Points increase for each correct answer (1 \u2192 2 \u2192 3 \u2192 4 \u2192 5), but one wrong answer resets your score to <code>0</code></li> <li>Speed and accuracy matter.</li> </ul> <p>Review ideas:</p> <ul> <li>Basic matrix and vector properties</li> <li>Derivatives and slopes</li> <li>Probability and independence</li> <li>General math logic and intuition</li> </ul>"},{"location":"0X-portfolio/math-challenge/#round-2-linear-algebra-concepts","title":"Round 2 \u2013 Linear Algebra Concepts","text":"<p>Focus on how data can be represented and transformed using matrices and vectors. You'll reason about relationships between features, combinations, and transformations.</p> <p>Review ideas:</p> <ul> <li>Matrices, vectors, and dimensions</li> <li>Matrix \u00d7 vector multiplication</li> <li>Linear combinations and feature meaning</li> <li>Projections and geometric interpretation</li> </ul>"},{"location":"0X-portfolio/math-challenge/#round-3-calculus-and-optimization","title":"Round 3 \u2013 Calculus and Optimization","text":"<p>Explore how change, slope, and direction affect learning. You\u2019ll think about how models improve by adjusting parameters.</p> <p>Review ideas:</p> <ul> <li>Derivative as rate of change</li> <li>Gradient and direction of improvement</li> <li>Learning rate intuition</li> <li>Local minima and convergence</li> </ul>"},{"location":"0X-portfolio/math-challenge/#round-4-probability-and-statistics","title":"Round 4 \u2013 Probability and Statistics","text":"<p>Work with small data samples to understand patterns and uncertainty. Expect reasoning around averages, variation, and data interpretation.</p> <p>Review ideas:</p> <ul> <li>Mean, median, mode</li> <li>Variance and standard deviation</li> <li>Distributions and outliers</li> <li>Interpreting data intuitively</li> </ul>"},{"location":"0X-portfolio/math-challenge/#scoring","title":"Scoring","text":"<ul> <li>Points for correct answers and strong reasoning.</li> <li>Some rounds may include bonus points for clear explanations or teamwork.</li> <li>The team with the highest total score wins!</li> </ul>"},{"location":"0X-portfolio/math-challenge/#tips-for-success","title":"Tips for Success","text":"<ul> <li>Think out loud and explain your reasoning.</li> <li>Focus on intuition, not memorization.</li> <li>Work together efficiently, share ideas quickly.</li> <li>Stay relaxed and enjoy the process, learning through play is the goal!</li> </ul>"}]}