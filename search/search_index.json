{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Holberton ML Handbook (Albania)","text":"<p>Learn modern machine learning by doing. Clear steps, small wins, and projects that build real skills over 9 months.</p> <p></p>"},{"location":"#start-here","title":"Start here","text":"<ul> <li>Begin with Tools, then move to Math, Data, Core ML,  Deep Learning, and Generative AI.</li> <li>Each page has a goal, steps, code, and links to go further.</li> <li>Use the search bar to jump straight to what\u2019s needed.</li> </ul>"},{"location":"#who-its-for","title":"Who it\u2019s for","text":"<ul> <li>Holberton School Albania trainees.</li> <li>Motivated learners comfortable with basic Python, terminal, and Git.</li> <li>Anyone who wants a focused ML guide without fluff.</li> </ul>"},{"location":"#whats-inside","title":"What's inside","text":"<ul> <li>Tools: Set up Git/GitHub, Jupyter Notebooks, VS Code, and sharpen Python fundamentals</li> <li>Math: Master linear algebra, NumPy, calculus, and probability/statistics for ML</li> <li>Data: Collect, clean, visualize, and query data with pandas, SQL, and MongoDB</li> <li>Core ML: Understand the ML lifecycle and implement supervised and unsupervised learning algorithms</li> </ul>"},{"location":"#suggested-path","title":"Suggested path","text":"<ul> <li>Foundations first: Tools \u2192 Math \u2192 Data.</li> <li>Core ML \u2192 Deep Learning \u2192 Capstone and deployment.</li> <li>Practice steadily; reflect and iterate.</li> </ul>"},{"location":"#study-tips","title":"Study tips","text":"<ul> <li>Keep short notes after each lesson.</li> <li>Recreate examples, then tweak them to test understanding.</li> <li>Explain what was learned to a peer\u2014teaching locks it in.</li> </ul>"},{"location":"#word-of-encouragement","title":"Word of encouragement","text":"<p>Keep going. Small steps, real projects, steady progress\u2014skill by skill, week by week.</p> <p>When it gets hard, return to the basics, build something small, and move forward again.</p> <p></p>"},{"location":"01-tools/","title":"Tools Overview","text":"<p>Three essential tools to get your machine learning projects off the ground. Each tutorial takes 25-45 minutes and gets you productive immediately.</p> <p></p>"},{"location":"01-tools/#zero-day-set-up","title":"Zero Day Set Up","text":"<p>Get ready for your Machine Learning journey</p> <p>You'll configure your learning platform access, communication channels, version control, and development environment to ensure you're ready for day one of training.</p>"},{"location":"01-tools/#git-and-github","title":"Git and GitHub","text":"<p>Save your work and collaborate with others</p> <p>Never lose code again. Track every change, work safely with teammates, and showcase your projects online. Includes step-by-step authentication setup and the commands you'll actually use daily.</p> <p>You'll learn: How to save and sync your code, collaborate without conflicts, and recover from mistakes</p>"},{"location":"01-tools/#jupyter-notebook-setup","title":"Jupyter Notebook Setup","text":"<p>Write code, see results instantly</p> <p>Perfect for experimenting with data and trying out ideas. Comes with all the math and data tools you need - no complex setup required. Just install and start exploring your data</p> <p>You'll learn: Interactive coding, instant visualizations, and sharing your discoveries</p>"},{"location":"01-tools/#vs-code-setup","title":"VS Code Setup","text":"<p>Your complete coding workspace</p> <p>Get the same professional editor used by top tech companies. Works on Windows with Linux power under the hood - giving you the best of both worlds for serious development.</p> <p>You'll learn: Professional editing, debugging, and building real applications</p> <p>Start here: These three tools work together well. Git keeps your work safe, Jupyter lets you experiment quickly, and VS Code handles everything else you need to build amazing ML projects.</p>"},{"location":"01-tools/git-and-github/","title":"Git and GitHub Setup and Essentials","text":"<p>This tutorial shows you how to set up and use Git and GitHub for effective version control and collaboration in machine learning projects. You'll learn essential commands, authentication setup, and workflows that form the foundation of professional ML development. </p> <p>Estimated time: 45 minutes</p>"},{"location":"01-tools/git-and-github/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: Managing code changes, collaborating with teammates, and maintaining project history becomes impossible without proper version control as ML projects grow in complexity.</p> <p>Practical benefits: Git and GitHub enable you to track experiments, collaborate safely with teammates, maintain reproducible ML workflows, and recover from mistakes without losing work. These skills are essential for any ML role where you'll work on shared codebases and need to manage model iterations.</p> <p>Professional context: Every ML team uses Git for code management, GitHub for collaboration, and version control for tracking model experiments and dataset changes. Mastering these tools early accelerates your integration into professional ML workflows.</p> <p></p>"},{"location":"01-tools/git-and-github/#prerequisites-learning-objectives","title":"Prerequisites &amp; Learning Objectives","text":"<p>Required knowledge: - Basic command line navigation - Understanding of files and folders - GitHub account (create at github.com)</p> <p>Required tools: - Git installed on your system - Command line access (Terminal/Command Prompt) - Text editor or IDE</p> <p>Learning outcomes: - Configure Git with your identity and GitHub authentication - Create, stage, commit, and push changes to repositories - Clone existing projects and collaborate through pull/push workflows - Troubleshoot common Git authentication and workflow issues</p> <p>High-level approach: You'll configure Git locally, set up secure GitHub authentication, practice core workflow commands, and connect local repositories to GitHub for collaboration.</p>"},{"location":"01-tools/git-and-github/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"01-tools/git-and-github/#step-1-configure-your-git-identity","title":"Step 1: Configure Your Git Identity","text":"<p>Git needs to know who you are to properly track changes and contributions.</p> <pre><code>git config --global user.name \"Your Full Name\"\ngit config --global user.email \"your-email@example.com\"\n</code></pre> <p>Why this matters: These details appear in your commit history and help teammates identify who made changes. Use the same email associated with your GitHub account for consistency.</p> <p>Expected output: No output means success. Verify with: <pre><code>git config --global --list\n</code></pre></p>"},{"location":"01-tools/git-and-github/#step-2-set-up-github-authentication","title":"Step 2: Set Up GitHub Authentication","text":"<p>GitHub requires secure authentication through Personal Access Tokens (PATs) instead of passwords.</p> <p>Create your PAT: - Log in to GitHub - Navigate to Settings \u2192 Developer settings \u2192 Personal access tokens \u2192 Tokens (classic) - Click Generate new token (classic) - Select scopes: <code>repo</code>, <code>workflow</code>, <code>write:packages</code> - Set expiration (no expiration, or 90 days) - Copy the token immediately - See short video tutorial</p> <p>Store your token securely: Save it in a password manager or secure note - you won't see it again.</p> <p>Expected outcome: You have a PAT that starts with <code>ghp_</code> and is 40+ characters long.</p>"},{"location":"01-tools/git-and-github/#step-3-create-your-repository-on-github-first","title":"Step 3: Create Your Repository on GitHub First","text":"<p>The most streamlined approach is to create your repository directly on GitHub, then clone it to your local machine. This eliminates potential conflicts and simplifies the authentication process.</p> <p>Create a new repository on GitHub: - Navigate to github.com and click the green \"New\" button - Name your repository (e.g., <code>my-ml-project</code>) - Keep it public for learning purposes - Important: Leave \"Initialize this repository with a README\" unchecked - Click \"Create repository\"</p> <p>Why this approach works better: Creating an empty repository first gives you a clean foundation and GitHub generates the exact clone URL you need, eliminating configuration errors.</p>"},{"location":"01-tools/git-and-github/#step-4-clone-your-repository-locally","title":"Step 4: Clone Your Repository Locally","text":"<p>Since you already have your Personal Access Token, you can clone the repository using the embedded authentication method:</p> <pre><code>git clone https://username:token@github.com/username/repository-name.git\n</code></pre> <p>**Make sure to replace <code>username</code>, <code>token</code> and <code>repository-name</code> with your actual data. </p> <p>Example: <pre><code>git clone https://your-username:ghp_your_token_here@github.com/your-username/my-ml-project.git\ncd my-ml-project\n</code></pre></p> <p>Why embed the token: This method stores your credentials temporarily and eliminates repeated authentication prompts during your session.</p> <p>Expected output: <pre><code>Cloning into 'my-ml-project'...\nremote: Enumerating objects: 3, done.\nremote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\nReceiving objects: 100% (3/3), done.\n</code></pre></p>"},{"location":"01-tools/git-and-github/#step-5-master-the-essential-git-workflow","title":"Step 5: Master the Essential Git Workflow","text":"<p>Now that your repository is connected, follow the fundamental pattern: create/modify \u2192 stage \u2192 commit \u2192 push.</p> <p>Create your first file: <pre><code>echo \"# My ML Project\" &gt; README.md # this creates a file with heading \"My ML Project\"\necho \"This repository contains my machine learning experiments.\" &gt;&gt; README.md\n</code></pre></p> <p>Check what's changed: <pre><code>git status\n</code></pre></p> <p>Stage your changes: <pre><code>git add README.md\n</code></pre></p> <p>Commit with a descriptive message: <pre><code>git commit -m \"Add project README with initial description\"\n</code></pre></p> <p>Push to GitHub: <pre><code>git push origin main\n</code></pre></p> <p>Why this workflow matters: This four-step process (modify \u2192 add \u2192 commit \u2192 push) forms the essential of version control. Each commit creates a checkpoint you can return to, and pushing synchronizes your work with GitHub.</p> <p>Expected output after pushing: <pre><code>Enumerating objects: 3, done.\nCounting objects: 100% (3/3), done.\nWriting objects: 100% (3/3), 245 bytes | 245.00 KiB/s, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo https://github.com/username/my-ml-project.git\n * [new branch]      main -&gt; main\n</code></pre></p> <p>Pro tip: Always use present-tense commit messages (\"Add feature\" not \"Added feature\") to maintain consistency with Git's own messaging style.</p>"},{"location":"01-tools/git-and-github/#quick-reference-for-daily-use","title":"Quick reference for daily use","text":"Command Purpose When to Use <code>git status</code> Check what changed Before staging files <code>git add .</code> Stage all changes When ready to commit everything <code>git commit -m \"message\"</code> Save changes locally After staging files <code>git push</code> Upload to GitHub After committing changes <code>git pull</code> Download latest changes Before starting new work"},{"location":"01-tools/git-and-github/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've configured Git with your identity, set up secure GitHub authentication, mastered the core workflow (stage \u2192 commit \u2192 push), and can now collaborate on repositories professionally.</p> <p>Best practices for ML development: - Commit frequently with descriptive messages to track experiment iterations - Pull before pushing to avoid conflicts when collaborating - Use <code>.gitignore</code> to exclude large model files and sensitive data - Branch for experiments to keep main branch stable during model development</p> <p>External resources for deeper learning:</p> <ul> <li>Pro Git Book - comprehensive Git reference</li> <li>GitHub Flow - branching strategies for teams  </li> <li>Git Branching Interactive Tutorial - visual practice environment</li> </ul>"},{"location":"01-tools/notebook-setup/","title":"Jupyter Notebook Setup with Anaconda","text":"<p>This tutorial shows you how to install and launch Jupyter Notebooks using Anaconda, the industry-standard platform for data science that comes pre-configured with essential ML libraries. You'll learn to navigate the Jupyter interface and master core notebook workflows for machine learning development.</p> <p>Estimated time: 25 minutes</p>"},{"location":"01-tools/notebook-setup/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: Machine learning development requires experimenting with code interactively, visualizing results immediately, and documenting analysis workflows - tasks that become overwhelming without proper tooling.</p> <p>Practical benefits: Jupyter Notebooks provide an interactive environment where you can write code, visualize results immediately, document your thought process, and share reproducible analyses with teammates. Anaconda eliminates setup complexity by pre-installing essential ML libraries including NumPy, Pandas, Matplotlib, and Scikit-learn.</p> <p>Professional context: Jupyter Notebooks are the standard for ML experimentation, data exploration, and collaborative research across all major tech companies and research institutions. Mastering this workflow is essential for any ML role involving data analysis, model prototyping, or research.</p> <p></p>"},{"location":"01-tools/notebook-setup/#prerequisites-learning-objectives","title":"Prerequisites &amp; Learning Objectives","text":"<p>Required knowledge: - Basic familiarity with Python syntax - Understanding of command line navigation (helpful but not required) - Concept of file systems and directories</p> <p>Learning outcomes: - Install Anaconda with pre-configured ML libraries - Launch Jupyter Notebook through multiple methods - Create, execute, and manage notebook cells and kernels - Apply best practices for notebook organization and reproducibility - Navigate the Jupyter interface efficiently for ML workflows</p> <p>High-level approach: You'll install Anaconda (which includes Jupyter and ML packages), learn multiple ways to launch Jupyter, and practice core notebook workflows including cell execution, markdown documentation, and kernel management.</p>"},{"location":"01-tools/notebook-setup/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"01-tools/notebook-setup/#step-1-download-and-install-anaconda","title":"Step 1: Download and Install Anaconda","text":"<p>Download the installer: - Visit the official Anaconda Distribution page - Select your operating system (Windows, macOS, or Linux) - Download the latest Python 3.x graphical installer (64-bit recommended)</p> <p>Run the installation: <pre><code># For Windows: Double-click the .exe file\n# For macOS: Double-click the .pkg file\n# For Linux: bash Anaconda3-2025.09-Linux-x86_64.sh\n</code></pre></p> <p>Installation settings: - Accept the license agreement - Choose \"Just Me\" installation type (recommended) - Use the default installation directory - Important: Do NOT add Anaconda to PATH when prompted (prevents conflicts) - Allow Anaconda to become your default Python</p> <p>Why these settings matter: Keeping Anaconda separate from system Python prevents version conflicts while giving you access to 250+ pre-installed packages including all essential ML libraries.</p> <p>Expected outcome: Anaconda Navigator appears in your applications menu, and essential packages like NumPy, Pandas, Matplotlib, and Scikit-learn are ready to use immediately.</p>"},{"location":"01-tools/notebook-setup/#step-2-launch-jupyter-notebook","title":"Step 2: Launch Jupyter Notebook","text":"<p>Method 1 - Anaconda Navigator (Recommended for beginners): - Open Anaconda Navigator from your applications menu - Click Launch under Jupyter Notebook in the main interface</p> <p>Method 2 - Windows Command Line: <pre><code># Open Command Prompt or PowerShell\njupyter notebook\n\n# Alternative if above doesn't work:\npython -m notebook\n</code></pre></p> <p>Method 3 - Anaconda Prompt (Windows): <pre><code># Search for \"Anaconda Prompt\" in start menu\njupyter notebook\n</code></pre> Expected behavior: Your default web browser opens showing the Jupyter dashboard at <code>http://localhost:8888</code>, displaying your file system.</p> <p>Understanding the interface: The dashboard shows your files and folders, allowing you to navigate, create notebooks, and manage running sessions.</p>"},{"location":"01-tools/notebook-setup/#step-3-create-and-navigate-your-first-notebook","title":"Step 3: Create and Navigate Your First Notebook","text":"<p>Create your first notebook: - In the Jupyter dashboard, click New \u2192 Python 3 - The notebook opens in a new tab with an empty code cell</p> <p>Test pre-installed ML libraries: <pre><code># Run this cell with Shift + Enter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_iris\n\nprint(\"All essential ML libraries loaded successfully!\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"Pandas version: {pd.__version__}\")\n\n# Quick data visualization test\ndata = load_iris()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\ndf.head()\n</code></pre></p> <p>Essential cell operations and shortcuts: - Execute cells: <code>Shift + Enter</code> (runs cell and moves to next) - Insert cells: <code>A</code> (above current) or <code>B</code> (below current) - Change cell type: <code>M</code> (Markdown) or <code>Y</code> (Code) - Delete cells: <code>D + D</code> (press D twice) - Save notebook: <code>Ctrl + S</code> (Windows/Linux) or <code>Cmd + S</code> (Mac)</p>"},{"location":"01-tools/notebook-setup/#step-4-master-essential-notebook-workflows","title":"Step 4: Master Essential Notebook Workflows","text":"<p>Notebook structure for ML projects: <pre><code># Project Title: [Your ML Experiment Name]\n\n## 1. Environment Setup and Data Loading\n## 2. Exploratory Data Analysis  \n## 3. Data Preprocessing and Feature Engineering\n## 4. Model Training and Hyperparameter Tuning\n## 5. Model Evaluation and Validation\n## 6. Results Analysis and Conclusions\n</code></pre></p> <p>Kernel management essentials: <pre><code># Access through menu: Kernel \u2192 Restart\n# Keyboard shortcut: 0 + 0 (press 0 twice)\nKernel \u2192 Restart &amp; Clear Output    # Fresh start\nKernel \u2192 Restart &amp; Run All         # Reproduce full analysis\n</code></pre></p> <p>Why kernel management matters: Restarting kernels ensures reproducible execution order, clears memory usage, and helps debug variable conflicts - essential for reliable ML experiments.[10][11]</p> <p>Markdown documentation best practices: <pre><code>## Data Analysis Summary\n\n**Findings:**\n- Dataset contains 150 samples with 4 features\n- No missing values detected\n- Clear separation between species classes\n\n**Next Steps:**\n- Apply feature scaling for SVM models\n- Test multiple classification algorithms\n- Perform cross-validation analysis\n</code></pre></p>"},{"location":"01-tools/notebook-setup/#step-5-save-export-and-manage-notebooks","title":"Step 5: Save, Export, and Manage Notebooks","text":"<p>Save and backup options: <pre><code># Auto-save is enabled, but manual save is recommended\nCtrl + S  # Save current notebook\n\n# Export to different formats\nFile \u2192 Download as \u2192 HTML          # For sharing results\nFile \u2192 Download as \u2192 Python (.py)  # Convert to script\nFile \u2192 Download as \u2192 PDF via LaTeX # Professional reports\n</code></pre></p> <p>Organize your ML workspace: <pre><code># Recommended folder structure:\nml-projects/\n\u251c\u2500\u2500 data/           # Raw datasets\n\u251c\u2500\u2500 notebooks/      # Jupyter notebooks\n\u251c\u2500\u2500 scripts/        # Python modules\n\u251c\u2500\u2500 models/         # Saved models\n\u2514\u2500\u2500 results/        # Output files and plots\n</code></pre></p> <p>Stop Jupyter safely: - Save all notebooks first (<code>Ctrl + S</code>) - Close browser tabs - Return to terminal/command prompt where Jupyter is running - Press <code>Ctrl + C</code> twice to shut down the server cleanly[1]</p> <p>Best practices for professional ML development: - Document your methodology with markdown cells explaining approach and findings - Clear outputs before version control to keep repositories clean - Use descriptive notebook names like <code>01-data-exploration.ipynb</code>, <code>02-model-training.ipynb</code> - Restart and run all cells periodically to ensure reproducible results</p>"},{"location":"01-tools/notebook-setup/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've installed Anaconda with pre-configured ML libraries, mastered multiple methods to launch Jupyter Notebook, learned essential cell operations and shortcuts, and established professional workflow practices for reproducible ML development.</p> <p>Best practices for ML development: - Use markdown extensively to document methodology, findings, and next steps - Restart kernels regularly to ensure reproducible execution and catch hidden dependencies - Organize notebooks systematically with clear naming conventions and logical project structure - Test essential imports at the beginning of each session to verify environment integrity</p> <p>External resources for deeper learning: - Jupyter Notebook Official Documentation - comprehensive reference guide - Anaconda Package List - explore 250+ included packages - Jupyter Notebook Best Practices - Google's professional workflow guide</p>"},{"location":"01-tools/python-warm-up/","title":"Practice with Python: Functions, Loops, and Data Structures","text":""},{"location":"01-tools/python-warm-up/#why-this-matters","title":"Why This Matters","text":"<p>Real-world machine learning (and software work) almost always involves reading, processing, and summarizing structured data. Becoming comfortable manipulating lists and dictionaries in Python will enable you to prepare datasets, analyze results, and build robust ML code.</p> <p>For this hands-on practice, you'll step through a mini-project working with a common data format (JSON), using real-world-style user data. You'll build small functions and use loops to transform, filter, and analyze the data, exactly the skillset needed for both ML work and technical interviews.</p> <p></p> <p>You'll use a set of sample user data provided by JSONPlaceholder, which mimics the structure of typical web APIs. Each user includes fields like name, email, address, company, etc.</p> <p>You have two options for how to get this data:</p> <ul> <li>Option 1: Copy the result from https://jsonplaceholder.typicode.com/users directly into your Python file as a list, assigning it to a variable named, say <code>data</code>.</li> <li>Option 2 (If curious): Use the <code>requests</code> module to fetch the data at runtime.</li> </ul> <p>Example setup (with <code>requests</code>): <pre><code>import requests\n\ndata = requests.get(\"https://jsonplaceholder.typicode.com/users\").json()\n\n# If you get an error, run this in the terminal first:\n# pip install requests\n</code></pre></p>"},{"location":"01-tools/python-warm-up/#step-by-step-practice-tasks","title":"Step-by-Step Practice Tasks","text":"<p>Let\u2019s start simple and build up to more complex processing. Create a Python script called <code>user_processing.py</code>. Each task can be a separate function in that file.</p>"},{"location":"01-tools/python-warm-up/#task-1-print-all-user-names","title":"Task 1: Print All User Names","text":"<p>Write a function that loops through the <code>data</code> and prints the <code>name</code> of each user.</p>"},{"location":"01-tools/python-warm-up/#task-2-collect-emails-with-a-specific-domain","title":"Task 2: Collect Emails with a Specific Domain","text":"<p>Write a function that returns a list of all users' emails ending with <code>.biz</code>.</p>"},{"location":"01-tools/python-warm-up/#task-3-find-users-in-a-given-city","title":"Task 3: Find Users in a Given City","text":"<p>Write a function that takes a city name as an argument and prints the names and emails of all users living in that city (<code>data[i][\"address\"][\"city\"]</code>).</p>"},{"location":"01-tools/python-warm-up/#task-4-count-companies","title":"Task 4: Count Companies","text":"<p>Write a function that builds and prints a dictionary: keys are company names (from <code>data[i][\"company\"][\"name\"]</code>), values are counts of how many users work at each company.</p>"},{"location":"01-tools/python-warm-up/#task-5-list-unique-zipcodes","title":"Task 5: List Unique Zipcodes","text":"<p>Write a function that extracts all unique zip codes from the address section of each user, and prints the sorted list of zip codes.</p>"},{"location":"01-tools/python-warm-up/#task-6-summarize-users-by-username-initial","title":"Task 6: Summarize Users by Username Initial","text":"<p>Write a function that creates a dictionary mapping first letters of usernames (from <code>data[i][\"username\"]</code>) to a list of names of users starting with that letter (e.g., <code>'B': ['Bret', 'Brandon']</code>).</p>"},{"location":"01-tools/python-warm-up/#task-7-nicely-print-a-users-full-address","title":"Task 7: Nicely Print a User's Full Address","text":"<p>Write a function that takes a username and prints their full address in this format: <code>\"Leanne Graham: Apt. 556, Kulas Light, Gwenborough, 92998-3874\"</code></p>"},{"location":"01-tools/python-warm-up/#submission-guidelines","title":"Submission Guidelines","text":"<ul> <li>Organize your code: Place each task in its own function, and call the functions from a main block (<code>if __name__ == \"__main__\":</code>).</li> <li>Comment your code: Briefly explain what each function does.</li> <li>Test each function: Call them with sample values to demonstrate the output.</li> </ul>"},{"location":"01-tools/python-warm-up/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Completing these tasks will help you: - Practice for-loops, list/dictionary processing, and function writing - Work with real-world nested data - Get ready for ML work, interviews, and project tasks</p> <p>Next, you'll expand this skillset to analyze larger datasets and write code for typical ML preprocessing steps.</p>"},{"location":"01-tools/vscode-setup/","title":"Visual Studio Code Setup with WSL for Machine Learning","text":"<p>This tutorial shows you how to install and configure Visual Studio Code with Windows Subsystem for Linux (WSL) to create a professional ML development environment. You'll learn to combine Windows usability with Linux power for seamless machine learning workflows.</p> <p>Estimated time: 35 minutes</p>"},{"location":"01-tools/vscode-setup/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: Machine learning development often requires Linux tools and libraries, but many developers use Windows machines. Traditional solutions like dual-boot or virtual machines create workflow friction and performance overhead.</p> <p>Practical benefits: VS Code with WSL provides the best of both worlds - Windows productivity tools with native Linux performance for ML development. You get access to Linux package managers, Python environments, and ML frameworks while maintaining familiar Windows workflows and file system integration.</p> <p>Professional context: Most ML production environments run on Linux, making WSL essential for developing code that matches deployment targets. VS Code with remote development is the industry standard for teams working across different operating systems and cloud environments.</p> <p></p>"},{"location":"01-tools/vscode-setup/#prerequisites-learning-objectives","title":"Prerequisites &amp; Learning Objectives","text":"<p>Required knowledge: - Basic Windows navigation and file management - Understanding of command line concepts - Familiarity with text editors or IDEs</p> <p>Learning outcomes: - Install and configure VS Code with essential ML extensions - Set up WSL with Ubuntu for Linux development environment - Connect VS Code to WSL for seamless remote development - Navigate between Windows and Linux file systems efficiently - Configure Python environments and run ML code in integrated terminals</p> <p>High-level approach: You'll install VS Code first, then enable WSL with Ubuntu, install the Remote-WSL extension, and establish an integrated development workflow that combines Windows interface with Linux backend.</p>"},{"location":"01-tools/vscode-setup/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"01-tools/vscode-setup/#step-1-install-visual-studio-code","title":"Step 1: Install Visual Studio Code","text":"<p>Download and install VS Code: - Visit https://code.visualstudio.com/ - Download the Windows installer (64-bit recommended) - Run the installer with these important settings:   - Add \"Open with Code\" action to Windows Explorer file context menu   - Add \"Open with Code\" action to Windows Explorer directory context menu   - Add to PATH (enables command line access)</p> <p>Why these settings matter: Adding to PATH allows you to open VS Code from any terminal, while context menu integration provides quick access to editing files and folders directly from Windows Explorer.</p> <p>Expected outcome: VS Code launches successfully and you can access it from Start menu, desktop shortcut, or by typing <code>code</code> in any command prompt.</p>"},{"location":"01-tools/vscode-setup/#step-2-enable-and-install-wsl-with-ubuntu","title":"Step 2: Enable and Install WSL with Ubuntu","text":"<p>Install WSL using PowerShell: <pre><code># Open PowerShell as Administrator\n# Right-click Start button \u2192 \"Terminal (Admin)\" or \"PowerShell (Admin)\"\nwsl --install\n</code></pre></p> <p>What this command accomplishes: - Enables WSL and Virtual Machine Platform features - Downloads and installs the Linux kernel - Sets WSL 2 as default (better performance) - Installs Ubuntu Linux distribution automatically[2]</p> <p>Complete the installation: - Restart your computer when prompted - After restart, Ubuntu setup will launch automatically - Create a Linux username and password (can be different from Windows) - Important: Remember this password - you'll need it for <code>sudo</code> commands</p> <p>Verify WSL installation: <pre><code># In any command prompt or PowerShell\nwsl --list --verbose\n</code></pre></p> <p>Expected output: <pre><code>  NAME      STATE           VERSION\n* Ubuntu    Running         2\n</code></pre></p>"},{"location":"01-tools/vscode-setup/#step-3-install-wsl-extension-and-connect-vs-code","title":"Step 3: Install WSL Extension and Connect VS Code","text":"<p>Install the Remote-WSL extension: - Open VS Code - Press <code>Ctrl + Shift + X</code> to open Extensions - Search for \"Remote - WSL\" by Microsoft - Click Install</p> <p>Connect to WSL: - Press <code>Ctrl + Shift + P</code> to open Command Palette - Type and select \"Remote-WSL: New WSL Window\" - Choose Ubuntu from the distribution list - A new VS Code window opens connected to your Linux environment</p> <p>Verify the connection: - Look for \"WSL: Ubuntu\" in the bottom-left corner of VS Code - Open integrated terminal with <code>Ctrl +</code> (backtick) - The terminal should show your Linux username and prompt</p> <p>Expected terminal prompt: <pre><code>username@DESKTOP-NAME:~$\n</code></pre></p>"},{"location":"01-tools/vscode-setup/#step-4-configure-your-linux-development-environment","title":"Step 4: Configure Your Linux Development Environment","text":"<p>Update Ubuntu packages: <pre><code># Run in the WSL-connected VS Code terminal\nsudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre></p> <p>Install essential development tools: <pre><code># Python and development essentials (if not already installed)\nsudo apt install python3 python3-pip python3-venv git curl wget -y\n\n# Verify installations\npython3 --version\npip3 --version\ngit --version\n</code></pre></p> <p>Install Python ML libraries: <pre><code># Create a virtual environment for ML projects\npython3 -m venv ~/ml-env\nsource ~/ml-env/bin/activate\n\n# Install essential ML packages\npip install numpy pandas matplotlib seaborn scikit-learn jupyter notebook\n</code></pre></p> <p>Why virtual environments are essential: Virtual environments isolate project dependencies, preventing conflicts between different ML projects and ensuring reproducible development across team members.</p> <p>Expected outcome: You can run Python ML code natively in Linux while editing comfortably in VS Code.</p>"},{"location":"01-tools/vscode-setup/#step-5-master-the-integrated-workflow","title":"Step 5: Master the Integrated Workflow","text":"<p>File system navigation: <pre><code># Access Windows files from WSL\ncd /mnt/c/Users/YourUsername/Documents\n\n# Create ML project in Linux home (recommended for performance)\ncd ~\nmkdir ml-projects\ncd ml-projects\n</code></pre></p> <p>Open projects efficiently: <pre><code># From WSL terminal, open current directory in VS Code\ncode .\n\n# Create and edit files directly\ncode my_ml_script.py\n</code></pre></p> <p>Essential VS Code shortcuts for remote development: - <code>Ctrl + Shift + P</code>: Command Palette (most important!) - <code>Ctrl +</code>: Toggle integrated terminal - <code>Ctrl + Shift + E</code>: File Explorer - <code>Ctrl + B</code>: Toggle sidebar - <code>F1</code>: Alternative Command Palette access</p> <p>Python development workflow: <pre><code># Activate your ML environment\nsource ~/ml-env/bin/activate\n\n# Run Python scripts\npython my_ml_script.py\n</code></pre></p>"},{"location":"01-tools/vscode-setup/#step-6-install-essential-vs-code-extensions-for-ml","title":"Step 6: Install Essential VS Code Extensions for ML","text":"<p>Install Python development extensions: - Press <code>Ctrl + Shift + X</code> in your WSL-connected window - Install these essential extensions:   - Python (Microsoft) - Python language support   - Jupyter (Microsoft) - Notebook support in VS Code</p> <p>File system best practices: - Store Linux-specific projects in <code>~/</code> (Linux home) for best performance - Access Windows files when needed via <code>/mnt/c/Users/YourUsername/</code> - Use WSL for running code, Windows for file management when convenient</p>"},{"location":"01-tools/vscode-setup/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've installed VS Code with WSL integration, created a Linux development environment with Python ML libraries, mastered file system navigation between Windows and Linux, and established a professional workflow for ML development.</p> <p>Best practices for ML development: - Store ML projects in Linux home (<code>~/ml-projects/</code>) for optimal performance and native tool access - Use virtual environments for each project to maintain clean dependency management - Leverage integrated terminal to avoid switching between VS Code and separate command windows - Keep sensitive data in WSL to benefit from Linux security and permissions model</p> <p>For non-Windows users: - macOS/Linux users: Install VS Code directly and use local Python development - no WSL needed - All platforms: Consider Remote-SSH extension for connecting to remote servers or cloud instances</p> <p>External resources for deeper learning: - VS Code WSL Tutorial - official comprehensive guide[4] - WSL Best Practices - Microsoft's development environment guide[5] - Remote Development Extension Pack - additional remote development capabilities[1]</p> <p>Practice exercises: - Create a sample ML project using scikit-learn in your WSL environment - Practice navigating between Windows and Linux file systems - Set up a Git repository and commit code from VS Code with WSL</p>"},{"location":"01-tools/zero-day/","title":"Zero Day: Essential Setup for Machine Learning Training","text":"<p>This tutorial guides you through setting up all accounts and tools required for your Machine Learning training at Holberton. You'll configure your learning platform access, communication channels, version control, and development environment to ensure you're ready for day one of training.</p> <p>Estimated time: 20 minutes</p>"},{"location":"01-tools/zero-day/#why-this-matters","title":"Why This Matters","text":"<p>Starting ML training without proper account setup and tool configuration leads to lost time troubleshooting access issues, missed communications, and inability to submit assignments when you should be focusing on learning.</p> <p></p>"},{"location":"01-tools/zero-day/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"01-tools/zero-day/#step-1-access-and-configure-your-holberton-intranet","title":"Step 1: Access and Configure Your Holberton Intranet","text":"<p>The intranet serves as your central hub for all course materials, project specifications, and progress tracking.</p> <p>Access the platform: - Navigate to https://intranet.hbtn.io/. Then log in with the credentials provided to you by email. You may bookmark this page for easy daily access. </p> <p>Why this matters: The intranet contains all project requirements, learning resources, and submission deadlines. It's your single source of truth throughout the program.</p> <p>Complete your profile: - Click on your profile icon (bottom left corner) - Critical step: Fill in all mandatory fields marked with asterisks (*) - Add a (professional) profile photo - Save your changes - At this phase, do not yet change the password.</p>"},{"location":"01-tools/zero-day/#step-2-connect-to-slack-for-team-communication","title":"Step 2: Connect to Slack for Team Communication","text":"<p>Slack serves as your real-time communication channel with instructors, mentors, and fellow students throughout the program.</p> <p>Access Slack: Locate the Slack icon or link within the intranet (left side panel on the navigation menu). Click to launch Slack. </p> <p>Important: Use the same credentials as your intranet login. Alternatively, download the Slack desktop app for better notifications. You'll automatically be added to your cohort's group channel. </p>"},{"location":"01-tools/zero-day/#step-3-create-your-github-account","title":"Step 3: Create Your GitHub Account","text":"<p>GitHub hosts your code repositories and integrates with the platform's automated grading system.</p> <p>If you already have a GitHub account you may use it. </p> <p>Alternatively,</p> <p>Create your account: - Navigate to https://github.com/signup. Enter your email address (use a professional email you'll access long-term) and create a strong password. </p> <p>Choose a professional username (avoid numbers or special characters if possible)</p> <p>Why username matters: Your GitHub username becomes part of your professional identity. Choose something you'd be comfortable sharing with future employers, as your ML projects will remain visible in your portfolio.</p> <p>Next action: Remember to add this exact username to your intranet profile as described in Step 1.</p>"},{"location":"01-tools/zero-day/#step-4-access-your-cloud-development-environment","title":"Step 4: Access Your Cloud Development Environment","text":"<p>Containers on Demand (COD) provides pre-configured Linux machines with all necessary ML libraries installed, eliminating local setup complexity.</p> <p>Access the platform: - Navigate to https://cod.hbtn.io/sign_in. Once there, log in with your same intranet credentials and wait for the dashboard to load</p> <p>Why cloud environments matter: COD ensures everyone works in identical environments with consistent library versions, eliminating \"it works on my machine\" problems common in ML development.</p> <p>Configure your container settings:</p> <p>Step 4.1: Select your region - Locate the \"Region\" dropdown at the top of the page  - Important: Select Europe for optimal performance and compliance - This choice affects connection speed and data residency</p> <p>Step 4.2: Choose your container - Scroll through the container list - Find and select ml_ubuntu_2204 - Click <code>Spin Up Container</code> - Wait 30-60 seconds for the container to initialize</p> <p>Step 4.3: Access your development environment - Click \"<code>Actions</code> and select <code>VS Code</code> to launch the web-based VS Code interface - The interface loads with a Linux terminal and file explorer</p> <p>Why this container: <code>ml_ubuntu_2204</code> comes pre-installed with Python, NumPy, pandas, scikit-learn, TensorFlow, PyTorch, and other essential ML libraries on Ubuntu 22.04 LTS.</p> <p>Important info: the container on demand expands after 4 hours. You need to repeat this process any time you work witht the platform. You can add more time as you are working.</p>"},{"location":"01-tools/zero-day/#quick-reference-for-daily-workflow","title":"Quick Reference for Daily Workflow","text":"Platform URL Purpose Credentials Intranet intranet.hbtn.io Course materials, projects, progress Primary account Slack Via intranet link Communication, support Same as intranet GitHub github.com Code hosting, version control Separate account COD cod.hbtn.io Development environment Same as intranet"},{"location":"01-tools/zero-day/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've configured your Holberton intranet profile with GitHub integration, connected to Slack for team communication, created a professional GitHub account, and launched your pre-configured ML development environment with VS Code customization.</p> <p>Next tutorial: Complete the Git and GitHub tutorial to finish setting up version control and learn the essential workflow for submitting projects.</p>"},{"location":"02-math/","title":"Math Overview","text":"<p>Build the math intuition that powers machine learning. Short, focused lessons with clear examples and practice you can use right away.</p> <p></p> <p>The language of data and models</p> <p>Understand vectors, matrices, and transformations\u2014the backbone of features, embeddings, and neural networks. Learn the operations that show up everywhere: dot products, norms, matrix multiplication, and projections.</p> <p>You\u2019ll learn: How data is represented, how models combine inputs, and how to work with matrices confidently.</p>"},{"location":"02-math/#numpy","title":"NumPy","text":"<p>Fast and easy math with arrays</p> <p>Learn to use NumPy, Python\u2019s main library for number crunching. See how to make and change arrays, do math on whole groups of numbers at once, and handle data the way ML tools expect.</p> <p>You\u2019ll learn: How to create arrays, use built-in functions, reshape data, pick out values, and do fast math without loops.</p>"},{"location":"02-math/#calculus","title":"Calculus","text":"<p>Change, slopes, and optimization</p> <p>Grasp derivatives, gradients, and chain rule to see how models learn. Connect loss functions to gradient descent and learn why step size and curvature matter.</p> <p>You\u2019ll learn: How to compute and interpret gradients, tune learning, and reason about optimization.</p>"},{"location":"02-math/#probability-statistics","title":"Probability &amp; Statistics","text":"<p>Uncertainty, inference, and decisions</p> <p>Work with distributions, sampling, estimation, and confidence. Learn how to evaluate models with sound metrics and understand variance, bias, and overfitting.</p> <p>You\u2019ll learn: How to quantify uncertainty, choose the right metrics, and make data\u2011driven decisions.</p> <p>Start here: Pick one topic, do a quick review, then try a small exercise in code. Repeat often as clarity compounds.</p>"},{"location":"02-math/calculus/","title":"Calculus for Machine Learning","text":"<p>This tutorial introduces the essential calculus concepts that power machine learning algorithms. You'll learn summation and product notation, derivatives for optimization, and integrals for probability\u2014all with clear examples connecting math to ML practice.</p> <p>Estimated time: 70 minutes</p>"},{"location":"02-math/calculus/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: Machine learning models learn by adjusting parameters to minimize errors. Without understanding derivatives (how functions change), gradients (direction of steepest change), and optimization (finding best parameters), you cannot grasp how models actually learn or why training sometimes fails.</p> <p>Practical benefits: Calculus provides the mathematical framework for optimization algorithms like gradient descent, the backbone of neural network training. Understanding derivatives helps you debug vanishing gradients, tune learning rates, interpret loss curves, and implement custom training loops. Integrals appear in probability calculations and computing expected values for model evaluation.</p> <p>Professional context: Every time a neural network trains, it's computing thousands of derivatives using the chain rule (backpropagation). Understanding calculus transforms \"black box\" training into something you can reason about, debug, and improve. When a model fails to converge or gradients explode, calculus knowledge helps you identify and fix the problem.</p> <p></p>"},{"location":"02-math/calculus/#prerequisites-learning-objectives","title":"Prerequisites &amp; Learning Objectives","text":"<p>Required knowledge:</p> <ul> <li>Algebra fundamentals (functions, equations, exponents)</li> <li>Basic understanding of graphs and coordinate systems</li> <li>Python basics for computational examples</li> <li>Completed Linear Algebra tutorial (vectors, matrices)</li> </ul> <p>Learning outcomes:</p> <ul> <li>Read and write summation and product notation</li> <li>Understand what derivatives measure and why they matter for ML</li> <li>Apply derivative rules (sum, product, chain) to compute gradients</li> <li>Compute partial derivatives for multivariate functions</li> <li>Understand integrals as area under curves and cumulative sums</li> <li>Connect calculus concepts to gradient descent and backpropagation</li> </ul> <p>Start with notation for sums and products, build intuition for derivatives as rates of change, master derivative rules used in ML, then understand integrals for probability and optimization.</p>"},{"location":"02-math/calculus/#core-concepts","title":"Core Concepts","text":""},{"location":"02-math/calculus/#why-calculus-for-machine-learning","title":"Why Calculus for Machine Learning?","text":"<p>Calculus is the mathematics of change and accumulation. Machine learning is fundamentally about:</p> <ol> <li>Understanding complex systems - how small input changes affect outputs</li> <li>Optimizing algorithms - finding parameter values that minimize loss</li> <li>Computing gradients - determining which direction improves the model</li> <li>Working with probability - integrating probability density functions</li> </ol> <p>Training a model means repeatedly asking \"if I change this parameter slightly, does my loss go up or down?\" That question is answered by derivatives.</p>"},{"location":"02-math/calculus/#calculus-in-the-ml-workflow","title":"Calculus in the ML Workflow","text":"ML Task Calculus Concept Example Model training Derivatives Gradient descent optimization Backpropagation Chain rule Computing gradients layer-by-layer Loss functions Derivatives Finding minimum prediction error Probability Integrals Computing expected values, cumulative distributions Feature analysis Partial derivatives Understanding feature sensitivity"},{"location":"02-math/calculus/#summation-notation","title":"Summation Notation","text":"<p>Summation notation provides a compact way to express the sum of many terms\u2014essential for expressing loss functions and aggregating predictions.</p> <p>Sigma notation (\\(\\Sigma\\)\\):</p> <p>The symbol \\(\\(\\Sigma\\)\\) (capital Greek sigma) means \"sum up all these terms.\"</p> \\[ \\sum_{i=1}^{n} a_i = a_1 + a_2 + a_3 + \\cdots + a_n \\] <p>Breaking it down:</p> <ul> <li>\\(\\(i\\)\\) is the index variable (usually starts at 1)</li> <li>\\(\\(i=1\\)\\) is the starting value</li> <li>\\(\\(n\\)\\) is the ending value (upper limit)</li> <li>\\(\\(a_i\\)\\) is the term being summed (depends on \\(\\(i\\)\\))</li> </ul> <p>Example 1: Sum of first 10 integers</p> \\[ \\sum_{i=1}^{10} i = 1 + 2 + 3 + \\cdots + 10 = 55 \\] <p>Example 2: Sum of squares</p> \\[ \\sum_{i=1}^{3} i^2 = 1^2 + 2^2 + 3^2 = 1 + 4 + 9 = 14 \\] <p>Python implementation:</p> <pre><code>import numpy as np\n\n# Sum of first 10 integers\nresult = sum(range(1, 11))\nprint(f\"Sum of 1 to 10: {result}\")\n\n# Sum of squares from 1 to 3\nsquares = [i**2 for i in range(1, 4)]\nresult = sum(squares)\nprint(f\"Sum of squares: {result}\")\n\n# Using NumPy\narr = np.arange(1, 11)\nprint(f\"NumPy sum: {np.sum(arr)}\")\n</code></pre> <p>Expected output:</p> <pre><code>Sum of 1 to 10: 55\nSum of squares: 14\nNumPy sum: 55\n</code></pre> <p>ML connection: The mean squared error (MSE) loss function is a summation:</p> \\[ \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 \\] <p>This sums up squared errors across all \\(\\(n\\)\\) data points.</p>"},{"location":"02-math/calculus/#product-notation","title":"Product Notation","text":"<p>Product notation uses \\(\\(\\Pi\\)\\) (capital Greek pi) to represent multiplication of multiple terms.</p> <p>Pi notation (\\(\\Pi\\)\\):</p> \\[ \\prod_{i=1}^{n} a_i = a_1 \\times a_2 \\times a_3 \\times \\cdots \\times a_n \\] <p>Example 1: Product of first 5 integers (factorial)</p> \\[ \\prod_{i=1}^{5} i = 1 \\times 2 \\times 3 \\times 4 \\times 5 = 120 \\] <p>Example 2: Nested products</p> \\[ \\prod_{i=1}^{2} \\prod_{j=1}^{3} (i \\cdot j) = (1 \\cdot 1)(1 \\cdot 2)(1 \\cdot 3)(2 \\cdot 1)(2 \\cdot 2)(2 \\cdot 3) = 1 \\cdot 2 \\cdot 3 \\cdot 2 \\cdot 4 \\cdot 6 = 288 \\] <p>Python implementation:</p> <pre><code>import numpy as np\n\n# Product of first 5 integers\nresult = np.prod(range(1, 6))\nprint(f\"Product of 1 to 5: {result}\")\n\n# Nested products\nresult = 1\nfor i in range(1, 3):\n    for j in range(1, 4):\n        result *= (i * j)\nprint(f\"Nested product: {result}\")\n</code></pre> <p>Expected output:</p> <pre><code>Product of 1 to 5: 120\nNested product: 288\n</code></pre> <p>ML connection: Probability calculations often involve products. For example, the likelihood of independent events:</p> \\[ P(\\text{all events}) = \\prod_{i=1}^{n} P(\\text{event}_i) \\]"},{"location":"02-math/calculus/#common-mathematical-series","title":"Common Mathematical Series","text":"<p>Certain series appear frequently in ML and have known formulas.</p> <p>Faulhaber's formulas for power sums:</p> <p></p> <p>Sum of first (\\(m\\)\\) integers:</p> \\[ \\sum_{k=1}^{m} k = \\frac{m(m+1)}{2} \\] <p>Example: \\(\\(\\sum_{k=1}^{100} k = \\frac{100 \\cdot 101}{2} = 5050\\)\\)</p> <p>Sum of first (\\(m\\)\\) squares:</p> \\[ \\sum_{k=1}^{m} k^2 = \\frac{m(m+1)(2m+1)}{6} \\] <p>Example: \\(\\(\\sum_{k=1}^{10} k^2 = \\frac{10 \\cdot 11 \\cdot 21}{6} = 385\\)\\)</p> <p>Sum of first (\\(m\\)\\) cubes:</p> \\[ \\sum_{k=1}^{m} k^3 = \\left[\\frac{m(m+1)}{2}\\right]^2 \\] <p>Example: \\(\\(\\sum_{k=1}^{5} k^3 = \\left[\\frac{5 \\cdot 6}{2}\\right]^2 = 15^2 = 225\\)\\)</p> <p>Python verification:</p> <pre><code>import numpy as np\n\nm = 10\n\n# Sum of integers\nformula_result = m * (m + 1) // 2\nactual_result = sum(range(1, m + 1))\nprint(f\"Sum of integers - Formula: {formula_result}, Actual: {actual_result}\")\n\n# Sum of squares\nformula_result = m * (m + 1) * (2*m + 1) // 6\nactual_result = sum([k**2 for k in range(1, m + 1)])\nprint(f\"Sum of squares - Formula: {formula_result}, Actual: {actual_result}\")\n\n# Sum of cubes\nformula_result = (m * (m + 1) // 2) ** 2\nactual_result = sum([k**3 for k in range(1, m + 1)])\nprint(f\"Sum of cubes - Formula: {formula_result}, Actual: {actual_result}\")\n</code></pre> <p>Expected output:</p> <pre><code>Sum of integers - Formula: 55, Actual: 55\nSum of squares - Formula: 385, Actual: 385\nSum of cubes - Formula: 3025, Actual: 3025\n</code></pre> <p>Why these matter: Understanding series helps you analyze algorithm complexity and compute aggregate statistics efficiently.</p>"},{"location":"02-math/calculus/#what-is-a-derivative","title":"What is a Derivative?","text":"<p>A derivative measures the rate of change of a function. It answers the question: \"If I change the input by a tiny amount, how much does the output change?\"</p> <p>Formal definition:</p> \\[ f'(x) = \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h} \\] <p>This says: \"Take the change in output \\(\\((f(x+h) - f(x))\\)\\), divide by the change in input \\(\\((h)\\)\\), and see what happens as \\(\\(h\\)\\) gets infinitely small.\"</p> <p>Geometric interpretation: The derivative is the slope of the tangent line to the function at a point.</p> <p></p> <p>Notation variations:</p> <ul> <li>\\(\\(f'(x)\\)\\) (prime notation)</li> <li>\\(\\(\\frac{df}{dx}\\)\\) (Leibniz notation)</li> <li>\\(\\(\\frac{dy}{dx}\\)\\) (when \\(\\(y = f(x)\\)\\))</li> </ul> <p>Example: Derivative of (\\(f(x) = x^2\\)\\)</p> <p>Using the definition:</p> \\[ f'(x) = \\lim_{h \\to 0} \\frac{(x+h)^2 - x^2}{h} = \\lim_{h \\to 0} \\frac{x^2 + 2xh + h^2 - x^2}{h} = \\lim_{h \\to 0} \\frac{2xh + h^2}{h} = \\lim_{h \\to 0} (2x + h) = 2x \\] <p>So \\(\\(\\frac{d}{dx}(x^2) = 2x\\)\\).</p> <p>Physical intuition: If \\(\\(f(x)\\)\\) is position, \\(\\(f'(x)\\)\\) is velocity. If \\(\\(f(x)\\)\\) is velocity, \\(\\(f'(x)\\)\\) is acceleration.</p> <p>ML connection: In gradient descent, the derivative tells us which direction to move parameters to reduce loss.</p>"},{"location":"02-math/calculus/#common-derivative-rules","title":"Common Derivative Rules","text":"<p>Instead of using the limit definition every time, we use standard rules.</p> <p>Power rule:</p> \\[ \\frac{d}{dx} x^n = n \\cdot x^{n-1} \\] <p>Examples:</p> <ul> <li> \\[\\frac{d}{dx} x^3 = 3x^2\\] </li> <li> \\[\\frac{d}{dx} x^{10} = 10x^9\\] </li> <li> \\[\\frac{d}{dx} \\sqrt{x} = \\frac{d}{dx} x^{1/2} = \\frac{1}{2}x^{-1/2} = \\frac{1}{2\\sqrt{x}}\\] </li> </ul> <p>Constant rule:</p> \\[ \\frac{d}{dx} c = 0 \\] <p>Constants don't change, so their rate of change is zero.</p> <p>Linear function:</p> \\[ \\frac{d}{dx} (ax + b) = a \\] <p>The slope of a line is constant.</p> <p>Logarithm:</p> \\[ \\frac{d}{dx} \\ln(x) = \\frac{1}{x} \\] <p>Exponential:</p> \\[ \\frac{d}{dx} e^x = e^x \\] <p>The exponential function is its own derivative!</p> <p>Reciprocal:</p> \\[ \\frac{d}{dx} \\frac{1}{x} = \\frac{d}{dx} x^{-1} = -x^{-2} = -\\frac{1}{x^2} \\] <p>Python verification:</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Function f(x) = x^2\ndef f(x):\n    return x**2\n\n# Derivative f'(x) = 2x\ndef f_prime(x):\n    return 2*x\n\n# Numerical derivative approximation\ndef numerical_derivative(f, x, h=1e-5):\n    return (f(x + h) - f(x)) / h\n\n# Test at x = 3\nx = 3\nanalytical = f_prime(x)\nnumerical = numerical_derivative(f, x)\n\nprint(f\"Analytical derivative at x={x}: {analytical}\")\nprint(f\"Numerical derivative at x={x}: {numerical:.6f}\")\n</code></pre> <p>Expected output:</p> <pre><code>Analytical derivative at x=3: 6\nNumerical derivative at x=3: 6.000001\n</code></pre>"},{"location":"02-math/calculus/#sum-rule-for-derivatives","title":"Sum Rule for Derivatives","text":"<p>When functions are added, their derivatives add.</p> <p>Sum rule:</p> \\[ \\frac{d}{dx}\\left[f(x) + g(x)\\right] = f'(x) + g'(x) \\] <p>Example:</p> \\[ h(x) = x^3 + 2x^2 + 5x + 7 \\] \\[ h'(x) = 3x^2 + 4x + 5 + 0 = 3x^2 + 4x + 5 \\] <p>ML connection: Loss functions are often sums of individual errors across data points:</p> \\[ L = \\sum_{i=1}^{n} L_i \\] <p>The derivative is:</p> \\[ \\frac{dL}{d\\theta} = \\sum_{i=1}^{n} \\frac{dL_i}{d\\theta} \\] <p>This is why we can compute gradients for mini-batches and sum them.</p>"},{"location":"02-math/calculus/#product-rule-for-derivatives","title":"Product Rule for Derivatives","text":"<p>When functions are multiplied, the derivative follows a specific pattern.</p> <p>Product rule:</p> \\[ \\frac{d}{dx}\\left[f(x) \\cdot g(x)\\right] = f'(x) \\cdot g(x) + f(x) \\cdot g'(x) \\] <p>Mnemonic: \"First times derivative of second, plus second times derivative of first.\"</p> <p>Example:</p> \\[ h(x) = x^2 \\cdot \\sin(x) \\] <p>Let \\(\\(f(x) = x^2\\)\\) and \\(\\(g(x) = \\sin(x)\\)\\).</p> \\[ h'(x) = 2x \\cdot \\sin(x) + x^2 \\cdot \\cos(x) \\] <p>Why not just (\\(f'(x) \\cdot g'(x)\\)\\)? Because multiplication is not a linear operation. The product rule accounts for how both functions change together.</p> <p>Python verification:</p> <pre><code>import numpy as np\n\n# Function h(x) = x^2 * e^x\ndef h(x):\n    return x**2 * np.exp(x)\n\n# Derivative using product rule: h'(x) = 2x * e^x + x^2 * e^x\ndef h_prime(x):\n    return 2*x * np.exp(x) + x**2 * np.exp(x)\n\n# Numerical verification\ndef numerical_derivative(f, x, h=1e-5):\n    return (f(x + h) - f(x)) / h\n\nx = 2\nanalytical = h_prime(x)\nnumerical = numerical_derivative(h, x)\n\nprint(f\"Analytical: {analytical:.6f}\")\nprint(f\"Numerical: {numerical:.6f}\")\n</code></pre> <p>Expected output:</p> <pre><code>Analytical: 33.556303\nNumerical: 33.556303\n</code></pre> <p>ML connection: Computing derivatives of combined transformations during optimization.</p>"},{"location":"02-math/calculus/#chain-rule-for-derivatives","title":"Chain Rule for Derivatives","text":"<p>The chain rule handles composite functions\u2014functions inside other functions. This is the most important derivative rule for neural networks.</p> <p>Chain rule:</p> \\[ \\frac{d}{dx}f(g(x)) = f'(g(x)) \\cdot g'(x) \\] <p>Or in Leibniz notation, if \\(\\(y = f(u)\\)\\) and \\(\\(u = g(x)\\)\\):</p> \\[ \\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} \\] <p>Intuition: \"How does \\(\\(y\\)\\) change with \\(\\(x\\)\\)? First see how \\(\\(y\\)\\) changes with \\(\\(u\\)\\), then see how \\(\\(u\\)\\) changes with \\(\\(x\\)\\), and multiply.\"</p> <p>Example 1: (\\(h(x) = (x^2 + 1)^3\\)\\)</p> <p>Let \\(\\(u = x^2 + 1\\)\\), so \\(\\(h = u^3\\)\\).</p> \\[ \\frac{dh}{dx} = \\frac{dh}{du} \\cdot \\frac{du}{dx} = 3u^2 \\cdot 2x = 3(x^2 + 1)^2 \\cdot 2x = 6x(x^2 + 1)^2 \\] <p>Example 2: (\\(h(x) = e^{x^2}\\)\\)</p> <p>Let \\(\\(u = x^2\\)\\), so \\(\\(h = e^u\\)\\).</p> \\[ \\frac{dh}{dx} = e^u \\cdot 2x = 2x \\cdot e^{x^2} \\] <p>Python verification:</p> <pre><code>import numpy as np\n\n# Function h(x) = (x^2 + 1)^3\ndef h(x):\n    return (x**2 + 1)**3\n\n# Derivative using chain rule: h'(x) = 6x(x^2 + 1)^2\ndef h_prime(x):\n    return 6*x * (x**2 + 1)**2\n\n# Numerical verification\ndef numerical_derivative(f, x, h=1e-5):\n    return (f(x + h) - f(x)) / h\n\nx = 2\nanalytical = h_prime(x)\nnumerical = numerical_derivative(h, x)\n\nprint(f\"Analytical: {analytical}\")\nprint(f\"Numerical: {numerical:.6f}\")\n</code></pre> <p>Expected output:</p> <pre><code>Analytical: 300\nNumerical: 300.000030\n</code></pre> <p>ML connection: Backpropagation</p> <p>Neural networks are deeply nested composite functions. Backpropagation is just the chain rule applied layer by layer:</p> \\[ \\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial a_n} \\cdot \\frac{\\partial a_n}{\\partial a_{n-1}} \\cdot \\cdots \\cdot \\frac{\\partial a_2}{\\partial w_1} \\] <p>Each \\(\\(\\frac{\\partial a_i}{\\partial a_{i-1}}\\)\\) is computed using the chain rule, propagating gradients backward through the network.</p>"},{"location":"02-math/calculus/#partial-derivatives","title":"Partial Derivatives","text":"<p>When functions have multiple variables, we use partial derivatives to see how the function changes with respect to one variable while keeping others constant.</p> <p>Notation:</p> \\[ \\frac{\\partial f}{\\partial x} \\quad \\text{(partial derivative of } f \\text{ with respect to } x\\text{)} \\] <p>How to compute: Treat all other variables as constants and differentiate normally.</p> <p>Example: (\\(f(x, y) = x^2y + 3xy^2\\)\\)</p> <p>Partial derivative with respect to \\(\\(x\\)\\) (treat \\(\\(y\\)\\) as constant):</p> \\[ \\frac{\\partial f}{\\partial x} = 2xy + 3y^2 \\] <p>Partial derivative with respect to \\(\\(y\\)\\) (treat \\(\\(x\\)\\) as constant):</p> \\[ \\frac{\\partial f}{\\partial y} = x^2 + 6xy \\] <p>Gradient vector:</p> <p>The gradient combines all partial derivatives into a vector:</p> \\[ \\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 2xy + 3y^2 \\\\ x^2 + 6xy \\end{bmatrix} \\] <p>Python implementation:</p> <pre><code>import numpy as np\n\n# Function f(x, y) = x^2 * y + 3*x*y^2\ndef f(x, y):\n    return x**2 * y + 3*x*y**2\n\n# Partial derivatives\ndef df_dx(x, y):\n    return 2*x*y + 3*y**2\n\ndef df_dy(x, y):\n    return x**2 + 6*x*y\n\n# Evaluate at (x=2, y=3)\nx, y = 2, 3\nprint(f\"f({x}, {y}) = {f(x, y)}\")\nprint(f\"\u2202f/\u2202x at ({x}, {y}) = {df_dx(x, y)}\")\nprint(f\"\u2202f/\u2202y at ({x}, {y}) = {df_dy(x, y)}\")\n\n# Gradient vector\ngradient = np.array([df_dx(x, y), df_dy(x, y)])\nprint(f\"Gradient \u2207f at ({x}, {y}) = {gradient}\")\n</code></pre> <p>Expected output:</p> <pre><code>f(2, 3) = 66\n\u2202f/\u2202x at (2, 3) = 39\n\u2202f/\u2202y at (2, 3) = 40\nGradient \u2207f at (2, 3) = [39 40]\n</code></pre> <p>ML connection: Gradient descent computes partial derivatives of the loss function with respect to each model parameter:</p> \\[ w_{\\text{new}} = w_{\\text{old}} - \\alpha \\frac{\\partial L}{\\partial w} \\] <p>Each parameter is updated based on its partial derivative.</p>"},{"location":"02-math/calculus/#what-is-an-integral","title":"What is an Integral?","text":"<p>An integral is the reverse of a derivative. While derivatives measure rates of change, integrals measure accumulation or total area under a curve.</p> <p>Two types:</p> <ol> <li>Indefinite integral (antiderivative): Find a function whose derivative gives you the original function</li> <li>Definite integral: Calculate the actual area under the curve between two points</li> </ol> <p>Notation:</p> \\[ \\int f(x) \\, dx \\] <p>The \\(\\(\\int\\)\\) symbol means \"integrate,\" and \\(\\(dx\\)\\) indicates we're integrating with respect to \\(\\(x\\)\\).</p> <p></p> <p>Why integrals matter in ML:</p> <ul> <li>Computing probabilities (area under probability density functions)</li> <li>Calculating expected values</li> <li>Optimization (finding cumulative sums)</li> <li>Analyzing convergence</li> </ul>"},{"location":"02-math/calculus/#indefinite-integrals","title":"Indefinite Integrals","text":"<p>An indefinite integral finds the antiderivative\u2014a function \\(\\(F(x)\\)\\) such that \\(\\(F'(x) = f(x)\\)\\).</p> <p>General form:</p> \\[ \\int f(x) \\, dx = F(x) + C \\] <p>The \\(\\(+ C\\)\\) is the constant of integration (because derivatives of constants are zero, we can't determine \\(\\(C\\)\\) from integration alone).</p> <p>Basic integration rules:</p> <p>Constant:</p> \\[ \\int a \\, dx = ax + C \\] <p>Power rule:</p> \\[ \\int x^n \\, dx = \\frac{x^{n+1}}{n+1} + C \\quad (n \\neq -1) \\] <p>Examples:</p> <ul> <li> \\[\\int x^2 \\, dx = \\frac{x^3}{3} + C\\] </li> <li> \\[\\int x^5 \\, dx = \\frac{x^6}{6} + C\\] </li> <li> \\[\\int 1 \\, dx = x + C\\] </li> </ul> <p>Reciprocal (special case when (\\(n = -1\\)\\)):</p> \\[ \\int \\frac{1}{x} \\, dx = \\ln|x| + C \\] <p>Exponential:</p> \\[ \\int e^x \\, dx = e^x + C \\] <p>Verification with derivatives:</p> <pre><code>import sympy as sp\n\n# Define variable\nx = sp.Symbol('x')\n\n# Function to integrate\nf = x**2\n\n# Compute indefinite integral\nF = sp.integrate(f, x)\nprint(f\"\u222b{f} dx = {F} + C\")\n\n# Verify by taking derivative\nderivative = sp.diff(F, x)\nprint(f\"Derivative of {F}: {derivative}\")\nprint(f\"Matches original? {derivative == f}\")\n</code></pre> <p>Expected output:</p> <pre><code>\u222bx**2 dx = x**3/3 + C\nDerivative of x**3/3: x**2\nMatches original? True\n</code></pre>"},{"location":"02-math/calculus/#definite-integrals","title":"Definite Integrals","text":"<p>A definite integral computes the exact area under a curve between two limits \\(\\(a\\)\\) and \\(\\(b\\)\\).</p> <p>Notation:</p> \\[ \\int_a^b f(x) \\, dx \\] <p>Fundamental Theorem of Calculus:</p> \\[ \\int_a^b f(x) \\, dx = F(b) - F(a) \\] <p>Where \\(\\(F(x)\\)\\) is any antiderivative of \\(\\(f(x)\\)\\).</p> <p>Example: (\\(\\int_0^2 x^2 \\, dx\\)\\)</p> <ol> <li>Find antiderivative: \\(\\(F(x) = \\frac{x^3}{3}\\)\\)</li> <li>Evaluate at limits: \\(\\(F(2) - F(0) = \\frac{2^3}{3} - \\frac{0^3}{3} = \\frac{8}{3} - 0 = \\frac{8}{3}\\)\\)</li> </ol> <p>So \\(\\(\\int_0^2 x^2 \\, dx = \\frac{8}{3} \\approx 2.667\\)\\).</p> <p>Python verification:</p> <pre><code>import numpy as np\nfrom scipy import integrate\n\n# Function to integrate\ndef f(x):\n    return x**2\n\n# Definite integral from 0 to 2\nresult, error = integrate.quad(f, 0, 2)\nprint(f\"\u222b\u2080\u00b2 x\u00b2 dx = {result:.6f}\")\n\n# Analytical solution\nanalytical = (2**3)/3 - (0**3)/3\nprint(f\"Analytical result: {analytical:.6f}\")\n</code></pre> <p>Expected output:</p> <pre><code>\u222b\u2080\u00b2 x\u00b2 dx = 2.666667\nAnalytical result: 2.666667\n</code></pre> <p>ML connection: Probability</p> <p>Probability density functions (PDFs) integrate to 1 over their domain:</p> \\[ \\int_{-\\infty}^{\\infty} p(x) \\, dx = 1 \\] <p>To find the probability that \\(\\(X\\)\\) falls in range \\(\\([a, b]\\)\\):</p> \\[ P(a \\leq X \\leq b) = \\int_a^b p(x) \\, dx \\]"},{"location":"02-math/calculus/#double-integrals","title":"Double Integrals","text":"<p>Double integrals extend integration to functions of two variables, computing volume under a surface.</p> <p>Notation:</p> \\[ \\iint_R f(x, y) \\, dA = \\int_a^b \\int_c^d f(x, y) \\, dy \\, dx \\] <p>How to compute:</p> <ol> <li>Integrate with respect to \\(\\(y\\)\\) first (treat \\(\\(x\\)\\) as constant)</li> <li>Then integrate the result with respect to \\(\\(x\\)\\)</li> </ol> <p>Example: (\\(\\int_0^1 \\int_0^2 xy \\, dy \\, dx\\)\\)</p> <p>Step 1: Inner integral (with respect to \\(\\(y\\)\\)):</p> \\[ \\int_0^2 xy \\, dy = x \\left[\\frac{y^2}{2}\\right]_0^2 = x \\cdot \\frac{4}{2} = 2x \\] <p>Step 2: Outer integral (with respect to \\(\\(x\\)\\)):</p> \\[ \\int_0^1 2x \\, dx = 2 \\left[\\frac{x^2}{2}\\right]_0^1 = 2 \\cdot \\frac{1}{2} = 1 \\] <p>So \\(\\(\\int_0^1 \\int_0^2 xy \\, dy \\, dx = 1\\)\\).</p> <p>Python verification:</p> <pre><code>from scipy import integrate\n\n# Function f(x, y) = x*y\ndef f(y, x):  # Note: order is reversed for dblquad\n    return x * y\n\n# Double integral\nresult, error = integrate.dblquad(f, 0, 1, 0, 2)\nprint(f\"\u222b\u222b xy dy dx = {result:.6f}\")\n</code></pre> <p>Expected output:</p> <pre><code>\u222b\u222b xy dy dx = 1.000000\n</code></pre> <p>ML connection: Expected values of functions over joint probability distributions use double integrals:</p> \\[ E[g(X, Y)] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x, y) p(x, y) \\, dx \\, dy \\]"},{"location":"02-math/calculus/#quick-reference-for-calculus-operations","title":"Quick Reference for Calculus Operations","text":"Concept Notation Formula/Rule ML Application Summation \\(\\(\\sum_{i=1}^n a_i\\)\\) \\(\\(a_1 + a_2 + \\cdots + a_n\\)\\) Loss functions, batch gradients Product \\(\\(\\prod_{i=1}^n a_i\\)\\) \\(\\(a_1 \\times a_2 \\times \\cdots \\times a_n\\)\\) Probability of independent events Derivative \\(\\(f'(x)\\)\\) or \\(\\(\\frac{df}{dx}\\)\\) \\(\\(\\lim_{h \\to 0} \\frac{f(x+h)-f(x)}{h}\\)\\) Gradient descent Power rule \\(\\(\\frac{d}{dx}x^n\\)\\) \\(\\(nx^{n-1}\\)\\) Computing gradients Sum rule \\(\\(\\frac{d}{dx}[f+g]\\)\\) \\(\\(f' + g'\\)\\) Additive loss functions Product rule \\(\\(\\frac{d}{dx}[f \\cdot g]\\)\\) \\(\\(f'g + fg'\\)\\) Combined transformations Chain rule \\(\\(\\frac{d}{dx}f(g(x))\\)\\) \\(\\(f'(g(x)) \\cdot g'(x)\\)\\) Backpropagation Partial derivative \\(\\(\\frac{\\partial f}{\\partial x}\\)\\) Derivative treating other vars constant Multi-parameter optimization Indefinite integral \\(\\(\\int f(x) \\, dx\\)\\) \\(\\(F(x) + C\\)\\) where \\(\\(F' = f\\)\\) Finding antiderivatives Definite integral \\(\\(\\int_a^b f(x) \\, dx\\)\\) \\(\\(F(b) - F(a)\\)\\) Probability calculations"},{"location":"02-math/calculus/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned summation and product notation for expressing series, understood derivatives as rates of change and their computation rules, mastered the chain rule essential for backpropagation, computed partial derivatives for multivariate optimization, and understood integrals for probability and accumulation.</p> <p>Best practices:</p> <ul> <li>Verify derivatives numerically when implementing custom functions</li> <li>Use automatic differentiation (like PyTorch's autograd) in production, but understand the math</li> <li>Check dimensions when computing gradients to catch errors early</li> <li>Visualize functions and their derivatives to build geometric intuition</li> <li>Start with simple examples before tackling complex nested functions</li> </ul> <p>Connections to ML:</p> <ul> <li>Gradient descent: Uses derivatives to find parameter updates: \\(\\(w := w - \\alpha \\frac{\\partial L}{\\partial w}\\)\\)</li> <li>Backpropagation: Applies chain rule layer-by-layer to compute gradients in neural networks</li> <li>Loss functions: Derivatives show which direction reduces error</li> <li>Probability: Integrals compute cumulative distributions and expected values</li> <li>Optimization: Second derivatives (curvature) help adaptive learning rate methods</li> </ul> <p>External resources:</p> <ul> <li>3Blue1Brown: Essence of Calculus - visual and intuitive calculus explanations</li> <li>Khan Academy: Calculus - comprehensive practice problems</li> <li>MIT OpenCourseWare: Single Variable Calculus - rigorous mathematical foundation</li> </ul> <p>Next tutorial: Apply these calculus concepts to understand gradient descent optimization and implement a simple neural network from scratch using derivatives.</p>"},{"location":"02-math/linear-algebra/","title":"Introduction to Linear Algebra for Machine Learning","text":"<p>This tutorial introduces the fundamental concepts of linear algebra that form the mathematical foundation of machine learning. You'll build geometric intuition for vectors and matrices, understand key operations, and implement them in Python to solidify your understanding.</p> <p>Estimated time: 60 minutes</p>"},{"location":"02-math/linear-algebra/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: Machine learning algorithms operate on numerical data represented as vectors and matrices. Without understanding linear algebra fundamentals, it's impossible to grasp how models process data, optimize parameters, or make predictions.</p> <p>Practical benefits: Linear algebra provides the mathematical framework to represent datasets, transform features, and understand model internals. Mastering these concepts enables you to implement ML algorithms from scratch, debug model behavior, optimize performance, and understand research papers describing new architectures.</p> <p>Professional context: Every ML practitioner uses linear algebra daily. Linear regression uses matrix operations to find optimal coefficients, Principal Component Analysis (PCA) relies on eigenvalues for dimensionality reduction, Convolutional Neural Networks perform matrix multiplications and convolutions, and Support Vector Machines use linear algebra to find optimal decision boundaries.</p> <p></p>"},{"location":"02-math/linear-algebra/#core-concepts","title":"Core Concepts","text":""},{"location":"02-math/linear-algebra/#what-is-linear-algebra","title":"What is Linear Algebra?","text":"<p>Linear algebra is the branch of mathematics dealing with vectors, vector spaces, and linear transformations. It provides the language and tools to work with multidimensional data and transformations.</p> <p>Why \"linear\"? Linear algebra studies operations that preserve vector addition and scalar multiplication. These operations can be visualized as transformations that keep grid lines parallel and evenly spaced.</p> <p>Key insight: Every ML model learns a transformation from input space to output space. Linear algebra gives us the tools to understand and manipulate these transformations.</p>"},{"location":"02-math/linear-algebra/#key-terminology","title":"Key Terminology","text":"<p>Before learning about operations, let's establish essential vocabulary:</p> <p>Scalar: A single number (e.g., 5, -3.14, 0.5)</p> <p>Vector: An ordered collection of numbers representing magnitude and direction. Can be thought of as</p> <ul> <li>An arrow in space (geometric view)</li> <li>A list of coordinates (computer science view)</li> <li>A point in <code>n</code>-dimensional space (mathematical view)</li> </ul> <p>Matrix: A rectangular array of numbers arranged in rows and columns. Represents:</p> <ul> <li>A linear transformation (e.g., rotation, scaling, shearing)</li> <li>A collection of vectors (each row or column is a vector)</li> <li>A dataset (<code>rows = samples</code>, <code>columns = features</code>)</li> </ul> <p>Dimension: The number of components in a vector, or the size of the space it lives in</p> <p>Linear transformation: An operation that transforms vectors while preserving lines and the origin</p>"},{"location":"02-math/linear-algebra/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"02-math/linear-algebra/#1-understanding-vectors","title":"1. Understanding Vectors","text":"<p>A vector is the fundamental building block of linear algebra. Think of it as an arrow pointing from the origin to a specific point in space.</p> <p></p> <p>Three perspectives on vectors:</p> <p>Physics perspective: A vector has magnitude (length) and direction. For example, velocity or force.</p> <p>Computer Science perspective: A vector is simply an ordered list of numbers: <code>[1][2][3][4][5]</code>.</p> <p>Mathematics perspective: A vector represents a point in <code>n</code>-dimensional space, where each number is a coordinate along one axis.</p> <p>Why vectors matter in ML: Every data point is a vector. A customer's age, income, and purchase history form a <code>3D</code> vector. An image with <code>1000</code> pixels is a <code>1000</code>-dimensional vector.</p> <p>Create a vector in Python:</p> <pre><code># Create a 5-dimensional vector\nmy_vector = [1, 2, 3, 4, 5]\n\n# Print the vector\nprint(my_vector)\nprint(f\"This vector has {len(my_vector)} dimensions\")\n</code></pre> <p>Expected output: <pre><code>[1, 2, 3, 4, 5]\nThis vector has 5 dimensions\n</code></pre></p>"},{"location":"02-math/linear-algebra/#2-vector-operations","title":"2. Vector Operations","text":"<p>There are several key operations which can be done with vectors.</p> <p>Vectors can be combined and scaled using fundamental operations that have geometric interpretations.</p>"},{"location":"02-math/linear-algebra/#vector-addition","title":"Vector Addition","text":"<p>Adding two vectors \\(\\(\\mathbf{v_1} + \\mathbf{v_2}\\)\\) means adding corresponding components element-wise. Geometrically, place the tail of \\(\\(\\mathbf{v_2}\\)\\) at the head of \\(\\(\\mathbf{v_1}\\)\\).</p> <p>Why it matters: Gradient descent updates parameters by adding the gradient vector (scaled) to the current parameter vector.</p> <p></p> <p>Implement vector addition:</p> <pre><code>def add_vectors(vec1, vec2):\n    \"\"\"\n    Add two vectors element-wise.\n    Returns None if vectors have different lengths.\n    \"\"\"\n    if len(vec1) != len(vec2):\n        print(\"Error: Vectors must have the same dimension\")\n        return None\n\n    result = []\n    for i in range(len(vec1)):\n        result.append(vec1[i] + vec2[i])\n\n    return result\n\n# Example usage\nv1 = [1, 2, 3]\nv2 = [4, 5, 6]\nresult = add_vectors(v1, v2)\nprint(f\"{v1} + {v2} = {result}\")\n</code></pre> <p>Expected output: <pre><code>[1, 2, 3] + [4, 5, 6] = [5, 7, 9]\n</code></pre></p>"},{"location":"02-math/linear-algebra/#vector-subtraction","title":"Vector Subtraction","text":"<p>Subtracting \\(\\(\\mathbf{v_2}\\)\\) from \\(\\(\\mathbf{v_1}\\)\\) gives the vector pointing from \\(\\(\\mathbf{v_2}\\)\\) to \\(\\(\\mathbf{v_1}\\)\\).</p> <pre><code>def subtract_vectors(vec1, vec2):\n    \"\"\"Subtract vec2 from vec1 element-wise.\"\"\"\n    if len(vec1) != len(vec2):\n        print(\"Error: Vectors must have the same dimension\")\n        return None\n\n    result = []\n    for i in range(len(vec1)):\n        result.append(vec1[i] - vec2[i])\n\n    return result\n\n# Example usage\nv1 = [10, 8, 6]\nv2 = [1, 2, 3]\nresult = subtract_vectors(v1, v2)\nprint(f\"{v1} - {v2} = {result}\")\n</code></pre> <p>Expected output: <pre><code>[10, 8, 6] - [1, 2, 3] = [9, 6, 3]\n</code></pre></p>"},{"location":"02-math/linear-algebra/#scalar-multiplication","title":"Scalar Multiplication","text":"<p>Multiplying a vector by a scalar \\(\\(c\\)\\) scales its length by \\(\\(|c|\\)\\) and reverses its direction if \\(\\(c &lt; 0\\)\\).</p> <p></p> <pre><code>def scalar_multiply(scalar, vec):\n    \"\"\"Multiply every element of the vector by a scalar.\"\"\"\n    result = []\n    for element in vec:\n        result.append(scalar * element)\n\n    return result\n\n# Example usage\nv = [1, 2, 3]\nprint(f\"2 * {v} = {scalar_multiply(2, v)}\")\nprint(f\"0.5 * {v} = {scalar_multiply(0.5, v)}\")\nprint(f\"-1 * {v} = {scalar_multiply(-1, v)}\")\n</code></pre> <p>Expected output: <pre><code>2 * [1, 2, 3] = [2, 4, 6]\n0.5 * [1, 2, 3] = [0.5, 1.0, 1.5]\n-1 * [1, 2, 3] = [-1, -2, -3]\n</code></pre></p>"},{"location":"02-math/linear-algebra/#dot-product","title":"Dot Product","text":"<p>The dot product of two vectors \\(\\(\\mathbf{v_1} \\cdot \\mathbf{v_2}\\)\\) is the sum of the products of corresponding components: \\(\\(\\sum_{i} v_{1i} \\times v_{2i}\\)\\).</p> <p>Geometric interpretation: \\(\\(\\mathbf{v_1} \\cdot \\mathbf{v_2} = |\\mathbf{v_1}| \\times |\\mathbf{v_2}| \\times \\cos(\\theta)\\)\\), where \\(\\(\\theta\\)\\) is the angle between the vectors.</p> <p>Why it matters: The dot product measures similarity. In ML, cosine similarity for text embeddings relies on the dot product. Neural networks compute dot products between inputs and weights.</p> <p></p> <pre><code>def dot_product(vec1, vec2):\n    \"\"\"\n    Calculate the dot product of two vectors.\n    Returns the scalar result.\n    \"\"\"\n    if len(vec1) != len(vec2):\n        print(\"Error: Vectors must have the same dimension\")\n        return None\n\n    result = 0\n    for i in range(len(vec1)):\n        result += vec1[i] * vec2[i]\n\n    return result\n\n# Example usage\nv1 = [1, 2, 3]\nv2 = [4, 5, 6]\nresult = dot_product(v1, v2)\nprint(f\"{v1} \u00b7 {v2} = {result}\")\n# Calculation: (1*4) + (2*5) + (3*6) = 4 + 10 + 18 = 32\n</code></pre> <p>Expected output: <pre><code>[1, 2, 3] \u00b7 [4, 5, 6] = 32\n</code></pre></p>"},{"location":"02-math/linear-algebra/#3-understanding-matrices","title":"3. Understanding Matrices","text":"<p>A matrix is a rectangular grid of numbers. Think of it as organizing multiple vectors together, or as representing a linear transformation.</p> <p>Key perspectives:</p> <p>Data representation: Each row is a data sample, each column is a feature. A dataset with <code>100</code> customers and <code>5</code> features is a \\(\\(100 \\times 5\\)\\) matrix.</p> <p>Linear transformation: A matrix transforms input vectors to output vectors. Rotation, scaling, and shearing are all matrix operations.</p> <p>Collection of vectors: Each row (or column) can be viewed as a separate vector.</p> <p>Matrix notation: A matrix with \\(\\(m\\)\\) rows and \\(\\(n\\)\\) columns is called an \\(\\(m \\times n\\)\\) matrix. Element at row \\(\\(i\\)\\), column \\(\\(j\\)\\) is denoted \\(\\(A_{ij}\\)\\).</p> \\[ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{bmatrix} \\] <p>Create a matrix in Python:</p> <pre><code># Create a 3x3 matrix\nmatrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\n# Print the matrix\nfor row in matrix:\n    print(row)\n</code></pre> <p>Expected output: <pre><code>[1, 2, 3]\n[4, 5, 6]\n[7, 8, 9]\n</code></pre></p>"},{"location":"02-math/linear-algebra/#4-matrix-shape-and-dimensions","title":"4. Matrix Shape and Dimensions","text":"<p>Understanding matrix shape is critical for ensuring operations are valid.</p> <p>Get matrix shape:</p> <pre><code>def get_matrix_shape(matrix):\n    \"\"\"\n    Return the shape of a matrix as (rows, columns).\n    \"\"\"\n    num_rows = len(matrix)\n    num_cols = len(matrix[0]) if num_rows &gt; 0 else 0\n    return (num_rows, num_cols)\n\n# Example usage\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nshape = get_matrix_shape(matrix)\nprint(f\"Matrix shape: {shape[0]} rows \u00d7 {shape[1]} columns\")\n</code></pre> <p>Expected output: <pre><code>Matrix shape: 3 rows \u00d7 3 columns\n</code></pre></p>"},{"location":"02-math/linear-algebra/#step-5-matrix-indexing-and-slicing","title":"Step 5: Matrix Indexing and Slicing","text":"<p>Accessing specific elements or submatrices is essential for feature extraction and batch processing.</p> <p>Access individual elements:</p> <pre><code>matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\n# Access element at row 0, column 0 (first element)\nprint(f\"Element [0][0]: {matrix[0][0]}\")\n\n# Access element at row 1, column 2\nprint(f\"Element [1][2]: {matrix[1][2]}\")\n\n# Access element at row 2, column 1\nprint(f\"Element [2][1]: {matrix[2][1]}\")\n</code></pre> <p>Expected output: <pre><code>Element [0][0]: 1\nElement [1][2]: 6\nElement [2][1]: 8\n</code></pre></p> <p>Extract rows and columns:</p> <pre><code>def get_row(matrix, row_index):\n    \"\"\"Extract a specific row from the matrix.\"\"\"\n    return matrix[row_index]\n\ndef get_column(matrix, col_index):\n    \"\"\"Extract a specific column from the matrix.\"\"\"\n    column = []\n    for row in matrix:\n        column.append(row[col_index])\n    return column\n\n# Example usage\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nprint(f\"Row 1: {get_row(matrix, 1)}\")\nprint(f\"Column 2: {get_column(matrix, 2)}\")\n</code></pre> <p>Expected output: <pre><code>Row 1: [4, 5, 6]\nColumn 2: [3, 6, 9]\n</code></pre></p> <p>Extract submatrices:</p> <pre><code>def get_submatrix(matrix, row_start, row_end, col_start, col_end):\n    \"\"\"\n    Extract a submatrix from row_start to row_end (exclusive)\n    and col_start to col_end (exclusive).\n    \"\"\"\n    submatrix = []\n    for i in range(row_start, row_end):\n        row = []\n        for j in range(col_start, col_end):\n            row.append(matrix[i][j])\n        submatrix.append(row)\n    return submatrix\n\n# Example: Extract bottom-right 2x2 submatrix\nmatrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nsub = get_submatrix(matrix, 1, 3, 1, 3)\nprint(\"Bottom-right 2\u00d72 submatrix:\")\nfor row in sub:\n    print(row)\n</code></pre> <p>Expected output: <pre><code>Bottom-right 2\u00d72 submatrix:\n[5, 6]\n[8, 9]\n</code></pre></p>"},{"location":"02-math/linear-algebra/#6-matrix-addition-and-subtraction","title":"6. Matrix Addition and Subtraction","text":"<p>Matrices of the same shape can be added or subtracted element-wise.</p> <p></p> <p>Why it matters: Model ensembling often involves averaging predictions (matrix addition). Residuals in regression are computed via matrix subtraction.</p> <pre><code>def add_matrices(mat1, mat2):\n    \"\"\"\n    Add two matrices element-wise.\n    Returns None if shapes don't match.\n    \"\"\"\n    if len(mat1) != len(mat2) or len(mat1[0]) != len(mat2[0]):\n        print(\"Error: Matrices must have the same shape\")\n        return None\n\n    result = []\n    for i in range(len(mat1)):\n        row = []\n        for j in range(len(mat1[0])):\n            row.append(mat1[i][j] + mat2[i][j])\n        result.append(row)\n\n    return result\n\n# Example usage\nmat1 = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\nmat2 = [[9, 8, 7], [6, 5, 4], [3, 2, 1]]\nresult = add_matrices(mat1, mat2)\n\nprint(\"Matrix 1 + Matrix 2:\")\nfor row in result:\n    print(row)\n</code></pre> <p>Expected output: <pre><code>Matrix 1 + Matrix 2:\n[10, 10, 10]\n[10, 10, 10]\n[10, 10, 10]\n</code></pre></p>"},{"location":"02-math/linear-algebra/#7-matrix-multiplication","title":"7.  Matrix Multiplication","text":"<p>Matrix multiplication is the most important operation in linear algebra for ML. Unlike addition, it's not element-wise.</p> <p>Rules: To multiply matrix \\(\\(A\\)\\) (shape \\(\\(m \\times n\\)\\)) by matrix \\(\\(B\\)\\) (shape \\(\\(n \\times p\\)\\)), the number of columns in \\(\\(A\\)\\) must equal the number of rows in \\(\\(B\\)\\). The result is an \\(\\(m \\times p\\)\\) matrix.</p> <p>Geometric interpretation: Matrix multiplication represents composing linear transformations. If \\(\\(A\\)\\) rotates and \\(\\(B\\)\\) scales, then \\(\\(AB\\)\\) first scales then rotates.</p> <p>Why it matters: Every layer in a neural network performs matrix multiplication: \\(\\(output = weight\\_matrix \\times input + bias\\)\\). Understanding this is essential for deep learning.</p> <p></p> <p>Formula: Element \\(\\(C_{ij}\\)\\) in the result is the dot product of row \\(\\(i\\)\\) from \\(\\(A\\)\\) and column \\(\\(j\\)\\) from \\(\\(B\\)\\).</p> \\[ C_{ij} = \\sum_{k=1}^{n} A_{ik} \\times B_{kj} \\] <pre><code>def matrix_multiply(mat1, mat2):\n    \"\"\"\n    Multiply two matrices using the standard algorithm.\n    Returns None if dimensions are incompatible.\n    \"\"\"\n    m1_rows = len(mat1)\n    m1_cols = len(mat1[0])\n    m2_rows = len(mat2)\n    m2_cols = len(mat2[0])\n\n    # Check compatibility\n    if m1_cols != m2_rows:\n        print(f\"Error: Cannot multiply {m1_rows}\u00d7{m1_cols} by {m2_rows}\u00d7{m2_cols}\")\n        return None\n\n    # Initialize result matrix with zeros\n    result = [[0 for _ in range(m2_cols)] for _ in range(m1_rows)]\n\n    # Compute each element as dot product of row and column\n    for i in range(m1_rows):\n        for j in range(m2_cols):\n            dot_product = 0\n            for k in range(m1_cols):\n                dot_product += mat1[i][k] * mat2[k][j]\n            result[i][j] = dot_product\n\n    return result\n\n# Example usage\nA = [[1, 2, 3], [4, 5, 6]]  # 2\u00d73 matrix\nB = [[7, 8], [9, 10], [11, 12]]  # 3\u00d72 matrix\nresult = matrix_multiply(A, B)  # Result will be 2\u00d72\n\nprint(\"A \u00d7 B =\")\nfor row in result:\n    print(row)\n</code></pre> <p>Expected output: <pre><code>A \u00d7 B =\n[58, 64]\n[139, 154]\n</code></pre></p> <p>How it works:</p> <ul> <li>First row of \\(\\(A\\)\\): <code>[1][2][3]</code>, first column of \\(\\(B\\)\\): <code>[7][9][11]</code> \u2192 \\(\\(1\u00d77 + 2\u00d79 + 3\u00d711 = 58\\)\\)</li> <li>First row of \\(\\(A\\)\\): <code>[1][2][3]</code>, second column of \\(\\(B\\)\\): <code>[8][10][12]</code> \u2192 \\(\\(1\u00d78 + 2\u00d710 + 3\u00d712 = 64\\)\\)</li> </ul>"},{"location":"02-math/linear-algebra/#8-transpose-operation","title":"8. Transpose Operation","text":"<p>The transpose of a matrix flips it over its diagonal, swapping rows and columns.</p> <p>Notation: \\(\\(A^T\\)\\) denotes the transpose of matrix \\(\\(A\\)\\).</p> <p>Why it matters: Transposes are used to compute covariance matrices, in backpropagation for neural networks, and to ensure dimension compatibility for matrix multiplication.</p> <p></p> <pre><code>def transpose_matrix(matrix):\n    \"\"\"\n    Return the transpose of a matrix.\n    Rows become columns and columns become rows.\n    \"\"\"\n    rows = len(matrix)\n    cols = len(matrix[0])\n\n    # Create result matrix with swapped dimensions\n    result = [[0 for _ in range(rows)] for _ in range(cols)]\n\n    for i in range(rows):\n        for j in range(cols):\n            result[j][i] = matrix[i][j]\n\n    return result\n\n# Example usage\nmatrix = [[1, 2, 3], [4, 5, 6]]\nprint(\"Original matrix (2\u00d73):\")\nfor row in matrix:\n    print(row)\n\ntransposed = transpose_matrix(matrix)\nprint(\"\\nTransposed matrix (3\u00d72):\")\nfor row in transposed:\n    print(row)\n</code></pre> <p>Expected output: <pre><code>Original matrix (2\u00d73):\n[1, 2, 3]\n[4, 5, 6]\n\nTransposed matrix (3\u00d72):\n[1, 4]\n[2, 5]\n[3, 6]\n</code></pre></p>"},{"location":"02-math/linear-algebra/#quick-reference-for-linear-algebra-operations","title":"Quick Reference for Linear Algebra Operations","text":"Operation Formula Python Implementation ML Application Vector Addition \\(\\(\\mathbf{v_1} + \\mathbf{v_2}\\)\\) Element-wise sum Gradient updates Scalar Multiplication \\(\\(c \\cdot \\mathbf{v}\\)\\) Multiply each element by \\(\\(c\\)\\) Learning rate scaling Dot Product \\(\\(\\mathbf{v_1} \\cdot \\mathbf{v_2} = \\sum v_{1i}v_{2i}\\)\\) Sum of element products Similarity measures Matrix Addition \\(\\(A + B\\)\\) Element-wise sum Model ensembling Matrix Multiplication \\(\\(AB\\)\\), where \\(\\(C_{ij} = \\sum A_{ik}B_{kj}\\)\\) Nested loops Neural network layers Transpose \\(\\(A^T\\)\\), swap rows/columns Flip across diagonal Backpropagation"},{"location":"02-math/linear-algebra/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've built geometric intuition for vectors and matrices, learned the fundamental operations of linear algebra, implemented these operations in pure Python, and connected each concept to machine learning applications.</p> <p>Best practices:</p> <ul> <li>Visualize operations geometrically before implementing them algorithmically</li> <li>Check dimensions carefully before matrix operations to avoid errors</li> <li>Understand the why behind each operation, not just the mechanics</li> <li>Practice with small examples (2D, 3D) before scaling to high dimensions</li> </ul> <p>Connections to ML:</p> <ul> <li>Linear regression: Solve \\(\\(\\mathbf{w} = (X^TX)^{-1}X^T\\mathbf{y}\\)\\) using matrix operations</li> <li>Neural networks: Each layer computes \\(\\(\\mathbf{h} = \\sigma(W\\mathbf{x} + \\mathbf{b})\\)\\)</li> <li>PCA: Find eigenvectors of covariance matrix \\(\\(C = \\frac{1}{n}X^TX\\)\\)</li> <li>SVMs: Solve optimization problems using matrix formulations</li> </ul>"},{"location":"02-math/linear-algebra/#external-resources","title":"External resources","text":"<ul> <li>3Blue1Brown: Essence of Linear Algebra - visual and intuitive explanations</li> <li>Interactive Linear Algebra (Georgia Tech) - free interactive online textbook with visualizations and exercises</li> </ul> <p>Next tutorial: Learn to implement these operations efficiently using NumPy, Python's numerical computing library optimized for array operations.</p>"},{"location":"02-math/numpy/","title":"NumPy for Machine Learning: Efficient Array Operations","text":"<p>This tutorial teaches you to use NumPy, Python's fundamental library for numerical computing. You'll learn to create, manipulate, and perform operations on arrays efficiently - skills essential for every machine learning task.</p> <p>Estimated time: 50 minutes</p>"},{"location":"02-math/numpy/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: </p> <p>Pure Python lists and loops are too slow for machine learning workloads involving millions of data points. </p> <p>Without NumPy's optimized array operations, training even simple models becomes impractically slow, and implementing ML algorithms from scratch is unnecessarily complex.</p> <p>Practical benefits: NumPy provides extremely fast array operations through optimized C and Fortran libraries underneath. What takes seconds with Python loops completes in milliseconds with NumPy. </p> <p>Every major ML library\u2014scikit-learn, TensorFlow, PyTorch, pandas - is built on top of NumPy, making it the universal language of numerical computing in Python.</p> <p>Professional context: NumPy is non-negotiable for ML work. Data preprocessing uses NumPy arrays, model inputs are NumPy arrays, predictions return as NumPy arrays. Understanding NumPy array operations, broadcasting, and vectorization enables you to write production-quality ML code and understand how libraries work under the hood.</p>"},{"location":"02-math/numpy/#core-concepts","title":"Core Concepts","text":""},{"location":"02-math/numpy/#what-is-numpy","title":"What is NumPy?","text":"<p>NumPy (Numerical Python) is the foundational library for scientific computing in Python. It provides a powerful <code>N</code>-dimensional array object and functions for fast operations on arrays.</p> <p>Why NumPy is fast: NumPy operations are implemented in C and execute at compiled speeds, not interpreted Python speeds. </p> <p>A NumPy operation on <code>1</code> million elements can be <code>100x</code> faster than an equivalent Python loop.</p> <p>Key insight: The secret to NumPy's power is vectorization; applying operations to entire arrays at once instead of looping through elements. This shift in thinking from <code>loop over each element</code> to <code>operate on the whole array</code> is fundamental to efficient ML code.</p>"},{"location":"02-math/numpy/#arrays-vs-lists","title":"Arrays vs Lists","text":"Feature Python List NumPy Array Speed Slow (interpreted) Fast (compiled C) Memory More overhead Compact, efficient Data types Mixed types allowed Homogeneous (one type) Operations Manual loops needed Vectorized operations Use case General programming Numerical computing <p>When to use NumPy: Anytime you're working with numerical data, especially large datasets, mathematical operations, or preparing data for ML models.</p>"},{"location":"02-math/numpy/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"02-math/numpy/#1-installing-and-importing-numpy","title":"1.  Installing and Importing NumPy","text":"<p>NumPy must be installed and imported before use.</p> <p>Install NumPy:</p> <pre><code>pip install numpy\n</code></pre> <p>Import NumPy (standard convention):</p> <pre><code>import numpy as np\n</code></pre> <p>Why <code>np</code>: The alias <code>np</code> is universally used in the Python data science community, making code readable and recognizable across projects.</p> <p>Verify installation:</p> <pre><code>import numpy as np\nprint(np.__version__)\n</code></pre> <p>Expected output:</p> <pre><code>1.24.3 # or a similar version\n</code></pre>"},{"location":"02-math/numpy/#2-creating-numpy-arrays-from-lists","title":"2. Creating NumPy Arrays from Lists","text":"<p>The simplest way to create arrays is by converting Python lists.</p> <p>Create a 1D array (vector):</p> <pre><code># From a simple list\nmy_list = [1, 2, 3, 4, 5]\nvector = np.array(my_list)\n\nprint(vector)\nprint(f\"Type: {type(vector)}\")\nprint(f\"Shape: {vector.shape}\")\n</code></pre> <p>Expected output:</p> <pre><code>[1 2 3 4 5]\nType: lass 'numpy.ndarrayay'&gt;\nShape: (5,)\n</code></pre> <p>Why shape matters: The shape <code>(5,)</code> indicates a 1D array with 5 elements. Understanding shapes is critical for ensuring array operations are compatible.</p> <p>Create a 2D array (matrix):</p> <pre><code># From a nested list\nmy_matrix = [\n    [1, 2, 3],\n    [4, 5, 6],\n    [7, 8, 9]\n]\n\nmatrix = np.array(my_matrix)\nprint(matrix)\nprint(f\"Shape: {matrix.shape}\")\n</code></pre> <p>Expected output:</p> <pre><code>[[1 2 3]\n [4 5 6]\n [7 8 9]]\nShape: (3, 3)\n</code></pre> <p>Interpretation: A shape of <code>(3, 3)</code> means 3 rows and 3 columns. In ML terminology, this could represent 3 data samples with 3 features each.</p>"},{"location":"02-math/numpy/#3-built-in-array-creation-functions","title":"3. Built-in Array Creation Functions","text":"<p>NumPy provides convenient functions for generating arrays without manually typing values.</p>"},{"location":"02-math/numpy/#arange-evenly-spaced-values","title":"arange: Evenly Spaced Values","text":"<p>Similar to Python's <code>range()</code>, but returns a NumPy array.</p> <pre><code># Create array from 0 to 10 (exclusive)\narr = np.arange(0, 11)\nprint(arr)\n\n# With step size\narr_step = np.arange(0, 11, 2)\nprint(arr_step)\n\n# Start from non-zero\narr_tens = np.arange(10, 101, 10)\nprint(arr_tens)\n</code></pre> <p>Expected output:</p> <pre><code>[ 0  1  2  3  4  5  6  7  8  9 10]\n[ 0  2  4  6  8 10]\n[ 10  20  30  40  50  60  70  80  90 100]\n</code></pre> <p>ML use case: Creating indices, generating sequences for time series data, or creating evenly spaced feature bins.</p>"},{"location":"02-math/numpy/#zeros-and-ones-initialize-arrays","title":"zeros and ones: Initialize Arrays","text":"<p>Useful for creating placeholder arrays before filling them with computed values.</p> <pre><code># 1D array of zeros\nzeros_1d = np.zeros(5)\nprint(zeros_1d)\n\n# 2D array of zeros\nzeros_2d = np.zeros((3, 4))\nprint(zeros_2d)\n\n# 2D array of ones\nones_2d = np.ones((2, 3))\nprint(ones_2d)\n</code></pre> <p>Expected output:</p> <pre><code>[0. 0. 0. 0. 0.]\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n[[1. 1. 1.]\n [1. 1. 1.]]\n</code></pre> <p>Why initialize with zeros: Many ML algorithms accumulate results (like gradient sums in backpropagation) starting from zero arrays.</p>"},{"location":"02-math/numpy/#linspace-linearly-spaced-values","title":"linspace: Linearly Spaced Values","text":"<p>Unlike <code>arange</code> which uses step size, <code>linspace</code> specifies how many values you want.</p> <pre><code># 10 evenly spaced values between 0 and 100\nlin = np.linspace(0, 100, 10)\nprint(lin)\n\n# 5 values between 2 and 20\nlin2 = np.linspace(2, 20, 5)\nprint(lin2)\n</code></pre> <p>Expected output:</p> <pre><code>[  0.          11.11111111  22.22222222  33.33333333  44.44444444\n  55.55555556  66.66666667  77.77777778  88.88888889 100.        ]\n[ 2.   6.5 11.  15.5 20. ]\n</code></pre> <p>ML use case: Creating evenly spaced feature values for plotting decision boundaries or generating test inputs.</p>"},{"location":"02-math/numpy/#eye-identity-matrix","title":"eye: Identity Matrix","text":"<p>An identity matrix has ones on the diagonal and zeros elsewhere. Critical for linear algebra operations.</p> <pre><code># 3x3 identity matrix\nidentity = np.eye(3)\nprint(identity)\n\n# 5x5 identity matrix\nidentity_5 = np.eye(5)\nprint(identity_5)\n</code></pre> <p>Expected output:</p> <pre><code>[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n[[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]\n</code></pre> <p>Why identity matrices matter: Used in matrix inversion, solving linear systems, and initializing certain neural network layers.</p>"},{"location":"02-math/numpy/#4-random-number-generation","title":"4. Random Number Generation","text":"<p>Random numbers are essential for initializing model weights, creating synthetic data, and splitting datasets.</p>"},{"location":"02-math/numpy/#rand-uniform-random-values","title":"rand: Uniform Random Values","text":"<p>Generates random numbers uniformly distributed between 0 and 1.</p> <pre><code># 1D array of 5 random values\nrand_1d = np.random.rand(5)\nprint(rand_1d)\n\n# 3x3 matrix of random values\nrand_2d = np.random.rand(3, 3)\nprint(rand_2d)\n</code></pre> <p>Expected output (values will vary):</p> <pre><code>[0.52134728 0.76543218 0.23498765 0.98123456 0.12345678]\n[[0.41234567 0.87654321 0.23456789]\n [0.65432109 0.34567890 0.78901234]\n [0.12345678 0.56789012 0.90123456]]\n</code></pre> <p>ML use case: Initializing neural network weights with small random values to break symmetry.</p>"},{"location":"02-math/numpy/#randn-standard-normal-distribution","title":"randn: Standard Normal Distribution","text":"<p>Generates random numbers from a Gaussian (normal) distribution with mean 0 and standard deviation 1.</p> <pre><code># 2x3 matrix from standard normal distribution\nrandn_2d = np.random.randn(2, 3)\nprint(randn_2d)\n</code></pre> <p>Expected output (values will vary):</p> <pre><code>[[ 0.51234567 -1.23456789  0.87654321]\n [-0.34567890  1.45678901 -0.67890123]]\n</code></pre> <p>Why normal distribution: Many ML algorithms assume data follows a normal distribution. Random normal values are commonly used for weight initialization in deep learning.</p>"},{"location":"02-math/numpy/#randint-random-integers","title":"randint: Random Integers","text":"<p>Generates random integers within a specified range.</p> <pre><code># 10 random integers between 1 and 100 (exclusive)\nrand_ints = np.random.randint(1, 100, 10)\nprint(rand_ints)\n\n# Single random integer\nsingle = np.random.randint(1, 100)\nprint(single)\n</code></pre> <p>Expected output (values will vary):</p> <pre><code>[42 73 18 91 35 67 22 88 14 59]\n76\n</code></pre> <p>ML use case: Creating random indices for data shuffling, generating discrete synthetic data, or random sampling.</p>"},{"location":"02-math/numpy/#5-array-attributes-and-methods","title":"5. Array Attributes and Methods","text":"<p>Understanding array properties helps you debug shape mismatches and verify data dimensions.</p> <p>Key attributes:</p> <pre><code>arr = np.arange(25)\nprint(f\"Array: {arr}\")\nprint(f\"Shape: {arr.shape}\")\nprint(f\"Size (total elements): {arr.size}\")\nprint(f\"Data type: {arr.dtype}\")\nprint(f\"Number of dimensions: {arr.ndim}\")\n</code></pre> <p>Expected output:</p> <pre><code>Array: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]\nShape: (25,)\nSize (total elements): 25\nData type: int64\nNumber of dimensions: 1\n</code></pre>"},{"location":"02-math/numpy/#6-reshaping-arrays","title":"6. Reshaping Arrays","text":"<p>Reshaping changes array dimensions without changing the data. Essential for preparing data for ML models.</p> <p>Reshape a 1D array to 2D:</p> <pre><code>arr = np.arange(30)\nprint(\"Original shape:\", arr.shape)\n\n# Reshape to 5 rows, 6 columns\nreshaped = arr.reshape(5, 6)\nprint(\"\\nReshaped to (5, 6):\")\nprint(reshaped)\nprint(\"New shape:\", reshaped.shape)\n</code></pre> <p>Expected output:</p> <pre><code>Original shape: (30,)\n\nReshaped to (5, 6):\n[[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]\n [24 25 26 27 28 29]]\nNew shape: (5, 6)\n</code></pre> <p>Important rule: Total elements must remain the same. You cannot reshape 30 elements into a <code>(4, 8)</code> array because \\(\\(4 \\times 8 = 32 \\neq 30\\)\\).</p> <p>ML use case: Image data often needs reshaping from flat vectors to \\(\\((height, width, channels)\\)\\) format for CNNs.</p>"},{"location":"02-math/numpy/#7-finding-maximum-minimum-and-indices","title":"7. Finding Maximum, Minimum, and Indices","text":"<p>These methods help identify extreme values in data.</p> <pre><code>ranarr = np.random.randint(0, 100, 10)\nprint(\"Array:\", ranarr)\nprint(\"Maximum value:\", ranarr.max())\nprint(\"Minimum value:\", ranarr.min())\nprint(\"Index of maximum:\", ranarr.argmax())\nprint(\"Index of minimum:\", ranarr.argmin())\n</code></pre> <p>Expected output (values will vary):</p> <pre><code>Array: [42 87 15 93 28 61 74 19 56 38]\nMaximum value: 93\nMinimum value: 15\nIndex of maximum: 3\nIndex of minimum: 2\n</code></pre> <p>ML use case: Finding the predicted class in classification (index of maximum probability), identifying outliers, or locating peak activations in neural networks.</p>"},{"location":"02-math/numpy/#8-array-indexing-and-slicing","title":"8. Array Indexing and Slicing","text":"<p>NumPy indexing works similarly to Python lists but extends to multiple dimensions.</p>"},{"location":"02-math/numpy/#basic-1d-indexing","title":"Basic 1D Indexing","text":"<pre><code>arr = np.arange(0, 11)\nprint(\"Array:\", arr)\n\n# Access single element\nprint(\"Element at index 5:\", arr[5])\n\n# Slice a range\nprint(\"Elements 1-5:\", arr[1:6])\n\n# Slice from start\nprint(\"First 4 elements:\", arr[:4])\n\n# Slice to end\nprint(\"Last 3 elements:\", arr[-3:])\n</code></pre> <p>Expected output:</p> <pre><code>Array: [ 0  1  2  3  4  5  6  7  8  9 10]\nElement at index 5: 5\nElements 1-5: [1 2 3 4 5]\nFirst 4 elements: [0 1 2 3]\nLast 3 elements: [ 8  9 10]\n</code></pre>"},{"location":"02-math/numpy/#broadcasting-assigning-values-to-slices","title":"Broadcasting: Assigning Values to Slices","text":"<p>NumPy allows assigning a single value to multiple array elements at once.</p> <pre><code>arr = np.arange(0, 11)\nprint(\"Original:\", arr)\n\n# Set indices 3-6 to 300\narr[3:6] = 300\nprint(\"After broadcasting:\", arr)\n</code></pre> <p>Expected output:</p> <pre><code>Original: [ 0  1  2  3  4  5  6  7  8  9 10]\nAfter broadcasting: [  0   1   2 300 300 300   6   7   8   9  10]\n</code></pre> <p>Why broadcasting matters: Enables efficient operations on entire array slices without loops, a cornerstone of vectorized programming.</p>"},{"location":"02-math/numpy/#9-2d-array-indexing","title":"9. 2D Array Indexing","text":"<p>Matrices require row and column indices.</p> <p>Access elements and slices:</p> <pre><code>arr_2d = np.array([[5, 10, 15],\n                   [20, 25, 30],\n                   [35, 40, 45]])\n\nprint(\"Full matrix:\")\nprint(arr_2d)\n\n# Access single element (row 1, column 2)\nprint(\"\\nElement [1, 2]:\", arr_2d[1, 2])\n\n# Get entire row\nprint(\"\\nRow 2:\", arr_2d[2])\n\n# Get entire column\nprint(\"\\nColumn 1:\", arr_2d[:, 1])\n\n# Slice submatrix (top-right 2x2)\nprint(\"\\nTop-right 2x2:\")\nprint(arr_2d[0:2, 1:3])\n\n# All rows, columns 1 and 2\nprint(\"\\nAll rows, columns 1-2:\")\nprint(arr_2d[:, 1:3])\n</code></pre> <p>Expected output:</p> <pre><code>Full matrix:\n[[ 5 10 15]\n [20 25 30]\n [35 40 45]]\n\nElement [1, 2]: 30\n\nRow 2: [35 40 45]\n\nColumn 1: [10 25 40]\n\nTop-right 2x2:\n[[10 15]\n [25 30]]\n\nAll rows, columns 1-2:\n[[10 15]\n [25 30]\n [40 45]]\n</code></pre> <p>Notation: <code>arr_2d[row, col]</code> is clearer than <code>arr_2d[row][col]</code> and slightly faster.</p>"},{"location":"02-math/numpy/#10-boolean-indexing-conditional-selection","title":"10. Boolean Indexing (Conditional Selection)","text":"<p>Select array elements that meet a condition\u2014extremely powerful for data filtering.</p> <pre><code>arr = np.arange(1, 11)\nprint(\"Array:\", arr)\n\n# Boolean mask: which elements are greater than 5?\nmask = arr &gt; 5\nprint(\"\\nBoolean mask (arr &gt; 5):\", mask)\n\n# Select only elements greater than 5\nfiltered = arr[arr &gt; 5]\nprint(\"Elements &gt; 5:\", filtered)\n\n# Select elements less than or equal to 5\nfiltered_le = arr[arr &lt;= 5]\nprint(\"Elements &lt;= 5:\", filtered_le)\n</code></pre> <p>Expected output:</p> <pre><code>Array: [ 1  2  3  4  5  6  7  8  9 10]\n\nBoolean mask (arr &gt; 5): [False False False False False  True  True  True  True  True]\nElements &gt; 5: [ 6  7  8  9 10]\nElements &lt;= 5: [1 2 3 4 5]\n</code></pre> <p>How it works: The condition <code>arr &gt; 5</code> returns a Boolean array. Using this as an index returns only <code>True</code> positions.</p> <p>ML use case: Filtering outliers, selecting samples meeting criteria, or applying threshold-based decisions.</p>"},{"location":"02-math/numpy/#11-vectorized-arithmetic-operations","title":"11. Vectorized Arithmetic Operations","text":"<p>NumPy performs element-wise operations across entire arrays without loops.</p> <pre><code>arr = np.arange(0, 10)\nprint(\"Array:\", arr)\n\n# Array + Array (element-wise)\nprint(\"arr + arr:\", arr + arr)\n\n# Array * Array (element-wise)\nprint(\"arr * arr:\", arr * arr)\n\n# Array - Array\nprint(\"arr - arr:\", arr - arr)\n\n# Scalar operations\nprint(\"arr + 10:\", arr + 10)\nprint(\"arr * 2:\", arr * 2)\nprint(\"arr ** 2:\", arr ** 2)\n</code></pre> <p>Expected output:</p> <pre><code>Array: [0 1 2 3 4 5 6 7 8 9]\narr + arr: [ 0  2  4  6  8 10 12 14 16 18]\narr * arr: [ 0  1  4  9 16 25 36 49 64 81]\narr - arr: [0 0 0 0 0 0 0 0 0 0]\narr + 10: [10 11 12 13 14 15 16 17 18 19]\narr * 2: [ 0  2  4  6  8 10 12 14 16 18]\narr ** 2: [ 0  1  4  9 16 25 36 49 64 81]\n</code></pre> <p>Critical insight: Operations like <code>arr * arr</code> multiply corresponding elements, not matrix multiplication. For matrix multiplication, use <code>np.dot()</code> or <code>@</code> operator.</p>"},{"location":"02-math/numpy/#12-universal-functions-ufuncs","title":"12. Universal Functions (ufuncs)","text":"<p>NumPy provides optimized mathematical functions that operate element-wise.</p> <pre><code>arr = np.arange(1, 11)\n\n# Square root\nprint(\"Square roots:\", np.sqrt(arr))\n\n# Exponential\nprint(\"Exponentials:\", np.exp(arr[:3]))  # First 3 to avoid huge numbers\n\n# Logarithm\nprint(\"Natural log:\", np.log(arr))\n\n# Trigonometric\nangles = np.array([0, np.pi/2, np.pi])\nprint(\"Sine:\", np.sin(angles))\n</code></pre> <p>Expected output:</p> <pre><code>Square roots: [1.         1.41421356 1.73205081 2.         2.23606798 2.44948975\n 2.64575131 2.82842712 3.         3.16227766]\nExponentials: [ 2.71828183  7.3890561  20.08553692]\nNatural log: [0.         0.69314718 1.09861229 1.38629436 1.60943791 1.79175947\n 1.94591015 2.07944154 2.19722458 2.30258509]\nSine: [0.0000000e+00 1.0000000e+00 1.2246468e-16]\n</code></pre> <p>ML use case: Activation functions (sigmoid, tanh), normalizing data (log transform), computing distances (square root of sum of squares).</p>"},{"location":"02-math/numpy/#quick-reference","title":"Quick Reference","text":"Operation Code Use Case Create from list <code>np.array([1, 2, 3])</code> Convert Python data to NumPy Range <code>np.arange(0, 10, 2)</code> Sequential values Evenly spaced <code>np.linspace(0, 1, 100)</code> Smooth intervals Zeros/Ones <code>np.zeros((3, 4))</code>, <code>np.ones((2, 3))</code> Initialize arrays Random uniform <code>np.random.rand(3, 3)</code> Random weights (0-1) Random normal <code>np.random.randn(3, 3)</code> Gaussian initialization Random integers <code>np.random.randint(1, 100, 10)</code> Discrete sampling Reshape <code>arr.reshape(5, 6)</code> Change dimensions Indexing <code>arr</code>, <code>arr[1:5]</code>, <code>arr[arr &gt; 3]</code> Access elements 2D indexing <code>matrix[row, col]</code>, <code>matrix[:, col]</code> Access matrix data Element-wise ops <code>arr + 5</code>, <code>arr * arr</code>, <code>arr ** 2</code> Vectorized arithmetic Math functions <code>np.sqrt()</code>, <code>np.exp()</code>, <code>np.log()</code> Element-wise functions"},{"location":"02-math/numpy/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned to create NumPy arrays using multiple methods, manipulate array shapes and dimensions, perform efficient indexing and slicing including Boolean selection, and apply vectorized operations that replace slow Python loops.</p> <p>Best practices:</p> <ul> <li>Always use NumPy arrays for numerical data instead of lists</li> <li>Think in terms of array operations rather than element-by-element loops</li> <li>Check array shapes frequently when debugging to catch dimension mismatches early</li> <li>Use vectorization wherever possible for speed and code clarity</li> <li>Understand broadcasting to efficiently combine arrays of different shapes</li> </ul> <p>Performance mindset: If you find yourself writing a <code>for</code> loop to process array elements, ask: \"Can I vectorize this?\" The answer is almost always yes in NumPy.</p> <p>Connections to ML:</p> <ul> <li>Data representation: Datasets become NumPy arrays with shape <code>(samples, features)</code></li> <li>Model operations: Linear layers compute \\(\\(\\mathbf{y} = W\\mathbf{x} + \\mathbf{b}\\)\\) using NumPy operations</li> <li>Gradient descent: Parameter updates use vectorized addition and multiplication</li> <li>Image processing: Images are NumPy arrays with shape <code>(height, width, channels)</code></li> </ul> <p>External resources:</p> <ul> <li>NumPy Official Documentation - comprehensive reference and tutorials</li> <li>NumPy for Absolute Beginners - official beginner guide</li> <li>From Python to NumPy - free online book on vectorization</li> </ul>"},{"location":"02-math/probability-statistics/","title":"Probability &amp; Statistics for Machine Learning","text":"<p>This tutorial introduces probability and statistics fundamentals that are essential for understanding and applying machine learning. You'll learn to quantify uncertainty, understand distributions, and learn how probabilistic thinking enables ML models to make predictions under uncertainty.</p> <p>Estimated time: 70 minutes</p>"},{"location":"02-math/probability-statistics/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: Machine learning models don't make perfectly accurate predictions\u2014they work with noisy, incomplete data and must quantify uncertainty. Without probability theory, we cannot model randomness, evaluate model reliability, or understand why algorithms make certain predictions.</p> <p>Practical benefits: Probability provides the mathematical framework for handling uncertainty. Understanding distributions helps you choose appropriate models, compute confidence intervals, detect outliers, and interpret what predictions actually mean. Statistics enables you to validate model improvements and make data-driven decisions backed by evidence.</p> <p>Professional context: Every ML algorithm involves probability. Classification models output probability distributions over classes (\"80% cat, 20% dog\"), regression models have probabilistic error terms, and neural networks learn probability distributions. Understanding these foundations is essential for building and trusting ML systems.</p> <p></p>"},{"location":"02-math/probability-statistics/#prerequisites-learning-objectives","title":"Prerequisites &amp; Learning Objectives","text":"<p>Required knowledge: - Basic algebra and arithmetic - Understanding of functions and graphs - Familiarity with summation notation - Basic Python (minimal needed)</p> <p>Learning outcomes: - Understand what probability measures and how to compute it - Distinguish between independent and disjoint events - Apply addition and multiplication rules for probabilities - Work with probability distributions and density functions - Compute and interpret mean, variance, and standard deviation - Recognize common distributions and their ML applications</p>"},{"location":"02-math/probability-statistics/#core-concepts","title":"Core Concepts","text":""},{"location":"02-math/probability-statistics/#what-is-probability","title":"What is Probability?","text":"<p>Probability is a number between 0 and 1 that measures the likelihood of an event occurring.</p> <p>Properties:</p> <ul> <li> \\[0 \\leq P(A) \\leq 1$$ for any event $$A\\] </li> <li> \\[P(\\text{certain event}) = 1\\] </li> <li> \\[P(\\text{impossible event}) = 0\\] </li> </ul> <p>Example: Rolling a fair die</p> <ul> <li> \\[P(\\text{rolling a 6}) = \\frac{1}{6} \\approx 0.167\\] </li> <li> \\[P(\\text{rolling even number}) = \\frac{3}{6} = 0.5\\] </li> <li>\\(\\(P(\\text{rolling 7}) = 0\\)\\) (impossible)</li> </ul> <p>In ML, probability models uncertainty. A classifier doesn't say \"this IS a cat\"\u2014it says \"I'm 85% confident this is a cat.\"</p>"},{"location":"02-math/probability-statistics/#random-variables","title":"Random Variables","text":"<p>A random variable is a variable whose value is determined by chance.</p> <p>Types:</p> <ul> <li>Discrete: Countable values (coin flips, dice rolls, number of customers)</li> <li>Continuous: Any value in a range (height, temperature, prediction error)</li> </ul> <p>Features are random variables (customer age varies), predictions have uncertainty, and errors are random.</p>"},{"location":"02-math/probability-statistics/#basic-probability-notation","title":"Basic Probability Notation","text":"<p>Key notation:</p> <ul> <li>\\(\\(P(A)\\)\\): Probability event \\(\\(A\\)\\) occurs</li> <li>\\(\\(P(A \\cap B)\\)\\): Probability both \\(\\(A\\)\\) AND \\(\\(B\\)\\) occur</li> <li>\\(\\(P(A \\cup B)\\)\\): Probability \\(\\(A\\)\\) OR \\(\\(B\\)\\) occurs</li> <li>\\(\\(P(A|B)\\)\\): Probability of \\(\\(A\\)\\) given \\(\\(B\\)\\) occurred (conditional)</li> <li>\\(\\(P(A^c)\\)\\): Probability \\(\\(A\\)\\) does NOT occur (complement)</li> </ul> <p>Complement rule:</p> \\[ P(A^c) = 1 - P(A) \\] <p>Example: Weather forecasting</p> <ul> <li> \\[P(\\text{rain}) = 0.3\\] </li> <li> \\[P(\\text{no rain}) = 1 - 0.3 = 0.7\\] </li> </ul>"},{"location":"02-math/probability-statistics/#independent-vs-disjoint-events","title":"Independent vs Disjoint Events","text":"<p>These concepts are different and often confused.</p> <p>Disjoint (Mutually Exclusive): Cannot both happen simultaneously</p> <ul> <li>Rolling a 3 AND rolling a 5 on one die (impossible)</li> <li>\\(\\(A \\cap B = \\emptyset\\)\\) (empty)</li> <li>If \\(\\(A\\)\\) happens, \\(\\(B\\)\\) definitely doesn't</li> </ul> <p>Independent: One event doesn't affect the other's probability</p> <ul> <li>Flipping two different coins</li> <li> \\[P(A \\cap B) = P(A) \\cdot P(B)\\] </li> <li>Knowing \\(\\(A\\)\\) occurred doesn't change \\(\\(P(B)\\)\\)</li> </ul> <p></p> <p>Critical distinction: Disjoint events are NEVER independent (unless probability is 0). If events can't occur together, they're dependent!</p> <p>Naive Bayes (a ML model) assumes features are independent. Checking this assumption matters for model accuracy.</p>"},{"location":"02-math/probability-statistics/#addition-rule","title":"Addition Rule","text":"<p>Computes probability that at least one event occurs.</p> <p>General addition rule: $$ P(A \\cup B) = P(A) + P(B) - P(A \\cap B) $$</p> <p>Why subtract? Adding \\(\\(P(A) + P(B)\\)\\) double-counts the overlap.</p> <p>Special case (disjoint): $$ P(A \\cup B) = P(A) + P(B) $$</p> <p>Example: Drawing a card</p> <ul> <li> \\[P(\\text{Heart}) = \\frac{13}{52} = 0.25\\] </li> <li> \\[P(\\text{Ace}) = \\frac{4}{52} = 0.077\\] </li> <li> \\[P(\\text{Ace of Hearts}) = \\frac{1}{52} = 0.019\\] </li> <li> \\[P(\\text{Heart OR Ace}) = 0.25 + 0.077 - 0.019 = 0.308\\] </li> </ul>"},{"location":"02-math/probability-statistics/#multiplication-rule","title":"Multiplication Rule","text":"<p>Computes probability that both events occur.</p> <p>General multiplication rule: $$ P(A \\cap B) = P(A) \\cdot P(B|A) $$</p> <p>Independent events: $$ P(A \\cap B) = P(A) \\cdot P(B) $$</p> <p>Example: Drawing two cards without replacement</p> <ul> <li> \\[P(\\text{first ace}) = \\frac{4}{52}\\] </li> <li> \\[P(\\text{second ace | first ace}) = \\frac{3}{51}\\] </li> <li> \\[P(\\text{both aces}) = \\frac{4}{52} \\times \\frac{3}{51} = 0.0045\\] </li> </ul> <p>ML connection: Computing joint probabilities in probabilistic models.</p>"},{"location":"02-math/probability-statistics/#conditional-probability-and-bayes-theorem","title":"Conditional Probability and Bayes' Theorem","text":"<p>Conditional probability: Probability of \\(\\(A\\)\\) given \\(\\(B\\)\\) occurred</p> \\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\] <p>Bayes' Theorem (fundamental for ML): $$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$</p> <p></p> <p>Example: Medical testing</p> <ul> <li>Disease prevalence: \\(\\(P(\\text{disease}) = 0.01\\)\\) (1%)</li> <li>Test accuracy: \\(\\(P(\\text{positive | disease}) = 0.95\\)\\)</li> <li>False positive rate: \\(\\(P(\\text{positive | no disease}) = 0.05\\)\\)</li> </ul> <p>Even with a positive test, the actual probability of having the disease is only about 16% because the disease is rare!</p> <p>ML connection: Classification uses Bayes' theorem to compute \\(\\(P(\\text{class | features})\\)\\) from \\(\\(P(\\text{features | class})\\)\\).</p>"},{"location":"02-math/probability-statistics/#probability-distributions","title":"Probability Distributions","text":"<p>A probability distribution describes how probability is allocated across possible values.</p> <p>Types:</p> <ul> <li>Discrete: Probability Mass Function (PMF) - specific values have probabilities</li> <li>Continuous: Probability Density Function (PDF) - probabilities over ranges</li> </ul> <p>All probabilities sum (discrete) or integrate (continuous) to 1.</p> <p>Example: Fair die has uniform distribution</p> <ul> <li> \\[P(X = 1) = P(X = 2) = \\cdots = P(X = 6) = \\frac{1}{6}\\] </li> </ul>"},{"location":"02-math/probability-statistics/#probability-mass-function-pmf","title":"Probability Mass Function (PMF)","text":"<p>For discrete variables, PMF gives \\(\\(P(X = x)\\)\\) for each value.</p> <p>Properties: - \\(\\(p_X(x) \\geq 0\\)\\) for all \\(\\(x\\)\\) - \\(\\(\\sum_x p_X(x) = 1\\)\\)</p> <p>Example: Number of heads in 3 coin flips</p> Heads (x) 0 1 2 3 P(X = x) 1/8 3/8 3/8 1/8 <p>Classification outputs are PMFs over classes.</p>"},{"location":"02-math/probability-statistics/#probability-density-function-pdf","title":"Probability Density Function (PDF)","text":"<p>For continuous variables, PDF \\(\\(f(x)\\)\\) describes relative likelihood.</p> <p>Key difference: For continuous variables, \\(\\(P(X = x) = 0\\)\\) for any specific \\(\\(x\\)\\). We compute probabilities over intervals:</p> \\[ P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx \\] <p>Properties:</p> <ul> <li> \\[f(x) \\geq 0\\] </li> <li> \\[\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\\] </li> <li>Height \\(\\(f(x)\\)\\) is NOT a probability (can exceed 1)</li> </ul> <p>Regression assumes errors follow a PDF (usually Gaussian/normal distribution).</p>"},{"location":"02-math/probability-statistics/#cumulative-distribution-function-cdf","title":"Cumulative Distribution Function (CDF)","text":"<p>The CDF gives probability that a random variable is less than or equal to a value:</p> \\[ F(x) = P(X \\leq x) \\] <p>Properties:</p> <ul> <li> \\[0 \\leq F(x) \\leq 1\\] </li> <li>\\(\\(F(x)\\)\\) is non-decreasing</li> <li> \\[F(-\\infty) = 0$$, $$F(\\infty) = 1\\] </li> </ul> <p>Use: Compute probability over range $$ P(a &lt; X \\leq b) = F(b) - F(a) $$</p> <p>Example: For standard normal distribution</p> <ul> <li>\\(\\(F(0) = 0.5\\)\\) (50% of values below 0)</li> <li>\\(\\(F(1) = 0.841\\)\\) (84.1% of values below 1)</li> <li>\\(\\(P(-1 \\leq X \\leq 1) = F(1) - F(-1) = 0.683\\)\\) (68.3%)</li> </ul>"},{"location":"02-math/probability-statistics/#percentiles","title":"Percentiles","text":"<p>The p-th percentile is the value below which p% of observations fall.</p> <p>Common percentiles:</p> <ul> <li>25th (Q1), 50th (median), 75th (Q3)</li> <li>Interquartile Range: \\(\\(IQR = Q3 - Q1\\)\\)</li> </ul> <p></p> <p>Outlier detection: Values outside \\(\\([Q1 - 1.5 \\times IQR, Q3 + 1.5 \\times IQR]\\)\\)</p> <p>ML connection: \"This model's 90th percentile error is 5 units\" means 90% of predictions have error \u2264 5.</p>"},{"location":"02-math/probability-statistics/#mean-variance-and-standard-deviation","title":"Mean, Variance, and Standard Deviation","text":"<p>Let's use a simple dataset to illustrate: Test scores: 70, 75, 80, 85, 90</p> <p>Mean (average): $$ \\mu = \\frac{70 + 75 + 80 + 85 + 90}{5} = \\frac{400}{5} = 80 $$</p> <p>The mean is the \"center\" or typical value.</p> <p>Variance (spread): $$ \\sigma^2 = \\frac{(70-80)^2 + (75-80)^2 + (80-80)^2 + (85-80)^2 + (90-80)^2}{5} $$ $$ = \\frac{100 + 25 + 0 + 25 + 100}{5} = \\frac{250}{5} = 50 $$</p> <p>Variance measures average squared distance from mean.</p> <p>Standard deviation: $$ \\sigma = \\sqrt{50} \\approx 7.07 $$</p> <p>Standard deviation is typical distance from mean, in original units.</p> <p>Interpretation: Most scores are within \u00b17 points of the average (80).</p> <p>Properties:</p> <ul> <li>Higher variance/std dev = more spread out data</li> <li>Zero variance = all values identical</li> <li>Units: variance is squared, std dev matches data units</li> </ul> <p>ML connection: </p> <ul> <li>Mean Squared Error (MSE) is the variance of prediction errors</li> <li>Standard deviation tells you typical prediction error magnitude</li> <li>Models try to minimize variance of errors</li> </ul>"},{"location":"02-math/probability-statistics/#common-probability-distributions","title":"Common Probability Distributions","text":""},{"location":"02-math/probability-statistics/#uniform-distribution","title":"Uniform Distribution","text":"<p>Intuition: All values equally likely in a range.</p> <p></p> <p>Use cases:</p> <ul> <li>Random initialization of neural network weights</li> <li>Generating test data</li> <li>Modeling \"no prior knowledge\" scenarios</li> </ul> <p>Example: Roll a fair die - each number has probability 1/6</p> <p>ML application: Dropout in neural networks randomly selects neurons uniformly.</p>"},{"location":"02-math/probability-statistics/#normal-gaussian-distribution","title":"Normal (Gaussian) Distribution","text":"<p>Intuition: Bell-shaped curve, most values near the mean, symmetric.</p> <p></p> <p>Use cases:</p> <ul> <li>Natural measurements (height, IQ)</li> <li>Modeling errors and noise</li> <li>Central Limit Theorem applications</li> </ul> <p>Parameters: Mean \\(\\(\\mu\\)\\), variance \\(\\(\\sigma^2\\)\\)</p> <p>68-95-99.7 rule:</p> <ul> <li>68% of data within 1 standard deviation of mean</li> <li>95% within 2 standard deviations</li> <li>99.7% within 3 standard deviations</li> </ul> <p>ML applications:</p> <ul> <li>Linear regression assumes normal error distribution</li> <li>Gaussian Naive Bayes classifier</li> <li>Gaussian Processes</li> <li>Many algorithms assume normality for mathematical convenience</li> </ul>"},{"location":"02-math/probability-statistics/#bernoulli-distribution","title":"Bernoulli Distribution","text":"<p>Intuition: Single yes/no trial (success/failure).</p> <p></p> <p>Use cases:</p> <ul> <li>Coin flip (heads/tails)</li> <li>Binary classification (cat/not cat)</li> <li>Click/no click on ad</li> </ul> <p>Parameters: \\(\\(p\\)\\) = probability of success</p> <p>ML connection: Logistic regression outputs Bernoulli probabilities for binary classification.</p>"},{"location":"02-math/probability-statistics/#binomial-distribution","title":"Binomial Distribution","text":"<p>Intuition: Number of successes in \\(\\(n\\)\\) independent yes/no trials.</p> <p></p> <p>Use cases:</p> <ul> <li>Number of heads in 10 coin flips</li> <li>Number of customers who buy product</li> <li>Number of correct answers on multiple-choice test</li> </ul> <p>Parameters: \\(\\(n\\)\\) (trials), \\(\\(p\\)\\) (success probability)</p> <p>Mean: \\(\\(np\\)\\), Variance: \\(\\(np(1-p)\\)\\)</p> <p>Example: Flip coin 10 times, expect 5 heads on average if fair.</p>"},{"location":"02-math/probability-statistics/#poisson-distribution","title":"Poisson Distribution","text":"<p>Intuition: Number of events occurring in fixed time/space interval when events happen independently at constant average rate.</p> <p></p> <p>Use cases:</p> <ul> <li>Number of emails received per hour</li> <li>Customer arrivals at store per day</li> <li>Website visits per minute</li> <li>Rare events</li> </ul> <p>Parameters: \\(\\(\\lambda\\)\\) (average rate)</p> <p>ML connection: Poisson regression for count data (predicting number of occurrences).</p>"},{"location":"02-math/probability-statistics/#exponential-distribution","title":"Exponential Distribution","text":"<p>Intuition: Time between events in a Poisson process.</p> <p></p> <p>Use cases: - Time until next customer arrives - Lifetime of electronic component - Time between failures</p> <p>Parameters: \\(\\(\\lambda\\)\\) (rate)</p> <p>Relationship: If events follow Poisson, time between events follows exponential.</p>"},{"location":"02-math/probability-statistics/#quick-reference","title":"Quick Reference","text":"Concept Definition ML Application Probability Likelihood measure, 0 to 1 Quantifying uncertainty Independent \\(\\(P(A \\cap B) = P(A)P(B)\\)\\) Naive Bayes assumption Addition rule \\(\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\\) Multiple outcomes Conditional \\(\\(P(A\\|B) = \\frac{P(A \\cap B)}{P(B)}\\)\\) Context-dependent prediction Bayes' theorem \\(\\(P(A\\|B) = \\frac{P(B\\|A)P(A)}{P(B)}\\)\\) Classification Mean Center of distribution Expected value Variance Spread measure Error magnitude Normal distribution Bell curve Error modeling PMF Discrete probabilities Classification outputs PDF Continuous density Regression errors CDF Cumulative probability Percentiles, p-values"},{"location":"02-math/probability-statistics/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned how probability quantifies uncertainty, mastered probability rules, understood distributions as models of randomness, computed statistical measures, and connected concepts to ML applications.</p> <p>Best practices:</p> <ul> <li>Check distribution assumptions before applying methods</li> <li>Visualize data distributions to verify model fit</li> <li>Consider base rates (priors) in classification</li> <li>Use appropriate metrics for your distribution type</li> </ul> <p>Connections to ML:</p> <ul> <li>Classification outputs probability distributions (softmax)</li> <li>Regression assumes error distributions (usually normal)</li> <li>Uncertainty quantification uses probability</li> <li>Bayesian ML treats parameters as random variables</li> </ul> <p>External resources: - Khan Academy: Probability &amp; Statistics - interactive lessons - 3Blue1Brown: Bayes' Theorem - visual explanation - Seeing Theory - visual introduction to probability</p>"},{"location":"03-data/","title":"Data Overview","text":"<p>Turn raw data into answers. Short, practical lessons that take data from messy to model\u2011ready.</p> <p></p>"},{"location":"03-data/#data-fundamentals","title":"Data Fundamentals","text":"<p>Understanding the data pipeline</p> <p>Learn why data quality determines ML success and master the six-phase pipeline: collection, exploration, cleaning, transformation, splitting, and storage.</p> <p>You'll learn: Data types, quality dimensions, common challenges (imbalanced, missing, drift), and best practices for reproducible workflows</p>"},{"location":"03-data/#pandas","title":"Pandas","text":"<p>Load, explore, and reshape data fast</p> <p>Read files, select rows/columns, filter, group, merge, and tidy your datasets so analysis feels smooth, not painful.</p> <p>You\u2019ll learn: Importing data, cleaning columns, joins, groupby, and quick summaries</p>"},{"location":"03-data/#visualization","title":"Visualization","text":"<p>Make patterns visible</p> <p>Tell a clear story with plots that highlight trends, comparisons, and outliers using Matplotlib and Seaborn.</p> <p>You\u2019ll learn: Histograms, box plots, scatter plots, line charts, and choosing the right chart for the question</p>"},{"location":"03-data/#data-wrangling","title":"Data wrangling","text":"<p>From messy to usable</p> <p>Handle missing values, fix types, engineer features, and prepare datasets that models can learn from.</p> <p>You\u2019ll learn: Imputation, encoding, scaling, datetime handling, and feature creation</p>"},{"location":"03-data/#data-collection","title":"Data collection","text":"<p>Get the data you need</p> <p>Pull data from files, APIs, and the web\u2014then store it in formats that are easy to use and share.</p> <p>You\u2019ll learn: CSV/Parquet basics, calling APIs, simple scraping, and organizing raw vs. processed data</p>"},{"location":"03-data/#sql-databases","title":"SQL databases","text":"<p>Query data with confidence</p> <p>Use SQL to select, join, aggregate, and filter data directly where it lives\u2014fast and reproducible.</p> <p>You\u2019ll learn: SELECT, WHERE, JOIN, GROUP BY, HAVING, and writing queries for analysis</p>"},{"location":"03-data/#nosql-mongodb","title":"NoSQL (MongoDB)","text":"<p>Work with flexible data</p> <p>Store and query JSON\u2011like documents for logs, events, and semi\u2011structured sources common in ML workflows.</p> <p>You\u2019ll learn: Inserting documents, filtering with queries, projections, and basic aggregations</p> <p>Start with a small dataset, answer one question with a plot, then clean and shape the data for a simple model. Repeat\u2014each pass makes the story (and the model) sharper.</p>"},{"location":"03-data/data-collection/","title":"Data collection","text":""},{"location":"03-data/data-collection/#materials-under-construction","title":"Materials Under Construction","text":"<p>This section is still brewing. In the meantime, grab a snack, ship some code, and check back soon.</p> <p></p>"},{"location":"03-data/data-foundations/","title":"Data: The Foundation of Machine Learning","text":"<p>This tutorial introduces the essential role of data in machine learning and walks through the key phases of preparing data for modeling. You'll understand why data quality matters more than algorithm choice and learn the systematic approach to handling data from collection to model-ready format.</p> <p>Estimated time: 40 minutes</p>"},{"location":"03-data/data-foundations/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: </p> <p>The most sophisticated machine learning algorithm is worthless without good data. </p> <p>Models learn patterns from data, so if your data is biased, incomplete, or messy, your predictions will be unreliable regardless of model complexity. The phrase \"garbage in, garbage out\" is fundamental, because poor data quality leads directly to poor model performance.</p> <p>Practical benefits: Understanding data workflows enables you to spot quality issues early, choose appropriate preprocessing strategies, and build robust ML systems that work in production. </p> <p>Professional context: Industry surveys consistently show that data scientists spend 60-80% of their time on data preparation rather than modeling. Companies with strong data infrastructure outperform competitors regardless of algorithm sophistication. The most impactful ML practitioners aren't necessarily those who know the fanciest algorithms; they're the ones who understand their data deeply, handle it correctly, and ensure quality throughout the pipeline.</p> <p></p>"},{"location":"03-data/data-foundations/#core-concepts","title":"Core Concepts","text":""},{"location":"03-data/data-foundations/#what-is-data-in-ml-context","title":"What is Data in ML Context?","text":"<p>In machine learning, data consists of examples (also called samples, observations, or rows) with features (attributes, variables, or columns) and optionally labels (target values for supervised learning).</p> <p>Example: Predicting house prices</p> <ul> <li>Sample: One house</li> <li>Features: Square footage, number of bedrooms, location, age, etc.</li> <li>Label: Sale price (what we're trying to predict)</li> </ul>"},{"location":"03-data/data-foundations/#types-of-data","title":"Types of Data","text":"<p>Structured vs Unstructured:</p> <ul> <li>Structured: Organized in tables with defined columns (databases, spreadsheets)</li> <li>Unstructured: No predefined format (text, images, audio, video)</li> </ul> <p>Numerical vs Categorical:</p> <ul> <li>Numerical: Quantitative values (age: 25, temperature: 18.5\u00b0C)</li> <li>Categorical: Discrete categories (color: red/blue/green, city: Tirana/Durr\u00ebs)</li> </ul> <p>Time-series: Data points ordered by time (stock prices, sensor readings, web traffic)</p>"},{"location":"03-data/data-foundations/#data-quality-dimensions","title":"Data Quality Dimensions","text":"<p>Completeness: Are all expected values present? Missing data can bias results.</p> <p>Accuracy: Do values reflect reality? Measurement errors propagate through models.</p> <p>Consistency: Do values follow expected formats and rules? Inconsistencies cause processing failures.</p> <p>Relevance: Do features relate to your prediction task? Irrelevant features add noise.</p>"},{"location":"03-data/data-foundations/#the-data-pipeline-key-phases","title":"The Data Pipeline: Key Phases","text":"<p>The machine learning data pipeline is a series of steps that transform raw data into a format suitable for training models. Think of it as a factory assembly line where raw materials become finished products.</p> <p></p>"},{"location":"03-data/data-foundations/#phase-1-data-collection","title":"Phase 1: Data Collection","text":"<p>Purpose: Gather data from various sources to create your dataset.</p> <p>Common sources:</p> <ul> <li>Databases: SQL/NoSQL databases (MySQL, PostgreSQL, MongoDB)</li> <li>APIs: Web services providing data programmatically</li> <li>Web scraping: Extracting data from websites</li> <li>Files: CSV, JSON, Excel files</li> <li>Sensors/IoT: Real-time streaming data</li> <li>User input: Forms, surveys, clicks</li> </ul> <p>Key considerations:</p> <ul> <li>Representative samples: Data should reflect the real-world distribution you'll encounter</li> <li>Bias awareness: Who collected this data? What populations are missing?</li> <li>Volume: Do you have enough samples for your task? Rule of thumb: at least 10 samples per feature</li> </ul> <p>How you collect data determines what patterns your model can learn. </p> <p>If your training data only includes customers from one region, your model won't work well for other regions.</p> <p>Example: Building a spam detector</p> <ul> <li>Good collection: Emails from diverse users, timeframes, and domains</li> <li>Poor collection: Only emails from one company's inbox over one week</li> </ul>"},{"location":"03-data/data-foundations/#phase-2-data-exploration-eda","title":"Phase 2: Data Exploration (EDA)","text":"<p>Purpose: Understand what you have before making changes.</p> <p>Key questions:</p> <ul> <li>What does the data look like? (first few rows)</li> <li>What types are the features? (numerical, categorical, dates)</li> <li>What are the distributions? (histograms, box plots)</li> <li>Are there obvious outliers or anomalies?</li> <li>What patterns exist? (correlations, trends)</li> <li>What's missing? (null values, empty fields)</li> </ul> <p>Tools: Summary statistics (mean, median, std), visualizations (histograms, scatter plots), correlation matrices</p> <p>ML connection: Exploration guides your modeling choices. If features are highly correlated, some algorithms may struggle. If classes are imbalanced, accuracy might be misleading. If distributions are skewed, you might need transformations.</p> <p>Real example: Exploring customer data reveals that 95% are from age 25-35. This tells you your model might not work well for older customers\u2014you need more diverse data or age-specific models.</p>"},{"location":"03-data/data-foundations/#phase-3-data-cleaning","title":"Phase 3: Data Cleaning","text":"<p>Purpose: Fix or remove problems that would cause errors or bias.</p> <p>Common tasks:</p> <p>Handling missing values:</p> <ul> <li>Remove: Delete rows/columns with missing data (if small percentage)</li> <li>Impute: Fill with mean, median, mode, or predicted values</li> <li>Flag: Create indicator variable showing missingness</li> </ul> <p>Dealing with duplicates:</p> <ul> <li>Identify exact or near-duplicates</li> <li>Remove or merge duplicate records</li> </ul> <p>Fixing inconsistencies:</p> <ul> <li>Standardize formats (dates: <code>YYYY-MM-DD</code>, names: Title Case)</li> <li>Correct typos and data entry errors</li> <li>Resolve conflicting values</li> </ul> <p>Handling outliers:</p> <ul> <li>Detect using statistical methods (Z-score, IQR)</li> <li>Decide: Remove, cap, or keep based on domain knowledge</li> </ul> <p>ML connection: Dirty data creates noisy patterns that models learn incorrectly. One extreme outlier can skew an entire regression line. Missing values handled poorly can introduce bias or cause crashes during training.</p> <p></p>"},{"location":"03-data/data-foundations/#phase-4-data-transformation","title":"Phase 4: Data Transformation","text":"<p>Purpose: Convert data into forms that algorithms can process effectively.</p> <p>Common transformations:</p> <p>Scaling and normalization:</p> <ul> <li>Min-max scaling: Squash values to  range</li> <li>Standardization: Center at 0 with std dev of 1</li> <li>Why: Prevents features with large values from dominating distance-based algorithms</li> </ul> <p>Encoding categorical variables:</p> <ul> <li>Label encoding: Assign numbers (red=0, blue=1, green=2)</li> <li>One-hot encoding: Create binary columns for each category</li> <li>Why: Most ML algorithms require numerical input</li> </ul> <p>Feature engineering:</p> <ul> <li>Combining features: Create \"price per square foot\" from price and area</li> <li>Extracting components: Pull day-of-week from datetime</li> <li>Binning: Convert continuous age to age groups</li> <li>Why: New features can capture relationships better than original data</li> </ul> <p>ML connection: Proper scaling is critical for gradient descent (used in neural networks) and distance-based algorithms (k-NN, k-means). Without it, features with larger scales dominate, leading to poor performance.</p> <p>Example: In predicting house prices</p> <ul> <li>Feature 1: Square footage (500-5000)</li> <li>Feature 2: Number of bedrooms (1-5)</li> </ul> <p>Without scaling, square footage dominates. After standardization, both contribute equally.</p>"},{"location":"03-data/data-foundations/#phase-5-data-splitting","title":"Phase 5: Data Splitting","text":"<p>Purpose: Separate data to train models and evaluate them fairly.</p> <p>Standard split:</p> <ul> <li>Training set (70-80%): Used to train the model</li> <li>Validation set (10-15%): Used to tune hyperparameters and select models</li> <li>Test set (10-15%): Used only once at the end for final evaluation</li> </ul> <p>Why split? A model evaluated on training data appears better than it actually is (overfitting). Testing on unseen data reveals true performance.</p> <p>ML connection: Without proper splitting, you can't trust performance metrics. A model with 99% accuracy on training data might have 60% on new data. The test set simulates how your model will perform in production.</p> <p>Critical rule: NEVER look at test set during development. It's your final \"exam\"\u2014use it only once.</p> <p></p>"},{"location":"03-data/data-foundations/#phase-6-data-storage-versioning","title":"Phase 6: Data Storage &amp; Versioning","text":"<p>Purpose: Organize datasets for reproducibility and collaboration.</p> <p>Best practices:</p> <ul> <li>Keep raw data separate: Never modify original files</li> <li>Version datasets: Track changes like you track code (DVC, Git LFS)</li> <li>Document transformations: Record every step applied</li> <li>Use consistent naming: <code>data_raw.csv</code>, <code>data_cleaned.csv</code>, <code>data_train.csv</code></li> </ul> <p>ML connection: When model performance changes, you need to know if it's due to code changes or data changes. Versioning enables debugging, auditing, and reproducing results months later.</p>"},{"location":"03-data/data-foundations/#common-data-challenges","title":"Common Data Challenges","text":""},{"location":"03-data/data-foundations/#insufficient-data","title":"Insufficient Data","text":"<p>Problem: Too few samples for algorithms to learn patterns effectively.</p> <p>Impact: High variance, poor generalization, overfitting to noise.</p> <p>Solutions: Data augmentation, transfer learning, collect more data.</p>"},{"location":"03-data/data-foundations/#imbalanced-data","title":"Imbalanced Data","text":"<p>Problem: Unequal representation of classes (95% normal, 5% fraud).</p> <p>Impact: Models predict majority class for everything, achieving high accuracy but missing minority class entirely.</p> <p>Solutions: Resampling, cost-sensitive learning, different metrics (F1-score, not accuracy).</p>"},{"location":"03-data/data-foundations/#high-dimensionality","title":"High Dimensionality","text":"<p>Problem: Too many features relative to samples (curse of dimensionality).</p> <p>Impact: Models require exponentially more data, distance metrics become meaningless, increased computation.</p> <p>Solutions: Feature selection, dimensionality reduction (PCA), regularization.</p>"},{"location":"03-data/data-foundations/#data-drift","title":"Data Drift","text":"<p>Problem: Production data distribution differs from training data over time.</p> <p>Impact: Model performance degrades silently in production.</p> <p>Solutions: Continuous monitoring, periodic retraining, drift detection systems.</p>"},{"location":"03-data/data-foundations/#best-practices","title":"Best Practices","text":"<p>Start with exploration: Look at your data before deciding on approaches. One hour of exploration saves ten hours of debugging.</p> <p>Document everything: Record data sources, transformations, and decisions. Your future self will thank you.</p> <p>Version control datasets: Treat data like code. Track changes and tag versions used for specific models.</p> <p>Check for bias early: Examine demographic representation, temporal coverage, and geographic diversity before investing in modeling.</p> <p>Validate continuously: Don't wait until the end to check data quality. Validate at each pipeline stage.</p> <p>Keep raw data untouched: Always maintain original data. Apply transformations in code, never by editing files directly.</p> <p>Test pipeline with small samples: Verify your entire workflow on small data before processing millions of records.</p>"},{"location":"03-data/data-foundations/#quick-reference-data-pipeline-phases","title":"Quick Reference: Data Pipeline Phases","text":"Phase Key Question Tools/Techniques Next Tutorial Collection Where does data come from? APIs, databases, web scraping Data Collection Exploration What does data look like? Summary stats, visualizations Pandas, Visualization Cleaning What's wrong with data? Handle missing, duplicates, outliers Data Wrangling Transformation How to prepare for models? Scaling, encoding, feature engineering Pandas, NumPy Splitting How to evaluate fairly? Train/val/test split, cross-validation Pandas Storage How to organize and track? File structures, versioning, documentation Data Collection"},{"location":"03-data/data-foundations/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned why data quality determines ML success, understood the systematic data pipeline workflow, recognized common data challenges and their impacts, and connected proper data handling to model performance and production readiness.</p> <p>Critical insights:</p> <ul> <li>80/20 rule in practice: Most ML work is data preparation, not modeling</li> <li>Quality over quantity: Small, clean datasets often outperform large, messy ones</li> <li>Domain knowledge matters: Understanding context helps make better data decisions</li> <li>Iteration is normal: Data work is cyclical\u2014exploration reveals cleaning needs, cleaning enables better exploration</li> </ul> <p>Connections to previous topics:</p> <ul> <li>Statistics: Use mean, variance, distributions to understand data patterns</li> <li>Probability: Sampling strategies affect what patterns models can learn</li> <li>Linear algebra: Data is organized as matrices (rows = samples, columns = features)</li> </ul> <p>What's next in this section:</p> <p>Pandas Essentials: Load, filter, transform, and aggregate data efficiently</p> <p>Visualization: Create plots to explore patterns and communicate insights</p> <p>Data Wrangling: Handle missing values, merge datasets, reshape data structures</p> <p>Data Collection: APIs, web scraping, and database queries for gathering data</p> <p>SQL &amp; NoSQL: Query structured and unstructured data stores</p> <p>Each tutorial builds hands-on skills for specific pipeline phases. By the end, you'll have a complete workflow from raw data to model-ready datasets.</p> <p>External resources:</p> <ul> <li>Google's Machine Learning Crash Course: Data Preparation - practical data prep strategies</li> <li>Kaggle Learn: Data Cleaning - interactive data cleaning exercises</li> </ul> <p>Remember: Great models start with great data. Master the pipeline, and you'll build better ML systems than those who chase fancy algorithms with messy data.</p>"},{"location":"03-data/data-wrangling/","title":"Data wrangling","text":""},{"location":"03-data/data-wrangling/#materials-under-construction","title":"Materials Under Construction","text":"<p>This section is still brewing. In the meantime, grab a snack, ship some code, and check back soon.</p> <p></p>"},{"location":"03-data/mongo/","title":"NoSQL (MongoDB)","text":""},{"location":"03-data/mongo/#materials-under-construction","title":"Materials Under Construction","text":"<p>This section is still brewing. In the meantime, grab a snack, ship some code, and check back soon.</p> <p></p>"},{"location":"03-data/pandas/","title":"Pandas Essentials: Data Manipulation in Python","text":"<p>This tutorial teaches you to work with tabular data using pandas, Python's most powerful data analysis library. You'll learn the fundamental operations that transform messy CSV files into clean, model-ready datasets through practical examples and clear explanations.</p> <p>Estimated time: 60 minutes</p>"},{"location":"03-data/pandas/#why-this-matters","title":"Why This Matters","text":"<p>Problem statement: Real-world data arrives in spreadsheets, databases, and CSV files\u2014tables with hundreds of columns, thousands of rows, missing values, mixed types, and unclear patterns. Without pandas, loading and manipulating this data requires hundreds of lines of code, nested loops, and manual error checking. Simple tasks like \"show me customers over age 25 from Tirana\" become programming challenges.</p> <p>Practical benefits: Pandas reduces complex data operations to single readable commands. What takes 50 lines in pure Python becomes one line in pandas. It handles the messy reality of real data\u2014missing values, mixed types, date parsing, string operations\u2014with built-in methods. Pandas makes data exploration fast enough to be interactive, cleaning operations repeatable, and feature engineering productive. You spend time discovering insights instead of debugging loops.</p> <p>Professional context: Every data scientist uses pandas daily\u2014it's as fundamental as knowing Git or SQL. Before neural networks or gradient boosting, data must be cleaned and shaped correctly. Pandas is the universal tool for this work. Job interviews test pandas proficiency because it's essential infrastructure. Open any data science notebook on Kaggle or GitHub, and you'll see pandas in the first cell. </p> <p>PLACEHOLDER</p>"},{"location":"03-data/pandas/#prerequisites-learning-objectives","title":"Prerequisites &amp; Learning Objectives","text":"<p>Required knowledge: - Python basics (lists, dictionaries, loops, functions) - Basic statistics (mean, median, sum, count) - Understanding of tables and spreadsheets (rows, columns, cells)</p> <p>Required tools: - Python 3.x installed - pandas library (<code>pip install pandas</code>) - Jupyter Notebook recommended for interactive exploration</p> <p>Learning outcomes: - Understand DataFrames and Series as pandas data structures - Load data from CSV files and inspect basic properties - Select specific columns and filter rows based on conditions - Handle missing values through removal or imputation - Create new columns from existing data (feature engineering) - Group data by categories and compute aggregate statistics - Connect each operation to machine learning pipeline phases</p> <p>High-level approach: Learn pandas through progressive examples, starting with loading data and building to complex grouping operations, always connecting concepts to practical ML workflows.</p>"},{"location":"03-data/pandas/#core-concepts","title":"Core Concepts","text":""},{"location":"03-data/pandas/#what-is-pandas","title":"What is Pandas?","text":"<p>Pandas is a Python library that provides high-performance data structures and analysis tools for working with structured (tabular) data. Think of it as \"Excel but programmable, faster, and more powerful.\"</p> <p>Two core data structures:</p> <p>DataFrame: A 2D labeled table with rows and columns (like a spreadsheet or SQL table). Each column can have a different data type.</p> <p>Series: A single column (or row) with an index. Essentially a 1D array with labels.</p> <p>Why \"pandas\"? The name comes from \"panel data,\" an econometrics term for multidimensional datasets, but it's commonly used for all tabular data.</p>"},{"location":"03-data/pandas/#why-pandas-for-machine-learning","title":"Why Pandas for Machine Learning?","text":"<p>Speed: Built on top of NumPy (C-optimized arrays), pandas inherits NumPy's performance while adding labels and intuitive syntax.</p> <p>Real-world data handling: Deals with missing values, mixed types, string operations, date parsing, which are all common in production datasets.</p> <p>Readable syntax: Operations read like English questions: \"give me rows where age is greater than 25.\"</p> <p>Integration: Works well with <code>scikit-learn</code>, <code>matplotlib</code>, and other ML libraries.</p> <p>Key insight: While NumPy provides fast numerical arrays, pandas adds structure and labels that make data manipulation intuitive. You think in terms of \"customers\" and \"purchases,\" not \"row 47, column 3.\"</p> <p>PLACEHOLDER</p>"},{"location":"03-data/pandas/#step-by-step-instructions","title":"Step-by-Step Instructions","text":""},{"location":"03-data/pandas/#understanding-dataframes-and-series","title":"Understanding DataFrames and Series","text":"<p>Before loading real data, understand pandas' two fundamental structures.</p> <p>DataFrame basics:</p> <pre><code>import pandas as pd\n\n# Create a DataFrame from a dictionary\ndata = {\n    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n    'age': [25, 30, 35, 28],\n    'city': ['Tirana', 'Durr\u00ebs', 'Tirana', 'Vlor\u00eb'],\n    'salary': [45000, 52000, 61000, 48000]\n}\n\ndf = pd.DataFrame(data)\nprint(df)\n</code></pre> <p>Expected output: <pre><code>      name  age    city  salary\n0    Alice   25  Tirana   45000\n1      Bob   30  Durr\u00ebs   52000\n2  Charlie   35  Tirana   61000\n3    Diana   28   Vlor\u00eb   48000\n</code></pre></p> <p>Key observations: - Left column (0, 1, 2, 3) is the index (row labels) - Top row contains column names - Each column can have different types (strings, integers) - Data organized in rows (samples) and columns (features)</p> <p>Basic properties:</p> <pre><code># Shape: (rows, columns)\nprint(f\"Shape: {df.shape}\")  # (4, 4)\n\n# Column names\nprint(f\"Columns: {df.columns.tolist()}\")\n\n# Data types\nprint(df.dtypes)\n</code></pre> <p>Series: Single column</p> <pre><code># Extract one column (returns a Series)\nages = df['age']\nprint(type(ages))  # &lt;class 'pandas.core.series.Series'&gt;\nprint(ages)\n</code></pre> <p>ML connection: In machine learning, DataFrames represent datasets where rows are samples (observations, data points) and columns are features (variables, attributes). A customer dataset might have 10,000 rows (customers) and 20 columns (age, location, purchase history, etc.).</p>"},{"location":"03-data/pandas/#loading-data-from-files","title":"Loading Data from Files","text":"<p>Real data comes from files, not dictionaries. Learn to import common formats.</p> <p>Loading CSV (most common):</p> <pre><code>import pandas as pd\n\n# Load CSV file\ndf = pd.read_csv('data.csv')\n\n# Common parameters\ndf = pd.read_csv('data.csv',\n                 sep=',',           # delimiter (default is comma)\n                 header=0,          # which row contains column names\n                 index_col=None,    # which column to use as index\n                 encoding='utf-8')  # character encoding\n</code></pre> <p>First look at data:</p> <pre><code># First 5 rows\nprint(df.head())\n\n# Last 5 rows\nprint(df.tail())\n\n# First N rows\nprint(df.head(10))\n</code></pre> <p>Dataset overview:</p> <pre><code># Concise summary: columns, types, non-null counts\nprint(df.info())\n\n# Statistical summary of numerical columns\nprint(df.describe())\n\n# Number of rows and columns\nprint(f\"Shape: {df.shape}\")\nprint(f\"{df.shape[0]} rows, {df.shape[1]} columns\")\n</code></pre> <p>Example output from <code>.info()</code>: <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 244 entries, 0 to 243\nData columns (total 7 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   total_bill  244 non-null    float64\n 1   tip         244 non-null    float64\n 2   sex         244 non-null    object \n 3   smoker      244 non-null    object \n 4   day         244 non-null    object \n 5   time        244 non-null    object \n 6   size        244 non-null    int64  \ndtypes: float64(2), int64(1), object(4)\n</code></pre></p> <p>Reading other formats:</p> <pre><code># Excel file\ndf = pd.read_excel('data.xlsx', sheet_name='Sheet1')\n\n# From URL\nurl = 'https://example.com/data.csv'\ndf = pd.read_csv(url)\n</code></pre> <p>ML connection: This is the Data Collection phase of the ML pipeline. Always start by loading data and inspecting it with <code>.head()</code>, <code>.info()</code>, and <code>.describe()</code> before any analysis or modeling.</p>"},{"location":"03-data/pandas/#selecting-columns","title":"Selecting Columns","text":"<p>Extract specific columns to focus your analysis or prepare features for modeling.</p> <p>Single column (returns Series):</p> <pre><code># Select 'age' column\nages = df['age']\nprint(type(ages))  # pandas.core.series.Series\n\n# Also works with dot notation (if column name has no spaces)\nages = df.age\n</code></pre> <p>Multiple columns (returns DataFrame):</p> <pre><code># Select multiple columns - pass a list\nsubset = df[['name', 'age', 'city']]\nprint(subset.head())\n\n# Order matters - columns appear in the order you specify\nreordered = df[['city', 'name', 'age']]\n</code></pre> <p>Column names:</p> <pre><code># List all column names\nprint(df.columns)\n\n# Convert to regular Python list\nprint(df.columns.tolist())\n\n# Number of columns\nprint(len(df.columns))\n</code></pre> <p>Dropping columns:</p> <pre><code># Drop one column\ndf_smaller = df.drop('salary', axis=1)\n\n# Drop multiple columns\ndf_smaller = df.drop(['age', 'salary'], axis=1)\n\n# Drop columns in place (modifies original DataFrame)\ndf.drop('salary', axis=1, inplace=True)\n</code></pre> <p>Why select columns?</p> <ul> <li>Focus on relevant features</li> <li>Reduce memory usage</li> <li>Prepare specific features for modeling</li> <li>Create different views of the same data</li> </ul> <p>ML connection: Feature selection is critical in machine learning. Not all columns are useful for predictions. Selecting relevant features improves model performance and reduces training time.</p>"},{"location":"03-data/pandas/#filtering-rows-selection-by-condition","title":"Filtering Rows (Selection by Condition)","text":"<p>Select rows that meet specific criteria\u2014one of pandas' most powerful features.</p> <p>Single condition:</p> <pre><code># People older than 30\nolder = df[df['age'] &gt; 30]\n\n# People from Tirana\nfrom_tirana = df[df['city'] == 'Tirana']\n\n# Salaries greater than or equal to 50000\nhigh_earners = df[df['salary'] &gt;= 50000]\n</code></pre> <p>Multiple conditions with AND (<code>&amp;</code>):</p> <pre><code># People from Tirana AND age &gt; 25\nresult = df[(df['city'] == 'Tirana') &amp; (df['age'] &gt; 25)]\n\n# IMPORTANT: Parentheses around each condition are required!\n</code></pre> <p>Multiple conditions with OR (<code>|</code>):</p> <pre><code># People from Tirana OR Durr\u00ebs\nresult = df[(df['city'] == 'Tirana') | (df['city'] == 'Durr\u00ebs')]\n\n# Age less than 25 OR greater than 35\nresult = df[(df['age'] &lt; 25) | (df['age'] &gt; 35)]\n</code></pre> <p>Using <code>.isin()</code> for multiple values:</p> <pre><code># People from Tirana, Durr\u00ebs, or Vlor\u00eb\ncities = ['Tirana', 'Durr\u00ebs', 'Vlor\u00eb']\nresult = df[df['city'].isin(cities)]\n</code></pre> <p>Using <code>.query()</code> method (more readable):</p> <pre><code># Same as above but cleaner syntax\nresult = df.query('age &gt; 30')\nresult = df.query('city == \"Tirana\" and age &gt; 25')\nresult = df.query('city in [\"Tirana\", \"Durr\u00ebs\"]')\n</code></pre> <p>Negation (NOT):</p> <pre><code># People NOT from Tirana\nresult = df[df['city'] != 'Tirana']\n\n# Using tilde (~) for negation\nresult = df[~(df['city'] == 'Tirana')]\n</code></pre> <p>Counting filtered results:</p> <pre><code># How many people are over 30?\ncount = len(df[df['age'] &gt; 30])\nprint(f\"{count} people are over 30\")\n</code></pre> <p>ML connection: Filtering is essential for:</p> <ul> <li>Removing outliers before training</li> <li>Creating training/test splits by condition</li> <li>Analyzing subgroups (stratified analysis)</li> <li>Generating class-specific statistics</li> </ul>"},{"location":"03-data/pandas/#handling-missing-data","title":"Handling Missing Data","text":"<p>Real-world data contains missing values. Pandas provides tools to detect and handle them.</p> <p>Detecting missing values:</p> <pre><code># Check for any missing values\nprint(df.isnull())  # Returns Boolean DataFrame\n\n# Count missing values per column\nprint(df.isnull().sum())\n\n# Count total missing values\nprint(df.isnull().sum().sum())\n\n# Which rows have ANY missing value?\nrows_with_missing = df[df.isnull().any(axis=1)]\n</code></pre> <p>Example output: <pre><code>name      0\nage       2\ncity      1\nsalary    0\ndtype: int64\n</code></pre></p> <p>Strategy 1: Drop missing values</p> <pre><code># Drop rows with ANY missing value\ndf_clean = df.dropna()\n\n# Drop rows where specific columns have missing values\ndf_clean = df.dropna(subset=['age', 'salary'])\n\n# Drop columns with ANY missing value\ndf_clean = df.dropna(axis=1)\n\n# Drop only if ALL values in row are missing\ndf_clean = df.dropna(how='all')\n</code></pre> <p>Strategy 2: Fill missing values</p> <pre><code># Fill with a specific value\ndf['age'].fillna(0, inplace=True)\n\n# Fill with mean\ndf['age'].fillna(df['age'].mean(), inplace=True)\n\n# Fill with median (better for skewed distributions)\ndf['salary'].fillna(df['salary'].median(), inplace=True)\n\n# Fill with mode (most common value)\ndf['city'].fillna(df['city'].mode()[0], inplace=True)\n\n# Forward fill (use previous valid value)\ndf['age'].fillna(method='ffill', inplace=True)\n\n# Backward fill (use next valid value)\ndf['age'].fillna(method='bfill', inplace=True)\n</code></pre> <p>Fill all columns at once:</p> <pre><code># Fill all numeric columns with their means\nnumeric_cols = df.select_dtypes(include=['number']).columns\nfor col in numeric_cols:\n    df[col].fillna(df[col].mean(), inplace=True)\n</code></pre> <p>ML connection: Machine learning models cannot process missing values. During the </p> <p>Data Cleaning phase, you must either:</p> <ul> <li>Remove rows/columns with missing data (acceptable if &lt; 5% of data)</li> <li>Impute (fill) missing values with statistical measures</li> <li>Use advanced imputation methods (KNN, iterative)</li> </ul> <p>The choice affects model performance\u2014test different strategies.</p>"},{"location":"03-data/pandas/#creating-new-columns","title":"Creating New Columns","text":"<p>Feature engineering creates new columns from existing data to improve model predictions.</p> <p>Simple calculations:</p> <pre><code># Create age in months\ndf['age_months'] = df['age'] * 12\n\n# Calculate salary per year of age\ndf['salary_per_age'] = df['salary'] / df['age']\n\n# Boolean column\ndf['is_tirana'] = df['city'] == 'Tirana'\n</code></pre> <p>Using functions with <code>.apply()</code>:</p> <pre><code># Define a function\ndef categorize_age(age):\n    if age &lt; 25:\n        return 'Young'\n    elif age &lt; 35:\n        return 'Middle'\n    else:\n        return 'Senior'\n\n# Apply to create new column\ndf['age_category'] = df['age'].apply(categorize_age)\n\n# Using lambda (inline function)\ndf['age_category'] = df['age'].apply(\n    lambda x: 'Young' if x &lt; 25 else 'Middle' if x &lt; 35 else 'Senior'\n)\n</code></pre> <p>Combining multiple columns:</p> <pre><code># Create full description\ndf['description'] = df['name'] + ' from ' + df['city']\n\n# Conditional based on multiple columns\ndf['high_earner_from_capital'] = (\n    (df['salary'] &gt; 50000) &amp; (df['city'] == 'Tirana')\n)\n</code></pre> <p>Using <code>.map()</code> for replacements:</p> <pre><code># Map cities to regions\ncity_to_region = {\n    'Tirana': 'Central',\n    'Durr\u00ebs': 'Coastal',\n    'Vlor\u00eb': 'Coastal',\n    'Shkod\u00ebr': 'North'\n}\ndf['region'] = df['city'].map(city_to_region)\n</code></pre> <p>Renaming columns:</p> <pre><code># Rename specific columns\ndf.rename(columns={'salary': 'annual_salary', 'age': 'years_old'}, inplace=True)\n\n# Rename all columns (useful for cleaning names)\ndf.columns = [col.lower().replace(' ', '_') for col in df.columns]\n</code></pre> <p>ML connection: Feature engineering is often the most impactful way to improve model performance. Good features can make a simple model outperform a complex one with raw features. Common transformations:</p> <ul> <li>Ratios (revenue per customer)</li> <li>Categories from continuous values (age groups)</li> <li>Combinations (interactions between features)</li> <li>Time-based features (day of week, month, season)</li> </ul>"},{"location":"03-data/pandas/#sorting-and-ranking","title":"Sorting and Ranking","text":"<p>Order data to find patterns, extremes, or prepare for specific analyses.</p> <p>Sort by single column:</p> <pre><code># Sort by age (ascending - default)\ndf_sorted = df.sort_values('age')\n\n# Sort by age (descending)\ndf_sorted = df.sort_values('age', ascending=False)\n\n# Sort in place\ndf.sort_values('age', inplace=True)\n</code></pre> <p>Sort by multiple columns:</p> <pre><code># Sort by city, then by age within each city\ndf_sorted = df.sort_values(['city', 'age'])\n\n# Different order for each column\ndf_sorted = df.sort_values(['city', 'age'], \n                           ascending=[True, False])\n</code></pre> <p>Reset index after sorting:</p> <pre><code># After sorting, index might be out of order\ndf_sorted = df.sort_values('age').reset_index(drop=True)\n</code></pre> <p>Finding top/bottom values:</p> <pre><code># Top 5 earners\ntop_5 = df.nlargest(5, 'salary')\n\n# Bottom 5 by age\nyoungest_5 = df.nsmallest(5, 'age')\n</code></pre> <p>Ranking:</p> <pre><code># Rank salaries (1 = lowest)\ndf['salary_rank'] = df['salary'].rank()\n\n# Rank in descending order (1 = highest)\ndf['salary_rank'] = df['salary'].rank(ascending=False)\n\n# Handle ties with average ranks\ndf['salary_rank'] = df['salary'].rank(method='average')\n</code></pre> <p>ML connection: Sorting is useful for:</p> <ul> <li>Finding outliers (highest/lowest values)</li> <li>Time-series data (ensuring chronological order)</li> <li>Ranking customers by value</li> <li>Identifying top performers for analysis</li> </ul>"},{"location":"03-data/pandas/#grouping-and-aggregating","title":"Grouping and Aggregating","text":"<p>Group data by categories and compute summary statistics\u2014one of pandas' most powerful features.</p> <p>Basic grouping:</p> <pre><code># Average salary by city\navg_salary = df.groupby('city')['salary'].mean()\nprint(avg_salary)\n\n# Count people per city\ncounts = df.groupby('city').size()\n# or\ncounts = df.groupby('city')['name'].count()\n</code></pre> <p>Expected output: <pre><code>city\nDurr\u00ebs    52000.0\nTirana    53000.0\nVlor\u00eb     48000.0\nName: salary, dtype: float64\n</code></pre></p> <p>Multiple aggregations:</p> <pre><code># Multiple statistics for salary\nsummary = df.groupby('city')['salary'].agg(['mean', 'median', 'min', 'max', 'std'])\nprint(summary)\n\n# Different aggregations for different columns\nsummary = df.groupby('city').agg({\n    'salary': ['mean', 'sum'],\n    'age': ['mean', 'min', 'max'],\n    'name': 'count'  # count of names = count of people\n})\n</code></pre> <p>Grouping by multiple columns:</p> <pre><code># Average salary by city AND age category\navg_by_city_age = df.groupby(['city', 'age_category'])['salary'].mean()\nprint(avg_by_city_age)\n\n# Convert to DataFrame with unstack()\nresult = df.groupby(['city', 'age_category'])['salary'].mean().unstack()\n</code></pre> <p>Filtering groups:</p> <pre><code># Only cities with more than 2 people\nlarge_cities = df.groupby('city').filter(lambda x: len(x) &gt; 2)\n\n# Cities where average salary exceeds 50000\nhigh_salary_cities = df.groupby('city').filter(\n    lambda x: x['salary'].mean() &gt; 50000\n)\n</code></pre> <p>Applying custom functions:</p> <pre><code>def salary_range(group):\n    return group['salary'].max() - group['salary'].min()\n\n# Apply custom function to each group\nranges = df.groupby('city').apply(salary_range)\n</code></pre> <p>ML connection: Grouping creates aggregate features that are highly predictive:</p> <ul> <li>Customer lifetime value (sum of purchases per customer)</li> <li>Average session duration (mean time per user)</li> <li>Purchase frequency (count of orders per month)</li> <li>Conversion rate by segment (ratio of conversions to visits)</li> </ul> <p>These aggregated features often perform better than raw transaction data.</p>"},{"location":"03-data/pandas/#value-counts-and-unique-values","title":"Value Counts and Unique Values","text":"<p>Understand the distribution of categorical variables.</p> <p>Value counts:</p> <pre><code># Count occurrences of each city\ncity_counts = df['city'].value_counts()\nprint(city_counts)\n\n# With percentages\ncity_pcts = df['city'].value_counts(normalize=True) * 100\nprint(city_pcts)\n\n# Include missing values in count\ncounts_with_na = df['city'].value_counts(dropna=False)\n</code></pre> <p>Expected output: <pre><code>city\nTirana    2\nDurr\u00ebs    1\nVlor\u00eb     1\nName: count, dtype: int64\n</code></pre></p> <p>Unique values:</p> <pre><code># List all unique cities\ncities = df['city'].unique()\nprint(cities)  # array(['Tirana', 'Durr\u00ebs', 'Vlor\u00eb'], dtype=object)\n\n# Count of unique values\nn_cities = df['city'].nunique()\nprint(f\"Number of unique cities: {n_cities}\")\n</code></pre> <p>Cross-tabulation:</p> <pre><code># Count combinations of two categorical variables\ncrosstab = pd.crosstab(df['city'], df['age_category'])\nprint(crosstab)\n</code></pre> <p>ML connection: Understanding category distributions helps with:</p> <ul> <li>Class imbalance detection: If 95% of samples are one class, model will be biased</li> <li>Feature encoding decisions: Categories with few samples might need special handling</li> <li>Stratification: Ensuring train/test splits represent all categories proportionally</li> </ul>"},{"location":"03-data/pandas/#data-types-and-conversion","title":"Data Types and Conversion","text":"<p>Proper data types improve performance, enable appropriate operations, and reduce memory usage.</p> <p>Check current types:</p> <pre><code># View data types of all columns\nprint(df.dtypes)\n\n# Check type of specific column\nprint(df['age'].dtype)\n</code></pre> <p>Convert types:</p> <pre><code># Convert to integer\ndf['age'] = df['age'].astype('int')\n\n# Convert to float\ndf['salary'] = df['salary'].astype('float')\n\n# Convert to string\ndf['id'] = df['id'].astype('str')\n\n# Convert to category (saves memory for repeated values)\ndf['city'] = df['city'].astype('category')\n</code></pre> <p>Numeric conversions with error handling:</p> <pre><code># Convert to numeric, coercing errors to NaN\ndf['age'] = pd.to_numeric(df['age'], errors='coerce')\n\n# Convert to numeric, raising error if impossible\ndf['age'] = pd.to_numeric(df['age'], errors='raise')\n</code></pre> <p>Categorical optimization:</p> <pre><code># Regular string column\ndf['city'] = df['city'].astype('object')  # default\nprint(df.memory_usage())\n\n# Convert to categorical (more efficient for repeated values)\ndf['city'] = df['city'].astype('category')\nprint(df.memory_usage())  # Typically much smaller\n</code></pre> <p>Why it matters:</p> <ul> <li>Memory: Categorical types use less memory than strings</li> <li>Speed: Operations on proper types are faster</li> <li>Correctness: Some operations require specific types</li> </ul> <p>ML connection: Proper data types are essential for:</p> <ul> <li>Encoding: Categorical variables need categorical type before one-hot encoding</li> <li>Numerical operations: Age stored as string can't be used in calculations</li> <li>Memory efficiency: Large datasets require careful type management</li> </ul>"},{"location":"03-data/pandas/#exporting-data","title":"Exporting Data","text":"<p>Save processed data for later use or sharing.</p> <p>Save to CSV:</p> <pre><code># Basic export\ndf.to_csv('output.csv')\n\n# Without row index\ndf.to_csv('output.csv', index=False)\n\n# With specific encoding\ndf.to_csv('output.csv', index=False, encoding='utf-8')\n\n# Only specific columns\ndf[['name', 'age']].to_csv('subset.csv', index=False)\n</code></pre> <p>Save to Excel:</p> <pre><code># Single sheet\ndf.to_excel('output.xlsx', index=False, sheet_name='Data')\n\n# Multiple sheets\nwith pd.ExcelWriter('output.xlsx') as writer:\n    df.to_excel(writer, sheet_name='Raw Data', index=False)\n    summary.to_excel(writer, sheet_name='Summary')\n</code></pre> <p>Other formats:</p> <pre><code># Parquet (efficient for large datasets)\ndf.to_parquet('output.parquet')\n\n# JSON\ndf.to_json('output.json', orient='records')\n\n# SQL database\n# df.to_sql('table_name', connection, if_exists='replace')\n</code></pre> <p>ML connection: Saving processed data is part of the Storage phase:</p> <ul> <li>Save cleaned data separately from raw data</li> <li>Export train/test splits for reproducibility</li> <li>Store engineered features for model training</li> <li>Version datasets alongside code</li> </ul>"},{"location":"03-data/pandas/#quick-reference","title":"Quick Reference","text":"Task Code Purpose Load CSV <code>pd.read_csv('file.csv')</code> Import data First rows <code>.head()</code>, <code>.tail()</code> Preview data Info <code>.info()</code>, <code>.describe()</code> Understand structure Select column <code>df['column']</code> Extract feature Select columns <code>df[['col1', 'col2']]</code> Multiple features Filter rows <code>df[df['age'] &gt; 25]</code> Conditional selection Missing values <code>.isnull().sum()</code> Detect nulls Drop nulls <code>.dropna()</code> Remove missing Fill nulls <code>.fillna(value)</code> Impute missing New column <code>df['new'] = calculation</code> Feature engineering Apply function <code>.apply(func)</code> Transform values Sort <code>.sort_values('col')</code> Order data Group <code>.groupby('col').mean()</code> Aggregate statistics Value counts <code>.value_counts()</code> Count categories Unique values <code>.unique()</code>, <code>.nunique()</code> Distinct values Data types <code>.dtypes</code>, <code>.astype()</code> Type information/conversion Save CSV <code>.to_csv('file.csv')</code> Export data"},{"location":"03-data/pandas/#common-workflows","title":"Common Workflows","text":"<p>Workflow 1: Load \u2192 Explore \u2192 Clean</p> <pre><code># Load data\ndf = pd.read_csv('data.csv')\n\n# Explore\nprint(df.info())\nprint(df.describe())\nprint(df.isnull().sum())\n\n# Clean\ndf = df.dropna()  # Remove missing\ndf = df[df['age'] &gt; 0]  # Remove invalid values\n</code></pre> <p>Workflow 2: Filter \u2192 Group \u2192 Analyze</p> <pre><code># Filter relevant subset\nactive_users = df[df['status'] == 'active']\n\n# Group and compute statistics\nsummary = active_users.groupby('region')['revenue'].agg(['sum', 'mean', 'count'])\n\n# Sort results\nsummary = summary.sort_values('sum', ascending=False)\n</code></pre> <p>Workflow 3: Feature Engineering \u2192 Export</p> <pre><code># Create derived features\ndf['revenue_per_customer'] = df['revenue'] / df['customers']\ndf['month'] = pd.to_datetime(df['date']).dt.month\n\n# Select relevant columns\ndf_model = df[['revenue_per_customer', 'month', 'region', 'target']]\n\n# Export for modeling\ndf_model.to_csv('model_data.csv', index=False)\n</code></pre>"},{"location":"03-data/pandas/#best-practices","title":"Best Practices","text":"<p>Always explore first: Before making changes, use <code>.head()</code>, <code>.info()</code>, <code>.describe()</code>, and <code>.value_counts()</code> to understand your data.</p> <p>Use <code>.copy()</code> for safety: When creating modified DataFrames, use <code>df_new = df.copy()</code> to avoid unintended changes to the original.</p> <p>Chain methods carefully: <code>df.groupby('city').mean().sort_values('salary')</code> is readable, but break complex chains into steps for debugging.</p> <p>Check after transformations: After each operation, verify it worked\u2014check shape, null counts, and sample values.</p> <p>Keep raw data unchanged: Never modify original files. Apply all transformations in code for reproducibility.</p> <p>Document transformations: Comment your code explaining why you made each transformation decision.</p> <p>Test with small samples: Before processing millions of rows, test your workflow on the first 1000 rows to catch errors quickly.</p>"},{"location":"03-data/pandas/#ml-pipeline-integration","title":"ML Pipeline Integration","text":"<p>Pandas operations map directly to machine learning pipeline phases:</p> Pipeline Phase Pandas Operations Purpose Collection <code>pd.read_csv()</code>, <code>pd.read_excel()</code>, <code>pd.read_sql()</code> Load data from sources Exploration <code>.head()</code>, <code>.info()</code>, <code>.describe()</code>, <code>.value_counts()</code> Understand structure and distributions Cleaning <code>.dropna()</code>, <code>.fillna()</code>, filtering, deduplication Remove errors and handle missing values Transformation New columns, <code>.apply()</code>, <code>.map()</code>, <code>.astype()</code> Engineer features and prepare for modeling Splitting Boolean indexing, <code>.sample()</code> Create train/validation/test sets Storage <code>.to_csv()</code>, <code>.to_parquet()</code> Save processed data"},{"location":"03-data/pandas/#summary-next-steps","title":"Summary &amp; Next Steps","text":"<p>Key accomplishments: You've learned to load data from CSV files, explore structure and distributions, select and filter subsets, handle missing values systematically, create new features through calculations and functions, group data by categories and compute aggregates, and connect every operation to machine learning workflows.</p> <p>Critical insights:</p> <ul> <li>DataFrames are the universal data structure for tabular data in Python ML ecosystem</li> <li>80% of ML work is data preparation: pandas is your primary tool for this phase</li> <li>Operations are composable: chain methods to build powerful data transformations</li> <li>Always validate: check results after each transformation to catch errors early</li> <li>Type matters: proper data types enable correct operations and reduce memory</li> </ul> <p>Real-world readiness: You can now:</p> <ul> <li>Load messy CSV files and make sense of their structure</li> <li>Clean data by detecting and handling missing values</li> <li>Filter datasets to analyze specific subgroups</li> <li>Engineer features that improve model predictions</li> <li>Group data to discover patterns and generate insights</li> <li>Prepare datasets in the exact format ML models expect</li> </ul> <p>Connections to previous topics:</p> <ul> <li>NumPy: Pandas is built on NumPy\u2014DataFrames are collections of NumPy arrays with labels</li> <li>Statistics: Use <code>.describe()</code>, <code>.groupby()</code>, and aggregations to compute statistical summaries</li> <li>Linear Algebra: Each DataFrame column is a vector; operations mirror vector/matrix operations</li> </ul> <p>External resources:</p> <ul> <li>Pandas Official Documentation - comprehensive reference with examples</li> <li>Pandas Cheat Sheet - quick command reference (2 pages)</li> <li>Kaggle Learn: Pandas - interactive micro-lessons with immediate feedback</li> <li>Python for Data Analysis (Book) - written by pandas creator Wes McKinney</li> </ul> <p>Next tutorial: Visualization with Matplotlib and Seaborn transform the insights you discovered in pandas into clear, compelling charts that communicate patterns and support decisions.</p>"},{"location":"03-data/sql/","title":"SQL databases","text":""},{"location":"03-data/sql/#materials-under-construction","title":"Materials Under Construction","text":"<p>This section is still brewing. In the meantime, grab a snack, ship some code, and check back soon.</p> <p></p>"},{"location":"03-data/visualization/","title":"Visualization","text":""},{"location":"03-data/visualization/#materials-under-construction","title":"Materials Under Construction","text":"<p>This section is still brewing. In the meantime, grab a snack, ship some code, and check back soon.</p> <p></p>"},{"location":"04-core-ml/","title":"Core ML Overview","text":"<p>Build solid ML skills step by step. Clear goals, small projects, and habits that make models reliable in the real world.</p> <p></p>"},{"location":"04-core-ml/#ml-lifecycle","title":"ML Lifecycle","text":"<p>From idea to impact</p> <p>See the full journey: define the problem, prepare data, build a baseline, evaluate with the right metrics, deploy, and monitor. Learn to loop\u2014measure, improve, and repeat.</p> <p>You\u2019ll learn: Framing problems, setting metrics, experiment tracking, deployment basics, and monitoring</p>"},{"location":"04-core-ml/#model-foundations","title":"Model foundations","text":"<p>The classics that still win</p> <p>Master the workhorses: linear/logistic regression, trees, ensembles, and k\u2011NN. Understand bias\u2013variance, regularization, cross\u2011validation, and when simple beats complex.</p> <p>You\u2019ll learn: Baselines, feature\u2013target fit, regularization, cross\u2011validation, and strong starter models</p>"},{"location":"04-core-ml/#supervised-learning","title":"Supervised learning","text":"<p>Predict with labels</p> <p>Build classifiers and regressors that generalize. Compare models, tune hyperparameters, and read learning curves to avoid overfitting.</p> <p>You\u2019ll learn: Pipelines, metrics (accuracy, F1, ROC\u2011AUC, MAE/RMSE), hyperparameter search, and model selection</p>"},{"location":"04-core-ml/#unsupervised-learning","title":"Unsupervised learning","text":"<p>Find structure without labels</p> <p>Discover patterns and reduce noise. Cluster similar items and compress features while keeping signal.</p> <p>You\u2019ll learn: k\u2011means, hierarchical clustering, DBSCAN, PCA, t\u2011SNE/UMAP basics, and evaluation without labels</p> <p>Start simple, measure honestly, and improve in small steps. A clear baseline plus good metrics will take most projects farther than fancy tricks.</p>"},{"location":"04-core-ml/classical-ml/","title":"Model foundations","text":""},{"location":"04-core-ml/classical-ml/#materials-under-construction","title":"Materials Under Construction","text":"<p>This section is still brewing. In the meantime, grab a snack, ship some code, and check back soon.</p> <p></p>"},{"location":"04-core-ml/lifecycle/","title":"ML lifecycle","text":""},{"location":"04-core-ml/lifecycle/#materials-under-construction","title":"Materials Under Construction","text":"<p>This section is still brewing. In the meantime, grab a snack, ship some code, and check back soon.</p> <p></p>"},{"location":"04-core-ml/supervised/","title":"Supervised learning","text":""},{"location":"04-core-ml/supervised/#materials-under-construction","title":"Materials Under Construction","text":"<p>This section is still brewing. In the meantime, grab a snack, ship some code, and check back soon.</p> <p></p>"},{"location":"04-core-ml/unsupervised/","title":"Unsupervised learning","text":""},{"location":"04-core-ml/unsupervised/#materials-under-construction","title":"Materials Under Construction","text":"<p>This section is still brewing. In the meantime, grab a snack, ship some code, and check back soon.</p> <p></p>"},{"location":"0X-portfolio/","title":"Portfolio Projects and ML Challenges","text":"<p>This section brings together a collection of Machine Learning practice challenges and mini projects designed to help you grow, while having fun.</p> <p>Each challenge focuses on building intuition, applying conceptual thinking, and hands-on project development. </p>"},{"location":"0X-portfolio/#challenges","title":"Challenges","text":""},{"location":"0X-portfolio/#math-foundations-challenge","title":"Math Foundations Challenge","text":"<p>A friendly, team-based review competition covering the core math concepts behind ML, such as Linear Algebra, Calculus, and Probability. It focuses on reasoning, intuition, and how these ideas connect to real ML workflows.</p> <p>More challenges and mini-projects will be added here over time, including topics such as: - Data exploration and visualization - Model understanding and interpretation - Applied ML problem-solving - Short research-inspired exercises  </p> <p>Stay tuned and keep exploring!</p>"},{"location":"0X-portfolio/math-challenge/","title":"ML Math Challenge - Competition Overview","text":""},{"location":"0X-portfolio/math-challenge/#objective","title":"Objective","text":"<p>This friendly competition is designed to help you review and connect key math concepts that form the foundation of Machine Learning (<code>Linear Algebra</code>, <code>Calculus</code>, and <code>Probability</code>.)</p> <p>You\u2019ll work in teams to solve short, intuitive challenges. No coding (maybe only vibe coding) or long calculations; just reasoning, teamwork, and quick thinking.</p>"},{"location":"0X-portfolio/math-challenge/#teams","title":"Teams","text":"<ul> <li>You\u2019ll work in teams. Discuss ideas together before answering.</li> <li>One team member will share the final answer for each question or task.</li> <li>Each team member must present at least one task</li> </ul>"},{"location":"0X-portfolio/math-challenge/#competition-flow","title":"Competition Flow","text":"<p>There are four rounds, each focusing on a different skill area. Each round includes a few short questions or mini tasks. Points are earned for correct answers, reasoning, and teamwork.</p>"},{"location":"0X-portfolio/math-challenge/#round-1-true-or-false-showdown","title":"Round 1 \u2013 True or False Showdown","text":"<ul> <li>This round will include quick statements from all topics.</li> <li>Points increase for each correct answer (1 \u2192 2 \u2192 3 \u2192 4 \u2192 5), but one wrong answer resets your score to <code>0</code></li> <li>Speed and accuracy matter.</li> </ul> <p>Review ideas:</p> <ul> <li>Basic matrix and vector properties</li> <li>Derivatives and slopes</li> <li>Probability and independence</li> <li>General math logic and intuition</li> </ul>"},{"location":"0X-portfolio/math-challenge/#round-2-linear-algebra-concepts","title":"Round 2 \u2013 Linear Algebra Concepts","text":"<p>Focus on how data can be represented and transformed using matrices and vectors. You'll reason about relationships between features, combinations, and transformations.</p> <p>Review ideas:</p> <ul> <li>Matrices, vectors, and dimensions</li> <li>Matrix \u00d7 vector multiplication</li> <li>Linear combinations and feature meaning</li> <li>Projections and geometric interpretation</li> </ul>"},{"location":"0X-portfolio/math-challenge/#round-3-calculus-and-optimization","title":"Round 3 \u2013 Calculus and Optimization","text":"<p>Explore how change, slope, and direction affect learning. You\u2019ll think about how models improve by adjusting parameters.</p> <p>Review ideas:</p> <ul> <li>Derivative as rate of change</li> <li>Gradient and direction of improvement</li> <li>Learning rate intuition</li> <li>Local minima and convergence</li> </ul>"},{"location":"0X-portfolio/math-challenge/#round-4-probability-and-statistics","title":"Round 4 \u2013 Probability and Statistics","text":"<p>Work with small data samples to understand patterns and uncertainty. Expect reasoning around averages, variation, and data interpretation.</p> <p>Review ideas:</p> <ul> <li>Mean, median, mode</li> <li>Variance and standard deviation</li> <li>Distributions and outliers</li> <li>Interpreting data intuitively</li> </ul>"},{"location":"0X-portfolio/math-challenge/#scoring","title":"Scoring","text":"<ul> <li>Points for correct answers and strong reasoning.</li> <li>Some rounds may include bonus points for clear explanations or teamwork.</li> <li>The team with the highest total score wins!</li> </ul>"},{"location":"0X-portfolio/math-challenge/#tips-for-success","title":"Tips for Success","text":"<ul> <li>Think out loud and explain your reasoning.</li> <li>Focus on intuition, not memorization.</li> <li>Work together efficiently, share ideas quickly.</li> <li>Stay relaxed and enjoy the process, learning through play is the goal!</li> </ul>"}]}